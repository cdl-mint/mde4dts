{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Snowballing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from functools import reduce\n",
    "import numpy as np\n",
    "\n",
    "IDENTIFIER = 'Identifier'\n",
    "DOI = 'DOI'\n",
    "TITLE = 'Title'\n",
    "ABSTRACT = 'Abstract'\n",
    "AUTHORS = 'Authors'\n",
    "PUBLISHED = 'Published'\n",
    "PUBLISHED_IN = 'Published_In'\n",
    "SOURCE = 'Source'\n",
    "RESOLVED_DOI = 'Resolved DOI'\n",
    "ABBREVATION = 'Abbrevation'\n",
    "RANK = 'Rank'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read DOIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hasDOI(entry):\n",
    "    doiExists = entry[DOI] == entry[DOI]    \n",
    "    identifer = entry[IDENTIFIER]\n",
    "    if not doiExists:\n",
    "        print(identifer + ' has no DOI')\n",
    "    return doiExists\n",
    "\n",
    "input_dois_pd = pd.read_csv('./input_dois.csv')\n",
    "input_dois = input_dois_pd.T.to_dict().values()\n",
    "input_dois = list(filter(hasDOI, input_dois))\n",
    "\n",
    "print()\n",
    "print('loaded ' + str(len(input_dois)) + ' DOIs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter Input DOIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.forward import get_forward_references\n",
    "\n",
    "MAX_PAPERS_PER_METRIC = 9999\n",
    "\n",
    "def get_selected_identifiers(row): \n",
    "    if(row.name == IDENTIFIER or row.name == TITLE or row.name == DOI):\n",
    "        return None\n",
    "    selected_cells = row.apply(lambda c: c == 'x' or c == 'X')\n",
    "    selected_indices = np.where(selected_cells)[0].tolist()\n",
    "    selected_identifiers = list(map(lambda i: identifiers[i], selected_indices))\n",
    "    return selected_identifiers\n",
    "\n",
    "df = input_dois_pd.T\n",
    "identifiers = df.iloc[[0]].values[0]\n",
    "papers_per_metric_pd = df.apply(get_selected_identifiers, axis=1).to_frame()\n",
    "papers_per_metric_pd = papers_per_metric_pd[papers_per_metric_pd[0].notnull()]\n",
    "\n",
    "def get_doi(identifier):\n",
    "    return input_dois_pd[input_dois_pd[IDENTIFIER].str.match(identifier)][DOI].values[0]\n",
    "\n",
    "def filter_most_used(papers, amount=MAX_PAPERS_PER_METRIC):\n",
    "    identifier_amount = []\n",
    "    for paper in papers:\n",
    "        doi = str(get_doi(paper))\n",
    "        if (doi == 'nan'):\n",
    "            continue\n",
    "        fr = get_forward_references({\n",
    "            IDENTIFIER: paper,\n",
    "            DOI: doi\n",
    "        })\n",
    "        cited_by = len(fr)\n",
    "        identifier_amount.append({\n",
    "            IDENTIFIER: paper,\n",
    "            'Cited_By': cited_by\n",
    "        })\n",
    "    identifier_amount.sort(reverse=True, key=lambda e: e['Cited_By'])\n",
    "    identifier_amount = list(map(lambda e: e[IDENTIFIER], identifier_amount))\n",
    "    return identifier_amount[0:amount]\n",
    "\n",
    "most_used_papers_per_metric_pd = papers_per_metric_pd.apply(lambda r: r.apply(filter_most_used))\n",
    "most_used_papers = list(reduce(list.__add__, most_used_papers_per_metric_pd[0].values))\n",
    "\n",
    "number_before_filtering = len(input_dois)\n",
    "input_dois = list(filter(lambda d:d[IDENTIFIER] in most_used_papers, input_dois))\n",
    "number_after_filtering = len(input_dois)\n",
    "\n",
    "print()\n",
    "print(str(number_after_filtering) + ' papers remain after filtering (' + str(number_before_filtering) + ')')\n",
    "# Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backwards Snowballing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.backward import get_backward_references\n",
    "\n",
    "backward_references = list(map(get_backward_references, input_dois))\n",
    "\n",
    "number_of_backward_references = len(backward_references)\n",
    "number_with_no_references = len(list(filter(lambda e: e is None, backward_references)))\n",
    "\n",
    "backward_references = list(filter(lambda e: e is not None, backward_references))\n",
    "\n",
    "print()\n",
    "print('loaded ' + str(number_of_backward_references) + ' backward reference entries, ' + str(number_with_no_references) + ' of them had no references')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resolve DOIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.backward import get_by_doi\n",
    "\n",
    "# extract backward references with dois\n",
    "backward_dois = list(map(lambda e: e['dois'], backward_references))\n",
    "backward_dois = list(reduce(list.__add__, backward_dois))\n",
    "backward_dois.sort()\n",
    "\n",
    "backward_references_from_dois = list(map(get_by_doi, backward_dois))\n",
    "backward_references_from_dois = list(filter(lambda b: b is not None, backward_references_from_dois))\n",
    "\n",
    "print()\n",
    "print('loaded '+ str(len(backward_references_from_dois)) + ' entries')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load references from bib files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.backward import read_bib_file\n",
    "from lib.general import get_published_in\n",
    "\n",
    "bib_entries = []\n",
    "for root, dirs, files in os.walk(\"../papers\"):\n",
    "    for file in files:\n",
    "        if file.endswith(\".bib\"):\n",
    "            es = read_bib_file(root, file)\n",
    "            bib_entries.extend(es)\n",
    "\n",
    "backward_references_bib_df = pd.DataFrame([e for e in bib_entries])\n",
    "backward_references_bib_df = backward_references_bib_df.loc[:, ['title', 'author', 'date', 'doi', 'referenced_by']]\n",
    "backward_references_bib_df.columns = [TITLE, AUTHORS, PUBLISHED, DOI, 'Referenced_By']\n",
    "backward_references_bib_df = backward_references_bib_df[backward_references_bib_df.Title.notnull()]\n",
    "backward_references_bib_df = backward_references_bib_df[backward_references_bib_df.Authors.notnull()]\n",
    "backward_references_bib_df[PUBLISHED_IN] = backward_references_bib_df[TITLE].map(get_published_in)\n",
    "\n",
    "print()\n",
    "print('loaded ' + str(len(backward_references_bib_df.index)) + ' bib-entries')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List references without DOI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "references_without_dois = list(filter(lambda e: e is not None, backward_references))\n",
    "references_without_dois = list(map(lambda e: e['non_dois'], references_without_dois))\n",
    "references_without_dois = list(reduce(list.__add__, references_without_dois))\n",
    "\n",
    "with open('intermediate/backward_references_without_dois.json', 'w') as outfile:\n",
    "    json.dump(references_without_dois, outfile, indent=4)\n",
    "\n",
    "print()\n",
    "print('Have found '+ str(len(references_without_dois)) + ' references without dois')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finish backward snowballing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TITLE = 'Title'\n",
    "\n",
    "backward_references_df = pd.DataFrame(backward_references_from_dois)\n",
    "backward_references_df = backward_references_df.append(backward_references_bib_df)\n",
    "\n",
    "number_before_filtering = len(backward_references_df.index)\n",
    "\n",
    "backward_references_df = backward_references_df[backward_references_df[TITLE].str.match('.*(test|mutant|mutation|coverage).*', case=False)==True]\n",
    "backward_references_df.drop_duplicates(subset=[TITLE], inplace=True)\n",
    "\n",
    "backward_references_df = backward_references_df.sort_values(TITLE)\n",
    "backward_references_df.to_csv('intermediate/backward.csv', index = False, header=True)\n",
    "\n",
    "number_after_filtering = len(backward_references_df.index)\n",
    "\n",
    "print()\n",
    "print(str(number_after_filtering) + ' studies remain after filtering the backward references (' + str(number_before_filtering) + ')')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forwards Snowballing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from functools import reduce\n",
    "from lib.forward import set_scholarly_logging\n",
    "from lib.forward import get_forward_references\n",
    "from lib.general import get_published_in\n",
    "\n",
    "set_scholarly_logging(False)\n",
    "cited_by_group = list(map(get_forward_references, input_dois))\n",
    "\n",
    "flat_map = lambda xs: reduce(lambda a, b: a + b, xs)\n",
    "forward_references = list(map(lambda x: x['bib'], flat_map(cited_by_group)))\n",
    "\n",
    "forward_references_df = pd.DataFrame(forward_references)\n",
    "forward_references_df = forward_references_df.loc[:, ['title', 'abstract', 'author', 'pub_year']]\n",
    "forward_references_df.columns = [TITLE, ABSTRACT, AUTHORS, PUBLISHED]\n",
    "forward_references_df[PUBLISHED_IN] = forward_references_df[TITLE].map(get_published_in)\n",
    "\n",
    "print()\n",
    "print('loaded ' + str(len(forward_references)) + ' forward reference entries')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finish forward snowballing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_before_filtering = len(forward_references_df.index)\n",
    "\n",
    "forward_references_df = forward_references_df[forward_references_df[TITLE].str.match('.*(test|mutant|mutation|coverage).*')==True]\n",
    "forward_references_df.drop_duplicates(subset=[TITLE], inplace=True)\n",
    "\n",
    "forward_references_df = forward_references_df.sort_values(TITLE)\n",
    "forward_references_df.to_csv('intermediate/forward.csv', index = False, header=True)\n",
    "\n",
    "number_after_filtering = len(forward_references_df.index)\n",
    "\n",
    "print()\n",
    "print(str(number_after_filtering) + ' studies remain after filtering the forward references (' + str(number_before_filtering) + ')')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.general import drop_duplicates_normalized\n",
    "\n",
    "backward_pd = pd.read_csv('intermediate/backward.csv')\n",
    "backward_pd[SOURCE] = 'backward'\n",
    "\n",
    "forward_pd = pd.read_csv('intermediate/forward.csv')\n",
    "forward_pd = forward_pd.loc[:, [TITLE, AUTHORS, PUBLISHED, PUBLISHED_IN]]\n",
    "forward_pd[SOURCE] = 'forward'\n",
    "\n",
    "all_stuides_pd = pd.read_csv('../all_studies.csv')\n",
    "all_stuides_pd = all_stuides_pd.loc[:, ['Title', 'Author', 'Year']]\n",
    "all_stuides_pd.columns = [TITLE, AUTHORS, PUBLISHED]\n",
    "all_stuides_pd[SOURCE] = 'original'\n",
    "\n",
    "result_pd = backward_pd.append(forward_pd)\n",
    "result_pd = result_pd.append(all_stuides_pd)\n",
    "\n",
    "drop_duplicates_normalized(result_pd, TITLE)\n",
    "result_pd = result_pd[result_pd[SOURCE].str.match('.*(backward|forward).*')==True]\n",
    "\n",
    "number_result = len(result_pd.index)\n",
    "\n",
    "print()\n",
    "print(str(number_result) + ' studies have been found after merging forward and backward snowballing')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter Abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.general import resolve_doi\n",
    "\n",
    "abstracts_pd = pd.read_csv('/home/stefan/Desktop/jabref/abstracts.csv')\n",
    "abstracts_dois = abstracts_pd.to_dict()['DOI'];\n",
    "abstracts_texts = abstracts_pd.to_dict()['Abstract'];\n",
    "\n",
    "abstracts = {}\n",
    "for i in range(0, len(abstracts_dois)):\n",
    "    doi = abstracts_dois[i]\n",
    "    text = abstracts_texts[i]\n",
    "    abstracts[doi] = text\n",
    "\n",
    "def get_abstract(doi):\n",
    "    if doi not in abstracts:\n",
    "        return ''\n",
    "    return abstracts[doi]\n",
    "\n",
    "result_pd[RESOLVED_DOI] = result_pd[TITLE].map(resolve_doi)\n",
    "result_pd[ABSTRACT] = result_pd[RESOLVED_DOI].map(get_abstract)\n",
    "\n",
    "manuel_abstracts_pd = pd.read_csv('intermediate/manuel_abstract.csv')\n",
    "manuel_abstracts_pd = manuel_abstracts_pd.loc[:, [TITLE, ABSTRACT]]\n",
    "manuel_abstracts = manuel_abstracts_pd.to_dict(orient='records')\n",
    "def find_abstract(title):\n",
    "    if title is None:\n",
    "        return ''\n",
    "    records = list(filter(lambda r: r[TITLE] == title, manuel_abstracts))\n",
    "    if len(records) < 1 or records[0][ABSTRACT] is np.NaN:\n",
    "        return ''\n",
    "    return records[0][ABSTRACT]\n",
    "def take_abstract(entry):\n",
    "    if entry[ABSTRACT] == '' or entry[ABSTRACT] is None:\n",
    "        return find_abstract(entry[TITLE])\n",
    "    return entry[ABSTRACT]\n",
    "result_pd[ABSTRACT] = result_pd.apply(take_abstract, axis=1)\n",
    "\n",
    "without_abstract_pd = result_pd[result_pd[ABSTRACT] == '']\n",
    "without_abstract_pd['Skip'] = ''\n",
    "without_abstract_pd = without_abstract_pd.loc[:, [TITLE, AUTHORS, 'Skip', ABSTRACT]]\n",
    "number_without_abstract = len(without_abstract_pd)\n",
    "\n",
    "without_abstract_pd.to_csv('intermediate/without_abstract.csv', index = False, header=True)\n",
    "\n",
    "number_before_abstract_check = len(result_pd)\n",
    "result_pd = result_pd[result_pd[ABSTRACT].str.match('.*(test suite).*')==True]\n",
    "number_after_abstract_check = len(result_pd)\n",
    "\n",
    "print()\n",
    "print(str(number_after_abstract_check) + ' studies remain after abstract check (' + str(number_before_abstract_check) + ')')\n",
    "print('for ' + str(number_without_abstract) + ' studies no abstract has been found')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter For Credible Publications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "RANKS = ['A', 'A*']\n",
    "\n",
    "core_pd = pd.read_csv('core-sources/CORE.csv', header=None, usecols=[0,1,2,3,4,5,6,7])\n",
    "core_pd = core_pd.loc[:, [1, 2, 4]]\n",
    "core_pd.columns = [TITLE, ABBREVATION, RANK]\n",
    "core_conferences = core_pd.to_dict(orient='records')\n",
    "core_conferences = list(filter(lambda c: c[RANK] in RANKS, core_conferences))\n",
    "core_conferences_abbrevations = list(map(lambda c: c[ABBREVATION], core_conferences))\n",
    "\n",
    "core_journals_pd = pd.read_csv('core-sources/CORE_journals.csv')\n",
    "core_journals_pd = core_journals_pd.loc[:, ['title', 'rank']]\n",
    "core_journals_pd.columns = [TITLE, RANK]\n",
    "core_journals = core_journals_pd.to_dict(orient='records')\n",
    "core_journals = list(filter(lambda c: c[RANK] in RANKS, core_journals))\n",
    "core_journals_title = list(map(lambda c: c[TITLE], core_journals))\n",
    "\n",
    "def is_in_core(published_in):\n",
    "    if type(published_in) != str:\n",
    "        return False \n",
    "    matching_conferences = list(filter(lambda a: a in published_in, core_conferences_abbrevations))\n",
    "    if len(matching_conferences) > 0:\n",
    "        return True\n",
    "    matching_titles = list(filter(lambda t: t == published_in, core_journals_title))\n",
    "    if len(matching_titles) > 0:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "number_before_cred_check = len(result_pd)\n",
    "result_pd = result_pd[result_pd[PUBLISHED_IN].map(is_in_core)]\n",
    "number_after_cred_check = len(result_pd)\n",
    "\n",
    "print()\n",
    "print(str(number_after_cred_check) + ' studies remain after credibility check (' + str(number_before_cred_check) + ')')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Store Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_pd = result_pd.sort_values(TITLE)\n",
    "\n",
    "result_pd = result_pd.loc[:, [TITLE, PUBLISHED_IN, ABSTRACT]]\n",
    "result_pd.to_csv('result.csv', index = False, header=True)\n",
    "\n",
    "number_result = len(result_pd.index)\n",
    "\n",
    "print()\n",
    "print(str(number_result) + ' studies have been found through snowballing')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "INDEX = 'Index'\n",
    "\n",
    "result_selected_pd = pd.read_csv('result_checked.csv')\n",
    "\n",
    "number_of_studies = len(result_selected_pd.index)\n",
    "result_selected_pd = result_selected_pd[result_selected_pd['USE'] == True]\n",
    "number_of_selected_studies = len(result_selected_pd.index)\n",
    "\n",
    "result_selected_pd[INDEX] = result_selected_pd.index.map(lambda i: 'snow' + str(i))\n",
    "result_selected_pd = result_selected_pd.loc[:, [INDEX, TITLE, PUBLISHED_IN, ABSTRACT]]\n",
    "\n",
    "result_selected_pd.to_csv('result_final.csv', index=False, header=True)\n",
    "\n",
    "print(str(number_of_selected_studies) + ' studies remain after snowballing manual check (' + str(number_of_studies) + ')')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  },
  "metadata": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
