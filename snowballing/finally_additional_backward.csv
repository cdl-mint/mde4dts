;title;abstract;dc:publisher;prism:aggregationType;prism:publicationName;prism:coverDate;prism:url
0;Knowledge-based lifecycle management approach for product service systems (PSS);Product Service Systems (PSS) are new industrial offerings which integrate a product and related services into a customer-specific bundle and offer the customer an overall solution to a specific issue. The lifecycle of PSS is characterized by merged product and service structures and a close interaction among the provider, the suppliers, and the customer, not only in the development phase but especially during the operation of PSS. Therefore a lifecycle management approach for PSS has to merge both the virtual and real (operational) lifecycle of PSS. To reach that goal operational knowledge about a PSS instance must be gathered, aggregated, managed, and used by enhanced lifecycle management methods in order to support providers, suppliers, and customers by developing, delivering and operating PSS. The paper in hand presents a knowledge-based lifecycle management approach for PSS, whichconsiders an ontological representation of PSS knowledge and 3 knowledgebased lifecycle methods to make engineering processes adaptive, to provide the involved actors with the appropriate PSS knowledge and support the stakeholders in their decision making processes. The components of this approach have been identified as key lifecycle management methods for managing PSS. They are still being developed within the collaborative research center TR29 funded by the German Research Foundation (DFG), who investigate the overall engineering of PSS.;Springer New York LLC;Book Series;IFIP Advances in Information and Communication Technology;2013-01-01;https://api.elsevier.com/content/abstract/scopus_id/84925063833
1;Unified IoT ontology to enable interoperability and federation of testbeds;After a thorough analysis of existing Internet of Things (IoT) related ontologies, in this paper we propose a solution that aims to achieve semantic interoperability among heterogeneous testbeds. Our model is framed within the EU H2020's FIESTA-IoT project, that aims to seamlessly support the federation of testbeds through the usage of semantic-based technologies. Our proposed model (ontology) takes inspiration from the well-known Noy et al. methodology for reusing and interconnecting existing ontologies. To build the ontology, we leverage a number of core concepts from various mainstream ontologies and taxonomies, such as Semantic Sensor Network (SSN), M3-lite (a lite version of M3 and also an outcome of this study), WGS84, IoT-lite, Time, and DUL. In addition, we also introduce a set of tools that aims to help external testbeds adapt their respective datasets to the developed ontology.;Institute of Electrical and Electronics Engineers Inc.;Conference Proceeding;2016 IEEE 3rd World Forum on Internet of Things, WF-IoT 2016;2017-02-06;https://api.elsevier.com/content/abstract/scopus_id/85015182893
2;20 Years of the Agile Manifesto: A Literature Review on Agile Project Management;Agile Project Management is a topic that has become popular both in business and academia, since the publication of the Agile Manifesto – a historic landmark in this subject. In the next 20 years, there was a relevant scientific production that must be analyzed to provoke reflection about the knowledge built up in this period. In this sense, this study aims to analyze the relevant scientific literature on Agile Project Management through a systematic review and a bibliometric analysis of articles published in scientific journals with Digital Object Identifier, in English, from the Web of Science and Scopus databases, from 2001 to 2021. The research results enable us to gain insights into the characteristics of this knowledge domain, regarding its volume and evolutionary trend, main contributors (i.e. scientific journals, authors, and their affiliations), main studies, methods used, and its central thematic axes.;Polska Akademia Nauk;Journal;Management and Production Engineering Review;2023-06-01;https://api.elsevier.com/content/abstract/scopus_id/85167422957
3;Framework for the agile development of innovative Product-Service-Systems for existing physical rehabilitation systems;Smart Health means innovation of the health sector through digitization. This leads to many possibilities regarding to an effective home-based, self-managed physical rehabilitation. With an innovative Product-Service-System (PSS), patients will have a better access, availability, and affordability of physical rehabilitation. In this contribution we present generic approaches of upgrading existent systems from a technological perspective. Furthermore, we introduce a framework for the agile development of innovative PSS, which could be adapted to other fields. The initial situation is an existing PSS for physical rehabilitation. The framework covers four phases. The planning phase consists of methods from the value based idea creation to Systems Engineering (SE) approaches, which focus on the conception of a Minimum Viable Product (MVP). The second phase describes a fast prototyping of the previously defined MVP, which addresses the PSS concept and the business model. Based on that, the next phase consists of the validation of the developed prototype and business model by real customers. The result is analyzed in the fourth stage. The learnings are interpreted regarding to the real market demand for the developed PSS. The developed PSS will be improved or completely changed through multiple iterations.;Elsevier B.V.;Conference Proceeding;Procedia Manufacturing;2018-01-01;https://api.elsevier.com/content/abstract/scopus_id/85050376141
4;Design science research contributions: Finding a balance between artifact and theory;With the rising interest in Design Science Research (DSR), it is crucial to engage in the ongoing debate on what constitutes an acceptable contribution for publishing DSR - the design artifact, the design theory, or both. In this editorial, we provide some constructive guidance across different positioning statements with actionable recommendations for DSR authors and reviewers. We expect this editorial to serve as a foundational step towards clarifying misconceptions about DSR contributions and to pave the way for the acceptance of more DSR papers to top IS journals.;Association for Information Systems;Journal;Journal of the Association for Information Systems;2018-01-01;https://api.elsevier.com/content/abstract/scopus_id/85048321087
5;Automated provenance graphs for models@run.time;Software systems are increasingly making decisions autonomously by incorporating AI and machine learning capabilities. These systems are known as self-adaptive and autonomous systems (SAS). Some of these decisions can have a life-changing impact on the people involved and therefore, they need to be appropriately tracked and justified: the system should not be taken as a black box. It is required to be able to have knowledge about past events and records of history of the decision making. However, tracking everything that was going on in the system at the time a decision was made may be unfeasible, due to resource constraints and complexity. In this paper, we propose an approach that combines the abstraction and reasoning support offered by models used at runtime with provenance graphs that capture the key decisions made by a system through its execution. Provenance graphs relate the entities, actors and activities that take place in the system over time, allowing for tracing the reasons why the system reached its current state. We introduce activity scopes, which highlight the high-level activities taking place for each decision, and reduce the cost of instrumenting a system to automatically produce provenance graphs of these decisions. We demonstrate a proof of concept implementation of our proposal across two case studies, and present a roadmap towards a reusable provenance layer based on the experiments.;Association for Computing Machinery, Inc;Conference Proceeding;Proceedings - 23rd ACM/IEEE International Conference on Model Driven Engineering Languages and Systems, MODELS-C 2020 - Companion Proceedings;2020-10-16;https://api.elsevier.com/content/abstract/scopus_id/85096778397
6;Conceptualizing smart service systems;Recent years have seen the emergence of physical products that are digitally networked with other products and with information systems to enable complex business scenarios in manufacturing, mobility, or healthcare. These “smart products”, which enable the co-creation of “smart service” that is based on monitoring, optimization, remote control, and autonomous adaptation of products, profoundly transform service systems into what we call “smart service systems”. In a multi-method study that includes conceptual research and qualitative data from in-depth interviews, we conceptualize “smart service” and “smart service systems” based on using smart products as boundary objects that integrate service consumers’ and service providers’ resources and activities. Smart products allow both actors to retrieve and to analyze aggregated field evidence and to adapt service systems based on contextual data. We discuss the implications that the introduction of smart service systems have for foundational concepts of service science and conclude that smart service systems are characterized by technology-mediated, continuous, and routinized interactions.;Springer Verlagservice@springer.de;Journal;Electronic Markets;2019-03-12;https://api.elsevier.com/content/abstract/scopus_id/85034269569
7;Towards Interoperable Metamodeling Platforms: The Case of Bridging ADOxx and EMF;Metamodeling platforms are an important cornerstone for building domain-specific modeling languages in an efficient and effective way. Two prominent players in the field are ADOxx and the Eclipse Modeling Framework (EMF) which both provide rich ecosystems on modeling support and related technologies. However, until now, these two worlds live in isolation while there would be several benefits of having a bridge to exchange metamodels and models for different purposes (e.g., reuse of features and plugins that are only available on one platform, access to additional modeler and developer communities). Therefore, in this paper, we propose first steps toward establishing interoperability between ADOxx and EMF. For this, we thoroughly analyze the metamodeling concepts employed by both platforms before proposing a bridge that enables bidirectional exchange of metamodels. We evaluate the bidirectional bridge with several openly available metamodels created with ADOxx and EMF, respectively. Moreover, we quantitatively and qualitatively analyze the bridge by an evaluation that incorporates the instantiation and use of the metamodels on both platforms. We show that the metamodels can be exchanged without information loss and similar modeling experiences with respect to the resulting models can be achieved.;Springer Science and Business Media Deutschland GmbH;Book Series;Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics);2022-01-01;https://api.elsevier.com/content/abstract/scopus_id/85132711232
8;OLIVE, a Model-Aware Microservice Framework;In this paper we want to introduce OLIVE, a model-centric and low-code microservice framework, resulting from the lessons learned in five years of European projects. The requirements on those projects are summarized and used to extract the characteristics that a microservice framework should support in order to be aware of models. An implementation of the OLIVE framework has been proposed, focusing more on the concepts, and required models, instead that on the technical details and has been evaluated in each project with definition of the needed microservices. Microservice development using a low code approach is still an open research field and with OLIVE we want to provide an initial contribution relative to the dependencies between microservices and models, using ADOxx as reference meta-modelling platform, and proposing an initial modelling method for the definition of OLIVE microservices.;Springer Science and Business Media Deutschland GmbH;Book Series;Lecture Notes in Business Information Processing;2021-01-01;https://api.elsevier.com/content/abstract/scopus_id/85119888815
9;Using conceptual modeling to support innovation challenges in smart cities;The digital transformation of cities towards Smart Cities proposes many opportunities, e.g., related to services, security, waste and energy management, and infrastructure management. These opportunities come with manifold innovation challenges, not only from a technological perspective but also for public authorities and citizens. The paper at hand introduces conceptual modeling as a means towards handling the complexity of Smart City planning, management, and operation. This paper presents multiple scenarios, indicating possibilities of bridging between the challenges of Smart Cities on the one hand and the opportunities of applying conceptual modeling on the other. Evaluation results and experience gained from two years of teaching such scenarios at the Next-generation Enterprise Modeling Summer School (NEMO) indicate a strong positive impact on student's motivation and learning success. The aim of this paper is therefore to emphasize on the benefits of adopting conceptual modeling in Smart Cities by exemplifying possible application scenarios.;Institute of Electrical and Electronics Engineers Inc.;Conference Proceeding;Proceedings - 18th IEEE International Conference on High Performance Computing and Communications, 14th IEEE International Conference on Smart City and 2nd IEEE International Conference on Data Science and Systems, HPCC/SmartCity/DSS 2016;2017-01-20;https://api.elsevier.com/content/abstract/scopus_id/85013650544
10;An open platform for modeling method conceptualization: The OMiLAB digital ecosystem;This paper motivates, describes, demonstrates in use, and evaluates the Open Models Laboratory (OMiLAB)-an open digital ecosystem designed to help one conceptualize and operationalize conceptual modeling methods. The OMiLAB ecosystem, which a generalized understanding of “model value” motivates, targets research and education stakeholders who fulfill various roles in a modeling method’s lifecycle. While we have many reports on novel modeling methods and tools for various domains, we lack knowledge on conceptualizing such methods via a full-fledged dedicated open ecosystem and a methodology that facilitates entry points for novices and an open innovation space for experienced stakeholders. This gap continues due to the lack of an open process and platform for 1) conducting research in the field of modeling method design, 2) developing agile modeling tools and model-driven digital products, and 3) experimenting with and disseminating such methods and related prototypes. OMiLAB incorporates principles, practices, procedures, tools, and services required to address the issues above since it focuses on being the operational deployment for a conceptualization and operationalization process built on several pillars: 1) a granularly defined “modeling method” concept whose building blocks one can customize for the domain of choice, 2) an “agile modeling method engineering” framework that helps one quickly prototype modeling tools, 3) a model-aware “digital product design lab”, and 4) dissemination channels for reaching a global community. In this paper, we demonstrate and evaluate the OMiLAB in research with two selected application cases for domain- and case-specific requirements. Besides these exemplary cases, OMiLAB has proven to effectively satisfy requirements that almost 50 modeling methods raise and, thus, to support researchers in designing novel modeling methods, developing tools, and disseminating outcomes. We also measured OMiLAB’s educational impact.;Association for Information Systemspublications@aisnet.org;Journal;Communications of the Association for Information Systems;2019-01-01;https://api.elsevier.com/content/abstract/scopus_id/85063641634
11;Framework to Model PSS Collaborative Value Networks and Assess Uncertainty of Their Economic Models;This paper presents a framework for addressing the challenge of economic value sharing among actors of Product-Service value networks. More specifically the framework is dedicated to the assessment of alternative collaborative value networks and their associated economic models, at the time of designing a product-service system (PSS). The framework includes three main components: Modelling, simulation and uncertainty assessment. The framework is briefly presented as parts of its components were discussed in previous research. The paper provides an illustration with a design project of a PSS solution in the agro-alimentary industry, requiring a balanced configuration of collaborative value network.;Springer New York LLCbarbara.b.bertram@gsk.com;Book Series;IFIP Advances in Information and Communication Technology;2019-01-01;https://api.elsevier.com/content/abstract/scopus_id/85072956421
12;Modeling product-service systems for the internet of things: The comvantage method;"This chapter provides an overview on the current state of the ComVantage modeling method implementation. The method addresses the domain of product-service systems that must run in an Internet of Things environment. This specificity manifests in several aspects: (i) the underlying business model is based on the provision of products, services or a mix of these (e.g., maintenance services attached to products with embedded systems); (ii) collaborative business processes must be supported by mobile apps that consume Linked Data; (iii) model contents must be available to model-aware run-time systems running in a Linked Data environment; (iv) modeling concepts must be linked to IoT resources and their properties, in order to provide a mash-up back-end knowledge base to Internet of Things applications. The method was initiated in a European FP7 research project, therefore project context will also be highlighted, to better outline the motivational frame.";Springer International Publishing;Book;Domain-Specific Conceptual Modeling: Concepts, Methods and Tools;2016-07-09;https://api.elsevier.com/content/abstract/scopus_id/85015734879
13;Model-aware software engineering a knowledge-based approach to model-driven software engineering;Standard modelling languages enabled the Model-Driven Software Engineering paradigm, allowing the development of model compilers for code generation. This, however, induces a subordination of implementation to the modelling language: the modelling benefits are confined to a fixed semantic space. On the other hand, the rise of agile software development practices has impacted model-driven engineering practices - an Agile Modelling paradigm was consequently introduced. This was later expanded towards the Agile Modelling Method Engineering (AMME) framework which generalizes agility at the modelling method level. By observing several AMME-driven implementation experiences, this paper specialises the notion of Model-Driven Software Engineering to that of Model-Aware Software Engineering – an approach that relies on modelling language evolution, in response to the evolution of the implemented system's requirements. The key benefit is that the modelling language-implementation dependency is reversed, as the implementation needs propagate requirements towards an agile modelling language.;SciTePress;Conference Proceeding;ENASE 2018 - Proceedings of the 13th International Conference on Evaluation of Novel Approaches to Software Engineering;2018-01-01;https://api.elsevier.com/content/abstract/scopus_id/85047728412
14;Product-service systems engineering: State of the art and research challenges;The design and development of a Product-Service System (PSS) raises new issues since the service component introduces further requirements than traditional product engineering. Compared to physical products, services are generally under-designed and inefficiently developed. For this reason, approaches such as New Service Development, Service Design and Service Engineering have emerged during the years to support the design and development of service either as a system itself or as a constituting element of a Product-Service System. However, only Service Engineering investigates service design and development with a systematic perspective and with a seamless integration of product and service contents. The purpose of this paper is to provide a holistic conceptualisation and an up-to-date review of the literature on Service Engineering with a specific focus on its adoption in the PSS context. A critical analysis is also performed with the aim to define a research agenda and the most prominent key actions that could give directions for future research. © 2012 Elsevier B.V. All rights reserved.;Elsevier B.V.;Journal;Computers in Industry;2012-01-01;https://api.elsevier.com/content/abstract/scopus_id/84860731929
15;A Modeling Method for Model-Driven API Management;This article reports on the Design Science engineering cycle foimplementing a modeling method to support model-driven, process-centric APmanagement. The BPMN standard was hereby enriched on semantic, syntactiand tool levels in order to provide a viable solution for integrating API requestwith diagrammatic business process models in order to facilitate thdocumentation or testing of REST API calls directly in a modeling environment. The method can be implemented by stakeholders that need to map and manage their API ecosystem, thus gaining more API management agility and improving their software engineering productivity. By assimilating APecosystem conceptualization in the modeling environment, the proposal differfrom both RPA (which typically employs non-BPMN process diagramming e.g., in UIPath) and BPM Systems (which typically isolate all API-related semantics outside the process modeling language to keep the diagrammatic representation standard-compliant).;Riga Technical University;Journal;Complex Systems Informatics and Modeling Quarterly;2020-01-01;https://api.elsevier.com/content/abstract/scopus_id/85118657155
16;Agile modelling method engineering: Lessons learned in the ComVantage research project;The paper reports on experiences accumulated during a EU research project where challenges pertaining to requirements-driven metamodelling agility have been analysed. Traditionally, modelling languages are perceived as stable artefacts – that is, if they address a sufficiently large community with fixed modelling requirements on a fixed layer of abstraction. However, the enterprise modelling community must also consider the case where evolving requirements emerge in a narrow domain, or even in a single enterprise, therefore reusability across domains will be sacrificed to the benefit of on-demand adaptation, specialization or integration. Under such conditions, an agile metamodelling approach was applied in the ComVantage project and this, in turn, raised specific requirements for conceptual and technological enablers, allowing us to derive conclusions that are generalized here beyond the project scope. The paper’s concluding SWOT analysis highlights the need to stimulate the emergence of an agile metamodelling paradigm based on community-driven enablers.;Springer Verlagservice@springer.de;Book Series;Lecture Notes in Business Information Processing;2015-01-01;https://api.elsevier.com/content/abstract/scopus_id/84952672210
17;Boosting servitization through digitization: Pathways and dynamic resource configurations for manufacturers;The importance of digital technologies for services in manufacturing has often been posited, but current literature has neglected to explain how companies can leverage digital methods to increase their service offering. In this article we contribute to current theory by examining how digitization can enable servitization for manufacturers. By performing a multiple-case study at four manufacturing SMEs, we provide evidence for a priming and a capability effect. In terms of priming, we find that specific digitization options lead to three servitization pathways: industrial, commercial and value servitization. Through a dynamic resource-based lens, the barriers, dynamic resource configurations and competitive benefits specific to each pathway are discussed. Finally, this paper offers managers insight on successfully reaching higher service levels through development of digital assets, and on the skills necessary to further integrate into customers’ processes.;Elsevier Inc.usjcs@elsevier.com;Journal;Industrial Marketing Management;2017-01-01;https://api.elsevier.com/content/abstract/scopus_id/84969960029
18;Interoperability assessment: A systematic literature review;The development of Interoperability is a necessity for organisations to achieve business goals and capture new market opportunities. Indeed, interoperability allows enterprises to exchange information and use it to seize their shared goals. Therefore, it should be verified and continuously improved. This is the main objective of the Interoperability Assessment (INAS). Indeed, such an assessment aims at determining the strengths and weakness of an enterprise in terms of interoperability. Many surveys and reviews have been proposed in the literature to analyse the existing INAS approaches. However, the majority of these reviews are focusing on specific properties rather than a general view of an INAS. Therefore, this paper proposes a systematic literature review of INAS approaches. The objectives are to identify the relevant INAS approaches and to compare them based on a holistic view based on their similar and different properties (e.g. type of assessment, the used measurement mechanism, and the addressed interoperability barriers). A bibliometric analysis of the selected INAS approaches is also conducted with a discussion of their advantages and limitations.;Elsevier B.V.;Journal;Computers in Industry;2019-04-01;https://api.elsevier.com/content/abstract/scopus_id/85060169989
19;Formalizing Conceptual Modeling Methods with MetaMorph;Models evolved from mere pictures supporting human understanding to sophisticated knowledge structures processable by machines. This entails an inevitable need for computer-understandable models and languages and causes formalization to be a crucial part in the lifecycle of a modeling method. An appropriate formalism must be a means for providing a unique, unambiguous but implementation-independent way of specifying arbitrary modeling languages and for this purpose must be generic and open to capture any domain and any functionality. In this paper we give a pervasive description of the formalism MetaMorph based on predicate logic – an approach fulfilling these requirements. This is done with an extensive proof-of-concept case illustrating the application of the formalism concept by concept. For the case study we use the modeling language ProVis from the domain of stochastic education. The language ProVis comprises only few objects and relation types but with high interconnection and therefore appears as a interesting specimen for formalization and showing the feasibility of the demonstrated approach.;Springer Science and Business Media Deutschland GmbH;Book Series;Lecture Notes in Business Information Processing;2021-01-01;https://api.elsevier.com/content/abstract/scopus_id/85111806088
20;Digital innovation as a fundamental and powerful concept in the information systems curriculum;The 50-year march of Moore's Law has led to the creation of a relatively cheap and increasingly easy-to-use world-wide digital infrastructure of computers, mobile devices, broadband network connections, and advanced application platforms. This digital infrastructure has, in turn, accelerated the emergence of new technologies that enable transformations in how we live and work, how companies organize, and the structure of entire industries. As a result, it has become important for all business students to have a strong grounding in IT and digital innovation in order to manage, lead, and transform organizations that are increasingly dependent on digital innovation. Yet, at many schools, students do not get such grounding because the required information systems core class is stuck in the past. We present a vision for a redesigned IS core class that adopts digital innovation as a fundamental and powerful concept (FPC). A good FPC serves as both a foundational concept and an organizing principle for a course. We espouse a particularly broad conceptualization of digital innovation that allows for a variety of teaching styles and topical emphases for the IS core class. This conceptualization includes three types of innovation (i.e., process, product, and business model innovation), and four stages for the overall innovation process (i.e., discovery, development, diffusion, and impact). Based on this conceptualization, we examine the implications of adopting digital innovation as an FPC. We also briefly discuss broader implications relating to (1) the IS curriculum beyond the core class, (2) the research agenda for the IS field, and (3) the identity and legitimacy of IS in business schools.;University of Minnesotadegro003@umn.edu;Journal;MIS Quarterly: Management Information Systems;2014-06-01;https://api.elsevier.com/content/abstract/scopus_id/84923459614
21;Long-term planning of wind and solar power considering the technology readiness level under China's decarbonization strategy;To address climate change, the Chinese government has committed to achieving carbon peaking by 2030. Projecting the wind power and photovoltaic installed capacity is essential for China's low carbon transition as these renewables have been widely recognized as the major energy sources in future. This study proposes a long-term strategic planning approach for wind power and photovoltaic by simulating multiple policies and market scenarios for the national-level energy transitions and incorporating the feedback effects of market development on technology readiness level. The proposed approach simulates the national energy consumption and energy mix under various transition scenarios based on the Long-range Energy Alternatives Planning System (LEAP). The dynamics of market development, technology readiness level, and future renewable costs are further incorporated into a nonlinear optimization model to generate economically optimal planning solutions for wind power and photovoltaic under different policy and market scenarios. Valuable insights are obtained from three perspectives, i.e., (1) the economic and emission reduction co-benefits under different scenarios, (2) the evolution of the Levelized Cost of Energy, and (3) the impact on the transition of power sector. In particular, the simulation and optimization results reveal that appropriate acceleration of wind power and photovoltaic development can promote technology readiness level, reduce overall transition costs, and effectively reduce the peak value of emissions. The maximum peak reduction in carbon dioxide emissions is 16.06%, and the maximum cumulative reduction in carbon dioxide emissions is 14.54%. The total project cost can be reduced by a maximum of 3.23%. Thus, more active supporting policies for renewables can further enhance long-term economic and environmental co-benefits under current conditions.;Elsevier Ltd;Journal;Applied Energy;2023-10-15;https://api.elsevier.com/content/abstract/scopus_id/85165085910
22;Distributing decision-making authority in manufacturing–review and roadmap for the factory of the future;The question of the benefits of autonomous control is more important than ever: production managers, governments and society hope that the vision of smart and digital production systems with high flexibility and low costs may save the value adding and therefore welfare in the high wage, industrialised countries. At the same time, the discussion on the social implications of autonomous objects and decentralised control approaches is growing. Looking back on the history of production research and practice, we find that there has been a constant ply among scholars and production managers between the advantages of the two concepts of centralised and decentralised control approaches. In this article, we study the concept of autonomy in production planning and control, enabled by cyber-physical systems and the distribution of decision-making authority. Based on a profound structured literature review, we analyse the perception of autonomy, the technological requirements and the increasing complexities of modern smart manufacturing. Moreover, we find that recently several research streams suggest the advantages and benefits of autonomous control concepts compared to traditional centralised approaches based on qualitative analysis and identify a distinct lack of quantitative results.;Taylor and Francis Ltd.;Journal;International Journal of Production Research;2022-01-01;https://api.elsevier.com/content/abstract/scopus_id/85130045702
23;Modelling Service-Oriented Systems and Cloud Services with Heraklit;Modern and next generation digital infrastructures are technically based on service oriented structures, cloud services, and other architectures that compose large systems from smaller subsystems. The composition of subsystems is particularly challenging, as the subsystems themselves may be represented in different languages, modelling methods, etc. It is quite challenging to precisely conceive, understand, and represent this kind of technology, in particular for a given level of abstraction. To capture refinement and abstraction principles, various forms of “technology stacks” and other semi-formal or natural language based on presentations have been suggested. Generally, useful concepts to compose such systems in a systematic way are even more rare. Heraklit provides means, principles, and unifying techniques to model and to analyze digital infrastructures. Heraklit integrates composition and hierarchies of subsystems, concrete and abstract data structures, as well as descriptions of behaviour. A distinguished set of means supports the modeler to express their ideas. The modeller is free to choose the level of abstraction, as well as the kind of composition. Heraklit integrates new concepts with tried and tested ones. Such a framework provides the foundation for a comprehensive Systems Mining as the next step after Process Mining.;Springer Science and Business Media Deutschland GmbH;Book Series;Communications in Computer and Information Science;2021-01-01;https://api.elsevier.com/content/abstract/scopus_id/85103501824
24;Knowledge blockchains: Applying blockchain technologies to enterprise modeling;Blockchains permit to store information in a tamper-resistant and irrevocable manner by reverting to distributed computing and cryptographic technologies. The primary purpose is to keep track of the ownership of tangible and intangible assets. In the paper at hand we apply these concepts and technologies to the domain of knowledge management. Based on the explication of knowledge in the form of enterprise models this permits the application of so-called knowledge proofs for a. enabling the transparent monitoring of knowledge evolution, b. tracking the provenance, ownership, and relationships of knowledge in an organization, c. establishing delegation schemes for knowledge management, and d. ensuring the existence of patterns in models via zero-knowledge proofs. To validate the technical feasibility of the approach a first technical implementation is described and applied to a fictitious use case.;IEEE Computer Society;Conference Proceeding;Proceedings of the Annual Hawaii International Conference on System Sciences;2018-01-01;https://api.elsevier.com/content/abstract/scopus_id/85060387218
25;Formalizing meta models with FDMM: The ADOXX case;This paper contains an extended and improved version of the FDMM formalism presented at ICEIS’2012. FDMM is a formalism to describe how meta models and models are defined in the ADOxx approach as used in the Open Models Initiative. It is based on set theory and first order logic statements. In this way, an exact description of ADOxx meta models and corresponding models can be provided. In the paper at hand we extend the description of the formalism by illustrating how the mathematical statements can be used to support the implementation on the ADOxx platform. For this purpose we show how the FDMM constructs are mapped to statements in the ADOxx Library Language (ALL). As an example of the approach, the formalism and the mapping to ALL are applied to a modeling language from the area of risk management.;Springer Verlagservice@springer.de;Book Series;Lecture Notes in Business Information Processing;2013-01-01;https://api.elsevier.com/content/abstract/scopus_id/84909580033
26;Advancing conceptual modeling education towards a generalized model value proposition;This paper proposes a teaching method and artifact for Conceptual Modeling education, motivated by a challenge in the authors’ university of bridging the gap between bachelor-level studies and research work on topics related to Conceptual Modeling. At bachelor-level, Conceptual Modeling is subordinated to Software Engineering or Business Process Management topics, making extensive use of available standards for graphical documentation purposes. However, at doctoral level and in project-based work, modeling methods must be scientifically framed within wider-scoped paradigms – e.g. Knowledge Management, Enterprise Modeling – or tailored for domain-specific scenarios. The teaching artifact presented in this paper is an example of an “agile modeling method” that can be iteratively evolved together with students through a metamodeling approach in support of a course flow that argues for a generalized model value proposition and modeling languages acting as “schema” that can be tailored and migrated to accommodate explicit requirements from any application domain.;Springer;Conference Proceeding;Lecture Notes in Information Systems and Organisation;2020-01-01;https://api.elsevier.com/content/abstract/scopus_id/85089673071
27;Open Innovation Laboratory for Rapid Realisation of Sensing, Smart and Sustainable Products: Motives, Concepts and Uses in Higher Education;Open Innovation is not a new concept and it is been actively used by different entities to cope with new challenges posed by the evolving society in business, science and education. However, for this last one seems to be poor documentation about how higher education institutions are dealing with it. It is evident that universities are applying concepts like Open Innovation Laboratories, however it is not clear the methodologies or resources they are using. Tecnologico de Monterrey recently created its own laboratory and in this article we present the motives, concepts and uses of it in the context of higher education. Different approaches are made, from the development of core competences concepts to the physical and virtual tools used in the lab. Two study cases are briefly presented in order to illuminate how external actors are collaborating with internal actors in an Open Innovation process.;Springer New York LLCbarbara.b.bertram@gsk.com;Book Series;IFIP Advances in Information and Communication Technology;2018-01-01;https://api.elsevier.com/content/abstract/scopus_id/85053790000
28;Semantic web methodologies, best practices and ontology engineering applied to Internet of Things;We discuss in this paper, semantic web methodologies, best practices and recommendations beyond the IERC Cluster Semantic Interoperability Best Practices and Recommendations (IERC AC4). The semantic web community designed best practices and methodologies which are unknown from the IoT community. In this paper, we synthesize and highlight the most relevant work regarding ontology methodologies, engineering, best practices and tools that could be applied to Internet of Things (IoT). To the best of our knowledge, this is the first work aiming at bridging such methodologies to the IoT community and go beyond the IERC AC4 cluster. This research is being applied to three uses cases: (1) the M3 framework assisting IoT developers in designing interoperable ontology-based IoT applications, (2) the FIESTA-IoT EU project encouraging semantic interoperability within IoT, and (3) a collaborative publication of legacy ontologies.;Institute of Electrical and Electronics Engineers Inc.;Conference Proceeding;IEEE World Forum on Internet of Things, WF-IoT 2015 - Proceedings;2015-01-01;https://api.elsevier.com/content/abstract/scopus_id/84964426026
29;A new paradigm for the continuous alignment of business and IT: Combining enterprise architecture modelling and enterprise ontology;The paper deals with Next Generation Enterprise Information Systems in the context of Enterprise Engineering. The continuous alignment of business and IT in a rapidly changing environment is a grand challenge for today's enterprises. The ability to react timeously to continuous and unexpected change is called agility and is an essential quality of the modern enterprise. Being agile has consequences for the engineering of enterprises and enterprise information systems. In this paper a new paradigm for next generation enterprise information systems is proposed, which shifts the development approach of model-driven engineering to continuous alignment of business and IT for the agile enterprise. It is based on a metamodelling approach, which supports both human-interpretable graphical enterprise architecture and machine-interpretable enterprise ontologies. Furthermore, next generation enterprise information systems are described, which embed modelling tools and algorithms for model analysis.;Elsevier;Journal;Computers in Industry;2016-06-01;https://api.elsevier.com/content/abstract/scopus_id/84940055540
30;Accelerated digital transformation: A framework for leading digital innovation and change;The pandemic has accelerated the need for organizations to embrace digital strategies for processes that are both internally as well as externally facing. The demand for innovation in how a firm serves customers, supports teams, and empowers employees has necessitated the adoption of digital solutions. In this chapter, the authors provide a framework for how organizational leaders can envision digital transformation in a way that is in alignment with business strategy. Digital innovation, while left to the IT enterprise in the past, is now elevated to the level of organizational strategy and can be instrumental to business competitiveness and continuity. Strategic digital transformation can help organizations build agility and resilience, capabilities that have proven to be essential in the face of unpredictable forces, such as those experienced during the COVID-19 pandemic.;IGI Global;Book;Leadership Strategies for the Hybrid Workforce: Best Practices for Fostering Employee Safety and Significance;2022-06-24;https://api.elsevier.com/content/abstract/scopus_id/85137003851
31;Development of product-service systems: Challenges and opportunities for the manufacturing firm;Product-Service Systems (PSS) raise interesting opportunities for the manufacturing firm as the function is provided to meet customer needs rather than the physical hardware itself. PSS offerings based on the manufacturer’s knowledge about the product and the technology can increase its status as problem-solver and solution-provider, reduce life cycle cost and produce high revenue. However, PSS including, e.g. hardware, services, software and electronics are efficient and competitive only if developed for the specific purpose with features such as easy to maintain, upgradeable, with built-in sensors for collecting in-use and service data, and easy to use. This changes the requirements on the manufacturing firm’s development process. Looking back historically, the last century gives an interesting changing landscape of the rationale for the product-development methods used in manufacturing firms. This article, based on the previous research in the product- and service-development fields, and on empirical results from studies at several manufacturing firms, looks into how the engineering work is affected by PSS and how it can be enhanced for PSS, especially in terms of required competencies and other capabilities. It results in recommendations for a new, functional product-development process.;Taylor and Francis Ltd.michael.wagreich@univie.ac.at;Journal;Journal of Engineering Design;2009-08-01;https://api.elsevier.com/content/abstract/scopus_id/85009863315
32;SemCheck: Checking constraints for multi-perspective modeling languages;Enterprises are complex and dynamic organizations that can hardly be understood from a single viewpoint. Enterprise modeling tackles this problem by providing multiple, specialized modeling languages, each designed for representing information about the enterprise from a given viewpoint. The OMiLAB initiative promotes the use of metamodeling to design such domain-specific languages and to provide them by an open repository to the community. In this chapter, we discuss how this metamodeling approach can be combined with the design of integrity constraints that span multiple modeling languages. We propose the services of the ConceptBase system as a constraint checker for modeling languages created by the ADOxx platform.;Springer International Publishing;Book;Domain-Specific Conceptual Modeling: Concepts, Methods and Tools;2016-07-09;https://api.elsevier.com/content/abstract/scopus_id/85026685548
33;Towards tool-supported situational roadmap development for business process improvement;"In times of high market transparency and rapidly changing customer requirements, business process improvement (BPI) is becoming ever more important for companies to reach strategic objectives and stay competitive. However, existing BPI approaches such as Six Sigma or Total Quality Management are increasingly perceived as overly complex and overdimensioned by employees. Therefore, we propose ""tool-supported situational roadmap development for BPI"" as an instrument for arriving at enterpriseadapted and easy-to-use approaches that can be applied straight away. In this way, employees with limited knowledge in the BPI discipline are enabled to design BPI approaches to match their particular needs. This paper presents a first concept of our solution.";GITO Verlag;Conference Proceeding;"Proceedings of the 15th International Conference on Business Information Systems 2020 ""Developments, Opportunities and Challenges of Digitization"", WIRTSCHAFTSINFORMATIK 2020";2020-01-01;https://api.elsevier.com/content/abstract/scopus_id/85101707147
34;Conceptual Modelling Methods: The AMME Agile Engineering Approach;"Current research in fields such as Business Process Management, Enterprise Architecture Management, Knowledge Management and Software Engineering raises a wide diversity of requirements for Conceptual Modelling, typically satisfied by Design Science artefacts such as modelling methods. When employed in the context of an Agile Enterprise, an underlying requirement for Conceptual Modelling agility emerges-manifested not only on model content level but also on modelling method level. Depending on the questions that must be answered and the systems that must be supported with modellingmeans, the need for agility may stem from the degree of domain-specificity, from gradual understanding of modelling possibilities, from evolving model-driven systems, etc. The hereby proposed Agile Modelling Method Engineering (AMME) approach thus becomes necessary to extend the traditional perspective of “modelling through standards”; consequently, the benefits of repeatability and wide adoption are traded for responsiveness to dynamic needs identified within an Agile Enterprise.";Springer International Publishing;Book;Domain-Specific Conceptual Modeling: Concepts, Methods and ADOxx Tools;2022-01-01;https://api.elsevier.com/content/abstract/scopus_id/85134977072
35;Metamodels as a Conceptual Structure: Some Semantical and Syntactical Operations;Modern enterprises are under permanent pressure for change to cope with new competitors and integrate emerging technologies. These changes involve adaptation of processes, operations, architectures, value propositions, and the response to evolving market requirements-especially regarding the digitalization. Modelling methods are an established approach for the conceptual representation, design, analysis, and implementation of complex systems and have gained much attention in both, academia and industry. At the core of a modelling method is the metamodel as a formalized specification of the syntactic nature of the domain under consideration. The paper at hand amplifies the notion of metamodels toward “metamodels as a conceptual structure” and introduces semantic and syntactic operations applied to this structure. Based on recent advances in the field, this paper shows, how metamodels as a conceptual structure facilitate managing complexity in fast changing environments.;Springer International Publishing;Book;The Art of Structuring: Bridging the Gap between Information Systems Research and Practice;2019-01-01;https://api.elsevier.com/content/abstract/scopus_id/85141263291
36;Linked Open Models: Extending Linked Open Data with conceptual model information;"As the uptake of the Semantic Web vision has been relatively slow, a strategy based on pragmatic steps is being deployed in order to setup enablers and to stimulate acceptance. ""Linked Open Data"" refers to one of these early steps, benefiting from an available technological space (RDF, HTTP). The paper proposes ""Linked Open Models"" as a possible additional step, whose aim is to enable users to externalize knowledge in the form of diagrammatic models - a type of content that is human-readable, as well as linkable in the way promoted by the Linked Data paradigm. Consequently, diagrams become user-generated content that semantically enriches Linked Data, thus allowing richer constraints or connections in queries. The vision emerged from the context and use cases provided by the ComVantage FP7 research project, where linking benefits for conceptual diagrammatic models have been investigated. However the paper also discusses the vision's degree of generality, beyond the scope of the exemplary project use cases. Feasibility was demonstrated with a vocabulary and a prototype mechanism for exposing the models created with a hybrid, domain-specific modeling method in a Linked Data-driven collaboration environment.";Elsevier Ltd;Journal;Information Systems;2016-03-01;https://api.elsevier.com/content/abstract/scopus_id/84945937726
37;OMiLAB: A Smart Innovation Environment for Digital Engineers;This position paper introduces a Smart Innovation Environment for experimentation related to digital transformation projects, for the consolidation of a proposed “Digital Engineer” skill profile (with a business-oriented facet labelled as “Digital Innovator”). In the Internet of Things era, this profile implies the ability to perform both digital design and engineering activities, to semantically bridge multiple layers of abstraction and specificity – from business analysis down to cyber-physical engineering. In the paper’s proposal, this integration is enabled by conceptual modelling methods and interoperable modelling tools, tailored to support the creation of Digital Twins for innovative digital business models. The architecture of the proposed environment is guided by a Design Research perspective – i.e., it is a treatment to an education “design problem” regarding the Digital Engineer skill profile in the IoT era. The proposed environment encompasses workspaces and toolkits are currently evaluated in “innovation corners” deployed across the OMiLAB ecosystem.;Springer Science and Business Media Deutschland GmbH;Book Series;IFIP Advances in Information and Communication Technology;2020-01-01;https://api.elsevier.com/content/abstract/scopus_id/85097444837
38;Fundamental conceptual modeling languages in OMiLAB;Regardless of the application domain, both the analysis of existing systems and the creation of new systems benefit extensively from having the system modeled from a conceptual point of view in order to capture its behavioral, structural or semantic characteristics, while abstracting away irrelevant details. Depending on which relevant details are assimilated in the modeling language, modeling tools may support different degrees of domain-specificity. The boundaries of what domain-specific means are as ambiguous as the definition of a domain-it may be a business sector, a paradigm, or a narrow application area. However, some patterns and invariants are recurring across domains and this has led to the emergence of commonly used modeling languages that incorporate such fundamental concepts. This chapter focuses on the metamodeling approach for the hybridization of BPMN, ER, EPC, UML and Petri Nets within a single modeling method identified as FCML, with a proof of concept named Bee-Up implemented in OMiLAB.;Springer International Publishing;Book;Domain-Specific Conceptual Modeling: Concepts, Methods and Tools;2016-07-09;https://api.elsevier.com/content/abstract/scopus_id/85023161970
39;A metamodeling approach to support the engineering of modeling method requirements;The notion of 'modeling method requirements' refers to a category typically neglected by RE taxonomies and frameworks - i.e., those requirements that motivate the realization of (conceptual) modeling methods and tools. They can be considered domain-specific, in the sense that all modeling methods provide a knowledge schema for some selected application domain (narrow or broad). Besides this inherent domain-specific nature, we are investigating how the characteristics of modeling methods inform the RE perspective, and how in turn RE can support the engineering of such artifacts. Thus, the work at hand aims to raise awareness about modeling method requirements in the RE community. The core contribution is the CoChaCo (Concept-Characteristic-Connector) method for the representation and management of such requirements, as well as for streamlining with subsequent engineering phases. CoChaCo is itself a modeling method - i.e., it achieves its goals through diagrammatic modeling means for which a supporting tool was prototyped and evolved. The proposal originates in required support for the initial phase of the Agile Modeling Method Engineering (AMME) methodology, which was successfully applied in developing a variety of project-specific modeling tools. From this accumulated experience, awareness of 'modeling method requirements' emerged and informed the design decisions of CoChaCo.;IEEE Computer Societyhelp@computer.org;Conference Proceeding;Proceedings of the IEEE International Conference on Requirements Engineering;2019-09-01;https://api.elsevier.com/content/abstract/scopus_id/85076927783
40;MetaMorph: formalization of domain-specific conceptual modeling methods—an evaluative case study, juxtaposition and empirical assessment;Models have evolved from mere pictures supporting human understanding and communication to sophisticated knowledge structures processable by machines and establish value through their processing capabilities. This entails an inevitable need for computer-understandable modeling languages and causes formalization to be a crucial part in the lifecycle of engineering a modeling method. An appropriate formalism must be a means for providing a structural definition to enable a theoretical investigation of conceptual modeling languages and a unique, unambiguous way of specifying the syntax and semantics of an arbitrary modeling language. For this purpose, it must be generic and open to capturing any domain and any functionality. This paper provides a pervasive description of the formalism MetaMorph based on logic and model theory—an approach fulfilling the requirements above for modeling method engineering. The evaluation of the formalism is presented following three streams of work: First, two evaluative case studies illustrate the applicability of MetaMorph formalism concept by concept on the modeling language ProVis from the domain of stochastic education and the well-known Entity-Relationship language. ProVis as well as ER comprise only a few objects and relation types but with high interconnection and expressive power and are therefore considered interesting specimens for formalization. Second, a comprehensive juxtaposition of MetaMorph to three other formalization approaches based on different foundational theories is outlined concept by concept to underpin the formalism design. Third, an empirical evaluation has been performed, assessing the usability and adequacy of the formalism within a classroom assessment. The results allow for conclusions on the completeness, intuitiveness, and complexity as well as on interdependencies with engineers’ skills.;Springer Science and Business Media Deutschland GmbH;Journal;Software and Systems Modeling;2023-02-01;https://api.elsevier.com/content/abstract/scopus_id/85139486988
41;Visual design thinking: Understanding the role of knowledge visualization in the design thinking process;This paper sheds light on the role of visualization methods within the design thinking process. It provides a conceptual framework showing illustrative examples of visualization for each phase indicating its functions and benefits. Based on a thorough understanding of the functions and benefits of knowledge visualization in general, this paper seeks to provide an overview of the use of visualization in design thinking informing and supporting practitioners and researcher for more conscious selections of visualization methods in their design thinking efforts.;Institute of Electrical and Electronics Engineers Inc.;Conference Proceeding;Information Visualisation - Biomedical Visualization, Visualisation on Built and Rural Environments and Geometric Modelling and Imaging, IV 2018;2018-12-05;https://api.elsevier.com/content/abstract/scopus_id/85060134758
42;Model-driven digital twin construction: Synthesizing the integration of cyber-physical systems with their information systems;Digital twins emerge in many disciplines to support engineering, monitoring, controlling, and optimizing cyber-physical systems, such as airplanes, cars, factories, medical devices, or ships. There is an increasing demand to create digital twins as representation of cyber-physical systems and their related models, data traces, aggregated data, and services. Despite a plethora of digital twin applications, there are very few systematic methods to facilitate the modeling of digital twins for a given cyber-physical system. Existing methods focus only on the construction of specific digital twin models and do not consider the integration of these models with the observed cyber-physical system. To mitigate this, we present a fully model-driven method to describe the software of the cyber-physical system, its digital twin information system, and their integration. The integration method relies on MontiArc models of the cyber-physical system's architecture and on UML/P class diagrams from which the digital twin information system is generated. We show the practical application and feasibility of our method on an IoT case study. Explicitly modeling the integration of digital twins and cyber-physical systems eliminates repetitive programming activities and can foster the systematic engineering of digital twins.;Association for Computing Machinery, Inc;Conference Proceeding;Proceedings - 23rd ACM/IEEE International Conference on Model Driven Engineering Languages and Systems, MODELS 2020;2020-10-16;https://api.elsevier.com/content/abstract/scopus_id/85096992990
43;Definitions of Agile Software Development and Agility;"The Agile Manifesto and Agile Principles are typically referred to as the definitions of ""agile"" and ""agility"". However, many other definitions exist in the literature. Thus the different definitions provide interesting source for research. For each definition we examine where their emphasis is and compare that to the emphases found in the Agile Principles. © Springer-Verlag Berlin Heidelberg 2013.";Springer Verlagservice@springer.de;Book Series;Communications in Computer and Information Science;2013-01-01;https://api.elsevier.com/content/abstract/scopus_id/84904697442
44;Specifying a modelling language for PSS Engineering – A development method and an operational tool;Although the literature is full of research on the transition of industry towards Product-Service Systems (PSS), the question of how to effectively support PSS engineering is poorly addressed. The compelling need for decision support throughout the various stages of the engineering process is particularly challenging due to the inherent complexity of PSS. In this sense, visualisation and modelling at large have been put forth as promising means for supporting PSS engineering. This paper proposes a method for specifying a modelling language for PSS engineering, putting together PSS domain-specific knowledge and modelling concepts inherited from conceptual modelling and model-based engineering. It relies on a recursive transformation process of the underlying PSS meta-model using knowledge from case studies and the literature. The method has proven to be a practical means for gradual enrichment of the modelling language leading to successful experimentations in the industrial context.;Elsevier B.V.;Journal;Computers in Industry;2019-06-01;https://api.elsevier.com/content/abstract/scopus_id/85062460785
45;Transforming haptic storyboards into diagrammatic models: The Scene2Model tool;Haptic storyboarding tools supporting storytelling as a Design Thinking approach, enable early exploration and validation of design alternatives regarding services, new product (features), innovative processes and disruptive business models. They do however not communicate the exact meaning and the importance of each object nor do they show relationships between them. Yet when aiming to materialize an innovative idea these aspects need to be unambiguously described. Diagrammatic models play an essential role here as they capture different aspects of the problem. When computed by means of software they also explicitly show details which in haptic storyboards users implicitly fill with their own world-understanding, thus fostering a clear and transparent representation of the problem space. In addition, diagrammatic models can be enriched by semantics and subsequently be machine-queried, -analysed and -processed. The paper at hand shows the DIGITRANS project approach for an automated transformation of haptic storyboards into diagrammatic models and their provision in a computer-aided design environment.;IEEE Computer Society;Conference Proceeding;Proceedings of the Annual Hawaii International Conference on System Sciences;2019-01-01;https://api.elsevier.com/content/abstract/scopus_id/85073749525
46;Does the construction of smart cities make cities green? Evidence from a quasi-natural experiment in China;This paper focuses on whether the construction of smart cities can expand green space. Based on smart cities policy in China, a quasi-natural experiment is conducted by using the Heterogeneity - Robust estimator in differences-in-differences model. According to detailed theoretical analysis and rigorous empirical research, and main conclusions are as follows. First, the construction of smart cities can directly expand the green space due to the land resource allocation. The above basic conclusion remains reliable after a series of robustness tests. Second, the effect of construction of smart cities on the green space is heterogenous. The construction of smart city in the three batches has a positive impact on expending green space and the strongest positive effect on green space is seen in the smart cities established in 2012. The high-level economic scale and high-level science and technology are vital premises for smart cities to expand green space. Third, the construction of smart cities can indirectly expand the green space by promoting online work and services and increasing disposable income. Last, the “Broadband China” Strategy has a positively moderate effect on the construction of smart cities in expanding green space.;Elsevier Ltd;Journal;Cities;2023-09-01;https://api.elsevier.com/content/abstract/scopus_id/85161695165
47;Memorandum on design-oriented information systems research;"Information Systems Research (""Wirtschaftsinformatik"") basically follows two research approaches: the behavioristic approach and the design-oriented approach. In this memorandum, 10 authors propose principles of design-oriented information systems research. Moreover, the memorandum is supported by 111 full professors from the German-speaking scientific community, who with their signature advocate the principles specified therein. © 2011 Operational Research Society Ltd. All rights reserved.";Palgrave Macmillan Ltd.michael.wagreich@univie.ac.at;Journal;European Journal of Information Systems;2011-01-01;https://api.elsevier.com/content/abstract/scopus_id/78651335194
48;Improving agility through enterprise architecture management: The mediating role of aligning business and IT;The economic environment of contemporary organizations is becoming increasingly dynamic. Organizational agility fosters sustainable competitive advantage under these turbulent conditions. Prior research demonstrated that strategic IT alignment could enhance organizational agility. Many organizations implemented an enterprise architecture management (EAM) function to achieve benefits such as strategic IT alignment and agility. However, there is little research that explains the pathways between these focal concepts. Hence, we ground our work in the dynamic capabilities view and develop a conceptual model to explain how EAM investments lead to agility mediated by strategic IT alignment. We conducted survey research and collected a sample of 110 respondents. Based on this dataset, we performed a PLS-SEM and cluster analysis to test our model and associated hypotheses. Our results indicate that EAM enhances organizational agility. Strategic IT alignment mediates this effect. Lastly, our results showcase the complementary effect of conducting a PLS-SEM and cluster analysis.;Association for Information Systems;Conference Proceeding;26th Americas Conference on Information Systems, AMCIS 2020;2020-01-01;https://api.elsevier.com/content/abstract/scopus_id/85094622581
49;Socially responsible design science in information systems for sustainable development: a critical research methodology;This study presents an Information Systems (IS) research methodology for the conduct of critical research into sustainable development that encompasses the objectives of socially inclusive and environmentally sustainable economic growth. The specific context is the application of critical research in the problem definition phase of IS Design Science Research for sustainable development. The paper guides IS research through problem scenarios of unsustainable development where power can distort truth and corrupt the public discourse in the furtherance of their ambitions. The methodology provides a structured approach to engage in inquiry of topics which are by their nature, deceptive and opaque. The methodology enables research inquiry encompassing societal topics, macro-social issues related to sustainable development and the application of nomothetic inquiry to address systemic problems. The paper concludes with illustrative examples of outcomes from the application of the prescribed methodology. The study provides an IS response to systemic social and environmental challenges by identifying the routes to transformation which in turn inform the design of IS solutions. Consequently, the study lays a foundation for IS engagement in socially responsible design science research.;Taylor and Francis Ltd.;Journal;European Journal of Information Systems;2023-01-01;https://api.elsevier.com/content/abstract/scopus_id/85111422424
50;Artifact evaluation in information systems design-science research - A holistic view;Design science in Information Systems (IS) research pertains to the creation of artifacts to solve reallife problems. Research on IS artifact evaluation remains at an early stage. In the design-science research literature, evaluation criteria are presented in a fragmented or incomplete manner. This paper addresses the following research questions: Which criteria are proposed in the literature to evaluate IS artifacts? Which ones are actually used in published research? How can we structure these criteria? Finally, which evaluation methods emerge as generic means to assess IS artifacts? The artifact resulting from our research comprises three main components: A hierarchy of evaluation criteria for IS artifacts organized according to the dimensions of a system (goal, environment, structure, activity, and evolution), a model providing a high-level abstraction of evaluation methods, and finally, a set of generic evaluation methods which are instantiations of this model. These methods result from an inductive study of twenty-six recently published papers.;Pacific Asia Conference on Information Systems;Conference Proceeding;Proceedings - Pacific Asia Conference on Information Systems, PACIS 2014;2014-01-01;https://api.elsevier.com/content/abstract/scopus_id/84928591992
51;Impact of smart services to current value networks;Promising market offerings are increasingly based on the close interaction of physical products and accompanying services - this is expresses by the term product service systems. A new trend is the expansion of the product service systems by data-driven, intelligent services (called Smart Service). Examples include predictive maintenance or automated re-ordering of consumables and materials. Such enhancements can significantly help to secure the customer interface and generate additional profitable business with new revenue models, such as pay-per-use rather than selling a machine. To take advantage of this opportunity, companies must develop promising Smart Services and implement them in their value network. The present paper shows the necessary steps for a company. The first step shows methods for the conception of Smart Services. The second step is the analysis of the current value networks. A comparison of the new functions and requirements of the Smart Services with the value network leads to effects and necessary changes. In the last step, changes are translated into a roadmap for implementation.;UiTM Presswkuntjoro@yahoo.com;Journal;Journal of Mechanical Engineering;2018-03-15;https://api.elsevier.com/content/abstract/scopus_id/85052567126
52;Digital twin: Values, challenges and enablers from a modeling perspective;Digital twin can be defined as a virtual representation of a physical asset enabled through data and simulators for real-time prediction, optimization, monitoring, controlling, and improved decision making. Recent advances in computational pipelines, multiphysics solvers, artificial intelligence, big data cybernetics, data processing and management tools bring the promise of digital twins and their impact on society closer to reality. Digital twinning is now an important and emerging trend in many applications. Also referred to as a computational megamodel, device shadow, mirrored system, avatar or a synchronized virtual prototype, there can be no doubt that a digital twin plays a transformative role not only in how we design and operate cyber-physical intelligent systems, but also in how we advance the modularity of multi-disciplinary systems to tackle fundamental barriers not addressed by the current, evolutionary modeling practices. In this work, we review the recent status of methodologies and techniques related to the construction of digital twins mostly from a modeling perspective. Our aim is to provide a detailed coverage of the current challenges and enabling technologies along with recommendations and reflections for various stakeholders.;Institute of Electrical and Electronics Engineers Inc.;Journal;IEEE Access;2020-01-01;https://api.elsevier.com/content/abstract/scopus_id/85081090770
53;Supporting early phases of digital twin development with enterprise modeling and capability management: Requirements from two industrial cases;Industry 4.0 is a concept that has attracted much research and development over the last decade. At its core is the need to connect physical devices with their digital representations which essentially means establishing a digital twin. Currently, the technological development of digital twins has gathered much attention while the organizational and business aspects are less investigated. In response, the suitability of enterprise modeling and capability management for the purpose of developing and management of business-driven digital twins has been analyzed. A number of requirements from literature are summarized and two industrial cases have been analyzed for the purpose of investigating how the digital twin initiatives emerge and what forces drive the start of their implementation projects. The findings are discussed with respect to how Enterprise Modeling and the Capability-Driven Development method are able to support the business motivation, design and runtime management of digital twins.;Springer;Book Series;Lecture Notes in Business Information Processing;2020-01-01;https://api.elsevier.com/content/abstract/scopus_id/85086302396
54;Seeing beyond Foreground Occlusion: A Joint Framework for SAP-Based Scene Depth and Appearance Reconstruction;Foreground occlusion is a significant challenge in three-dimensional (3-D) reconstruction. In this paper, we first characterize the differences between multiview reconstruction with and without foreground occlusion. In order to reconstruct target scene, we attempt to see through the foreground occlusion using synthetic aperture photography. Different from existing methods, we propose a more generalized model for scene reconstruction, in which the target scene may not be fully observed by any of reference view. Assuming both scene depth and appearance are unknown, we reconstruct 3-D scene from camera array data by selecting optimal views with pixel based clustering. Then, we propose an iterative reconstruction approach in global optimization framework, in which we refine the reconstruction results by applying a coarse-to-fine strategy. Even when all views are partially occluded, our approach can recover accurate depth map as well as scene appearance using camera array data. Experimental results have indicated that the proposed approach is more robust to foreground occlusions and outperforms state-of-the-art approaches.;Institute of Electrical and Electronics Engineers Inc.;Journal;IEEE Journal on Selected Topics in Signal Processing;2017-10-01;https://api.elsevier.com/content/abstract/scopus_id/85021812422
55;Digital twin generation: Re-conceptualizing agent systems for behavior-centered cyber-physical system development;Cyber-Physical Systems (CPS) form the new backbone of digital ecosystems. Upcoming CPS will be operated on a unifying basis, the Internet of Behaviors (IoB). It features autonomous while federated CPS architectures and requires corresponding behavior modeling for design and control. CPS design and control involves stakeholders in different roles with different expertise accessing behavior models, termed Digital twins. They mirror the physical CPS part and integrate it with the digital part. Representing role-specific behaviors and provided with automated execution capabilities Digital twins facilitate dynamic adaptation and (re-)configuration. This paper proposes to conceptualize agent-based design for behavior-based Digital twins through subject-oriented models. These models can be executed and, thus, increase the transparency at design and runtime. Patterns recognizing environmental factors and operation details facilitate the configuration of CPS. Subject-oriented runtime support enables dynamic adaptation and the federated use of CPS components.;MDPI AG;Journal;Sensors (Switzerland);2021-02-02;https://api.elsevier.com/content/abstract/scopus_id/85100473353
56;Information technology and the search for organizational agility: A systematic review with future research possibilities;Organizations are increasingly turning to information technology (IT) to help them respond to unanticipated environmental threats and opportunities. In this paper, we introduce a systematic review of the literature on IT-enabled agility, helping to establish the boundary between what we know and what we don't know. We base our review on a wide body of literature drawn from the AIS Basket of Eight IT journals, a cross-section of non-Basket journals, IT practitioner outlets, and premier international IS conferences. We review the use of different theoretical lenses used to investigate the relationship between IT and organizational agility and how the literature has conceptualized agility, its antecedents, and consequences. We also map the evolution of the literature through a series of stages that highlight how researchers have built on previous work. Lastly, we discuss opportunities for future research in an effort to close important gaps in our understanding.;Elsevier B.V.;Journal;Journal of Strategic Information Systems;2019-06-01;https://api.elsevier.com/content/abstract/scopus_id/85059152735
57;Supersystem digital twin-driven framework for new product conceptual design;Digital twin (DT) is a technology that creates a digital replica of its physical entity with full information including historical and real-time data. They are expected to become an essential tool in industry 4.0 due to their ability to collect massive amounts of real-world data which can be utilized by artificial inelegance technologies for various purposes. The application of DT concepts in product design field has drawn wide interest, and DT has recently been proposed as a tool for supporting early-stage product design. Based on the difference in physical entities, DTs are classified as similar-product DT and mock-up prototype DT in this paper. Since there is often no product or physical prototype in early design stages, these two types of DTs for product design are primarily for re-design cases where data is collected from the DT of the similar products or prototypes. Introducing DTs to a new product design scenario is rarely explored. To fill the gap, this paper proposes the third type of DT, Supersystem DT that collects data from a product's supersystem (i.e., the working/operation environment). A systematic framework that integrates the Supersystem DT and Theory of Inventive Problem Solving (i.e., TRIZ) tools are presented for early design activities including task clarification and conceptual design. A case study of designing a new camper box of an expedition vehicle is presented to show the process and effectiveness of the proposed framework. Finally, guidelines for selecting the three types of DTs in different design scenarios are discussed.;Elsevier Ltd;Journal;Advanced Engineering Informatics;2023-10-01;https://api.elsevier.com/content/abstract/scopus_id/85168559465
58;A BPM lifecycle plug-in for modeling methods agility;"Business Process Management literature has proposed several BPM lifecycles on a level of abstraction that is ""modeling method""-agnostic, i.e. they consider the modeling language and tool support an underlying invariant or technological concern. While remaining on the same abstraction layer, we highlight a ""method agility"" requirement observed in commercial BPM consulting projects - concretely, it manifests as change requests for the modeling language or tool, from one lifecycle iteration to the next, leading to situations of ""model value co-creation"" as customer demands are assimilated in the modeling method. Based on a conceptualization of such situations, a lifecycle ""plug-in"" is proposed in the form of a methodology and associated tool support, allowing for responsive evolution of the adopted modeling method with impact on several lifecycle phases. Historical examples from the evolution of a BPM product are provided to illustrate and classify the demands that motivate the existence of this ""lifecycle plug-in"".";Association for Information Systems;Conference Proceeding;26th Americas Conference on Information Systems, AMCIS 2020;2020-01-01;https://api.elsevier.com/content/abstract/scopus_id/85097708630
59;A Knowledge-Based Conceptual Modelling Approach to Bridge Design Thinking and Intelligent Environments;One aspect of knowledge management is concerned with the alignment between what is captured in the heads of people and what is encoded by technology. The alignment of knowledge is necessary as humans possess an efficient ability to design innovation based on business insights, while technological systems are able to operating efficiently in different environments. To support knowledge management, this study presents systematic foundations covering a knowledge-based conceptual modelling approach. On a systematic level, three procedures are presented to facilitate the alignment of knowledge between people and technology: the decomposition of concepts from design thinking in conceptual models, the abstraction of capabilities from intelligent environments in conceptual models, and the (semi-) automated, intelligent transformation of conceptual models. Furthermore, the architecture of ICT infrastructure supporting the three procedures is addressed. These systematic foundations are integrated in the OMiLAB ecosystem and instantiated in two projects. The first project revolves around PRINTEPS, which is a framework to develop practical Artificial Intelligence. The second project revolves around s*IoT, which is a unifying semantic-aware modelling environment for the Internet of Things. Additionally, two concrete cases are presented for both project. Due to employing common systematic foundations, transfer and reuse among the two projects is facilitated.;Springer;Book Series;Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics);2019-01-01;https://api.elsevier.com/content/abstract/scopus_id/85081613804
60;Challenging Digital Innovation Through the OMiLAB Community of Practice;Digitalization requires cyber-physical ecosystems to achieve the goals of its transformation process, which should be primarily driven by innovation. OMi- LAB (www.omilab.org) supports digital innovation within a community of practice and technical environment, based on a global network of physical laboratory nodes. The Digital Innovation Environment (DiEn) powered by OMiLAB located at industrial and academic organizations responds to digital transformation challenges. It facilitates the co-creation, design, and engineering of early prototypes. Digital innovation is challenged by the OMiLAB community of practice through tool-aided conceptual modelling and elevates model value in domain-specific scenarios and experiments.;Springer International Publishing;Book;Domain-Specific Conceptual Modeling: Concepts, Methods and ADOxx Tools;2022-01-01;https://api.elsevier.com/content/abstract/scopus_id/85125410078
61;Learning and Becoming Writers: Meaning, Identity, and Epistemology in a Newsroom Community of Practice;This article analyzes a yearlong ethnography of the Southside Free Press, a non-school-affiliated community-based organization that served diverse adolescent staff writers who prepared articles for a monthly newspaper publication. The study employed a perspective of learning as legitimate peripheral participation with/in a newsroom community of practice. Specifically, using Rogoff’s three planes of analysis—apprenticeship, guided participation, and participatory appropriation—demonstrates that the process of becoming staff writers affected their learning and development. Thus, their ongoing participation provided opportunities to come to enact and embody the available meanings, identities, and epistemologies around becoming a staff writer and citizenship.;Routledgeaabs@uw.edu;Journal;Mind, Culture, and Activity;2018-04-03;https://api.elsevier.com/content/abstract/scopus_id/85035118118
62;Design science methodology: For information systems and software engineering;This book provides guidelines for practicing design science in the fields of information systems and software engineering research. A design process usually iterates over two activities: first designing an artifact that improves something for stakeholders and subsequently empirically investigating the performance of that artifact in its context. This validation in context is a key feature of the book - since an artifact is designed for a context, it should also be validated in this context.;Springer Berlin Heidelberg;Book;Design Science Methodology: For Information Systems and Software Engineering;2014-01-01;https://api.elsevier.com/content/abstract/scopus_id/84956782190
63;Industrial Digital Environments in Action: The OMiLAB Innovation Corner;The digital transformation is a global mega trend that is triggered by the evolution of digital technology, that has the potential for every organisation to either optimize their current business via a digital innovation or by transforming the business via digital disruption. The challenge for every organisation is therefore to select and personalise the appropriate digital innovation. There is a plethora of methods and assessment frameworks, here we introduce the OMiLAB Innovation Corner that assists in (1) creating new business, (2) design the organisational model and (3) engineer proof-of-concept prototypes as a “communication media”. The unique value proposition of OMiLAB Innovation Corner is the model-based foundation that supports decision makers in key phases of the innovation. First, the creation of new business models by providing digital design thinking tools is assisted. Second, the design of the digital organisation by providing extended modelling capabilities is supported. Third, a proof-of-concept engineering providing robots and sensors is enabled. We share our practical exeriences by introducing (a) how new business models are created in the H2020 project Change2Twin to help manufacturing SMEs in their digital transformation, (b) how conceptual models are design in the H2020 project BIMERR to create digital twins of renovation processes and (d) how proof-of-concept engineering is performed in the FFG project complAI to analyse different robotic behaviour.;Springer Science and Business Media Deutschland GmbH;Book Series;Lecture Notes in Business Information Processing;2020-01-01;https://api.elsevier.com/content/abstract/scopus_id/85097098993
64;Conceptual digital twin modeling based on an integrated five-dimensional framework and TRIZ function model;Digital twin represents a fusion of the informational and physical domains, to bridge the material and virtual worlds. Existing methods of digital twin modeling are mainly based on modular representation, which limits guidance of the modeling process. Such methods do not consider the components or operational rules of the digital twin in detail, thereby preventing designers from applying these methods in their fields. With the increasing application of digital twin to various engineering fields, an effective method of modeling a multi-dimensional digital twin at the conceptual level is required. To such an end, this paper presents a method for the conceptual modeling of a digital twin based on a five-dimensional digital twin framework to represent the complex relationship between digital twin objects and their attributes. The proposed method was used to model the digital twin of an intelligent vehicle at the concept level.;Elsevier B.V.;Journal;Journal of Manufacturing Systems;2021-01-01;https://api.elsevier.com/content/abstract/scopus_id/85089247209
65;Opportunity-led ideation: How to convert corporate opportunities into innovative ideas;Opportunities, that is, action possibilities for innovative business models, goods, services and processes, particularly affect idea generation, which is vital for innovation success. Capitalizing on opportunities requires complementing predominating problem-centred innovation approaches. Despite mature knowledge on idea generation, there is still a limited understanding on how to leverage opportunities. Hence, there is a limited set of methods available that provide formalized guidance. To address this gap, we co-developed an opportunity-led ideation method in an action design research project with one of Australia's leading financial service providers. Thanks to this immersive collaboration, our method reflects not only the intent of researchers and existing knowledge but also the influence and needs of practitioners. Building on established opportunity sources from the literature, this method structures the idea generation stage of the innovation process into the activities initiation, immersion, investigation and integration. The method provides guidance on how to transform opportunities into ideas and presents activities, techniques, tools and roles that are important within the idea generation stage. Our research theoretically extends the understanding of opportunity identification within the front end of innovation. Moreover, it provides insights on balancing formalization and creativity within idea generation. Organizations can use the method as a blueprint to systematically and proactive sense, assess and translate opportunities into ideas.;Life Science Publishing Co. Ltd;Journal;Creativity and Innovation Management;2021-09-01;https://api.elsevier.com/content/abstract/scopus_id/85109396486
66;Model-driven engineering of mediation information system for enterprise interoperability;The MISE approach (mediation information system engineering) aims at defining and designing a platform, dedicated to initiate and support any collaborative situation among potential partners. Collaborative situations may emerge from manufacturing contexts (industrial production partners), economic contexts (supply chain), social contexts (crisis management partners) and any other contexts in which a set of organisations should work altogether to reach common and individual goals, exploiting each others competencies. The MISE approach is based on a model-driven engineering vision (MDE) dedicated to designing a mediation information system (MIS) in charge of improving interoperability in emerging collaborative situations. This MIS is dedicated to supporting the collaborative behaviour of the collaborative network by dealing with exchanged data, shared services and collaborative workflows. The final objective is a mediator system able to manage the operational collaboration of partners, through there information system, without constraint (at least with as less constraint as possible). The MIS design crosses the different abstraction layers of design (business, logical and technological) and exploits the associated models at each level to build the models of the next level. This paper presents the models involved (dedicated to the computer independent model, platform independent model and platform specific model levels of the MDE approach) and the transition mechanisms between levels.;Taylor and Francis Ltd.michael.wagreich@univie.ac.at;Journal;International Journal of Computer Integrated Manufacturing;2018-01-02;https://api.elsevier.com/content/abstract/scopus_id/85029537612
67;Knowledge-driven digital twin manufacturing cell towards intelligent manufacturing;Rapid advances in new generation information technologies, such as big data analytics, internet of things (IoT), edge computing and artificial intelligence, have nowadays driven traditional manufacturing all the way to intelligent manufacturing. Intelligent manufacturing is characterised by autonomy and self-optimisation, which proposes new demands such as learning and cognitive capacities for manufacturing cell, known as the minimum implementation unit for intelligent manufacturing. Consequently, this paper proposes a general framework for knowledge-driven digital twin manufacturing cell (KDTMC) towards intelligent manufacturing, which could support autonomous manufacturing by an intelligent perceiving, simulating, understanding, predicting, optimising and controlling strategy. Three key enabling technologies including digital twin model, dynamic knowledge bases and knowledge-based intelligent skills for supporting the above strategy are analysed, which equip KDTMC with the capacities of self-thinking, self-decision-making, self-execution and self-improving. The implementing methods of KDTMC are also introduced by a thus constructed test bed. Three application examples about intelligent process planning, intelligent production scheduling and production process analysis and dynamic regulation demonstrate the feasibility of KDTMC, which provides a practical insight into the intelligent manufacturing paradigm.;Taylor and Francis Ltd.michael.wagreich@univie.ac.at;Journal;International Journal of Production Research;2020-02-16;https://api.elsevier.com/content/abstract/scopus_id/85065139151
68;LabVIEW Implementation of Tuning PID Controller Using Advanced Control Optimization Techniques for Micro-robotics System;Microparticles have the potentials to be used for many medical purposes in-side the human body such as drug delivery and other operations. This paper attempts to provide a thorough comparison between eight meta-heuristic search algorithms: Sparrow Search Algorithm (SSA), Flower Pollination Algorithm (FPA), Slime Mould Algorithm (SMA), Marine Predator Algorithm (MPA), Multi-Verse Optimizer (MVO) Grey Wolf Optimization (GWO), Sine-Cosine Algorithm (SCA), and Whale Optimization Algorithm (WOA). These approaches were used to calculate the PID controller optimal indicators with the application of different functions, including Integral Absolute Error (IAE), Integral of Time Multiplied by Square Error (ITSE), Integral Square Time multiplied square Error (ISTES), Integral Square Error (ISE), Integral of Square Time multiplied by square Error ( (ISTSE), and Integral of Time multiplied by Absolute Error (ITAE). Every method of controlling was presented in a MATLAB Simulink numerical model, and LABVIEW software was used to run the experimental tests.. It is observed that the GWO technique achieves the highest values of settling error for both simulation and experimental results among other control approaches, while the SSA approach reduces the settling error by 50% compared to former experiments. The results indicate that SSA is the best method among all approaches and that ISTES is the best choice of PID for optimizing the controlling parameters.;International Journal of Mechanical Engineering and Robotics Research;Journal;International Journal of Mechanical Engineering and Robotics Research;2022-09-01;https://api.elsevier.com/content/abstract/scopus_id/85135779877
69;Current software barriers to advanced model-based control design for energy-efficient buildings;Fast and easy advanced model-based control design for energy-efficient multi-zone buildings is crucial for optimal energy savings, and this strongly depends on the availability and capability of advanced simulation and control design software and tools. In this paper, first a state-of-the-art review of the commonly used major software and tools by the community is done with respect to the barriers they present to advanced model-based control design for energy-efficient buildings. Next, the relevant novel concept of Functional Moke-up Interface is reviewed and the associated advances up to date are summarized. Finally, a set desired control-oriented features for new generation tools are given towards better solutions for energy-efficient building control designs.;Elsevier Ltd;Journal;Renewable and Sustainable Energy Reviews;2017-01-01;https://api.elsevier.com/content/abstract/scopus_id/85012149130
70;Embedded Model Control: Outline of the theory;Embedded Model Control allows one to proceed systematically from fine plant dynamics and control requirements to the Embedded Model (EM), which is the core of control design and algorithms. The model defines three interconnected parts: the controllable dynamics, the disturbance class to be rejected and the neglected dynamics. Controllable and disturbance dynamics must be observable from the plant measurements. Control algorithms are designed around the first two parts, while stability and performance are constrained by the third one. The key design issue is discriminating between driving noise and neglected dynamics, to guarantee updating disturbance in view of its rejection. To this end, concept and equations of the 'error loop' are outlined: it maps error sources to performance and shows how to discriminate destabilizing sources, while meeting performance requirements. An introductory example with analytical and simulated results illustrates the design steps. © 2007 ISA.;ISA - Instrumentation, Systems, and Automation Society;Journal;ISA Transactions;2007-01-01;https://api.elsevier.com/content/abstract/scopus_id/34247173596
71;An integrated simulation paradigm for lifecycle-covering maintenance in the Industry 4.0 context;The Industry 4.0 is pushing a fast evolution in all manufacturing operations under heterogeneous aspects. Maintenance, in all its numerous declinations along the life cycle of a production asset, is no exception. A myriad of decisions need taking—often on short time horizons, involving different physical and technological domains, and with limited knowledge available. A myriad of data sources need exploiting and interpreting—often poorly structured, requiring reconciliation, and affected by uncertainty. A lot of tools are therefore used—each one conceived for a specific purpose, often leveraging knowledge drawn from different cultures, and dealing with production assets' models that are hard to integrate with one another. We conjecture that the OOMS (Object-Oriented Modelling and Simulation) paradigm can help address such a tough scenario. In this paper we structure and motivate the statement just made, provide high-level supporting examples, and outline future work as for both research and technology.;Elsevier B.V.;Conference Proceeding;IFAC-PapersOnLine;2020-01-01;https://api.elsevier.com/content/abstract/scopus_id/85105547414
72;A review of unit level digital twin applications in the manufacturing industry;"In recent years, the hype around Digital Twins (DTs) has been exponentially increasing in both industry and academia. DTs are a potential solution to increase automation and advance towards Smart Manufacturing. Manufacturing DTs have been implemented at different hierarchical levels, ranging from system of systems to unit level. Increasing computational capacity and data exchange rates can enable DT implementations for real-time applications. Several literature reviews on manufacturing DTs have been published. However, no previous paper focuses on manufacturing DTs at the unit level for which real-time control is most applicable. Simultaneously, the challenges to engineer DTs with real-time capabilities are enormous, both from a scientific and technological perspective. Therefore, we focus on DTs of single production units such as traditional machine tools, additive manufacturing machines and advanced robotic applications. In this systematic literature review, 96 papers about practical unit level DT applications found in the Scopus database using a combination of the keywords “Digital Twin”, “Production” and “Manufacturing” are reviewed. We summarize how DTs are currently implemented and operated, and what potential benefits DTs offer at the unit process level in four categories: generic reference models, services, DT content (models and data) and DT deployment (hardware and software). Following the thematic analysis, an overall discussion, summary of key contributions and identified research gaps, and outlook into future research avenues is given. Key findings of this review can be summarized as: focus on DT components versus being holistic; need to share data and models across multiple stakeholders; lack of physical fidelity of the models; stark contrast of lab scale developments and real world testing, e.g., historical data and storage related challenges; lack of clear definition of DT in industry, and missing semantic interoperability between a wide variety of domains.";Elsevier Ltd;Journal;CIRP Journal of Manufacturing Science and Technology;2023-10-01;https://api.elsevier.com/content/abstract/scopus_id/85164018738
73;Principles of Object Oriented Modeling and Simulation with Modelica 3.3: A Cyber-Physical Approach;Fritzson covers the Modelica language in impressive depth from the basic concepts such as cyber-physical, equation-base, object-oriented, system, model, and simulation, while also incorporating over a hundred exercises and their solutions for a tutorial, easy-to-read experience. • The only book with complete Modelica 3.3 coverage • Over one hundred exercises and solutions • Examines basic concepts such as cyber-physical, equation-based, object-oriented, system, model, and simulation.;Wiley-IEEE Press;Book;Principles of Object Oriented Modeling and Simulation with Modelica 3.3: A Cyber-Physical Approach;2014-11-24;https://api.elsevier.com/content/abstract/scopus_id/84915764420
74;Automated control system design with model-based commissioning;A process of industrial control system design contains a set of steps. The result of the design process is significantly influenced by quality of execution of each step. An important phase of the design is a testing on different level of the design. The testing process influences a commissioning of the control system which follows the design process. The article describes innovative approach of automated control system design and analyses the possibilities of model based testing and commissioning. Both, the automated control system design and model based commissioning increase efficiency and quality of engineering process and of the designed control system. The model based commission is very close to digital twin approach which is one of important trends in automation.;North Atlantic University Union942 Windemere Dr. NW.,SalemOregon97304north.atlantic.university.union@naun.org;Journal;International Journal of Circuits, Systems and Signal Processing;2019-01-01;https://api.elsevier.com/content/abstract/scopus_id/85060989382
75;Digital Twin in manufacturing: A categorical literature review and classification;The Digital Twin (DT) is commonly known as a key enabler for the digital transformation, however, in literature is no common understanding concerning this term. It is used slightly different over the disparate disciplines. The aim of this paper is to provide a categorical literature review of the DT in manufacturing and to classify existing publication according to their level of integration of the DT. Therefore, it is distinct between Digital Model (DM), Digital Shadow (DS) and Digital Twin. The results are showing, that literature concerning the highest development stage, the DT, is scarce, whilst there is more literature about DM and DS.;Elsevier B.V.;Conference Proceeding;IFAC-PapersOnLine;2018-01-01;https://api.elsevier.com/content/abstract/scopus_id/85052915281
76;Digital Twin-driven smart manufacturing: Connotation, reference model, applications and research issues;This paper reviews the recent development of Digital Twin technologies in manufacturing systems and processes, to analyze the connotation, application scenarios, and research issues of Digital Twin-driven smart manufacturing in the context of Industry 4.0. To understand Digital Twin and its future potential in manufacturing, we summarized the definition and state-of-the-art development outcomes of Digital Twin. Existing technologies for developing a Digital Twin for smart manufacturing are reviewed under a Digital Twin reference model to systematize the development methodology for Digital Twin. Representative applications are reviewed with a focus on the alignment with the proposed reference model. Outstanding research issues of developing Digital Twins for smart manufacturing are identified at the end of the paper.;Elsevier Ltd;Journal;Robotics and Computer-Integrated Manufacturing;2020-02-01;https://api.elsevier.com/content/abstract/scopus_id/85070213247
77;Industry 4.0 as a Cyber-Physical System study;Advances in computation and communication are taking shape in the form of the Internet of Things, Machine-to-Machine technology, Industry 4.0, and Cyber-Physical Systems (CPS). The impact on engineering such systems is a new technical systems paradigm based on ensembles of collaborating embedded software systems. To successfully facilitate this paradigm, multiple needs can be identified along three axes: (i) online configuring an ensemble of systems, (ii) achieving a concerted function of collaborating systems, and (iii) providing the enabling infrastructure. This work focuses on the collaborative function dimension and presents a set of concrete examples of CPS challenges. The examples are illustrated based on a pick and place machine that solves a distributed version of the Towers of Hanoi puzzle. The system includes a physical environment, a wireless network, concurrent computing resources, and computational functionality such as, service arbitration, various forms of control, and processing of streaming video. The pick and place machine is of medium-size complexity. It is representative of issues occurring in industrial systems that are coming online. The entire study is provided at a computational model level, with the intent to contribute to the model-based research agenda in terms of design methods and implementation technologies necessary to make the next generation systems a reality.;Springer Verlagservice@springer.de;Journal;Software and Systems Modeling;2016-02-01;https://api.elsevier.com/content/abstract/scopus_id/84956605744
78;A Review of the Roles of Digital Twin in CPS-based Production Systems;"The Digital Twin (DT) is one of the main concepts associated to the Industry 4.0 wave. This term is more and more used in industry and research initiatives; however, the scientific literature does not provide a unique definition of this concept. The paper aims at analyzing the definitions of the DT concept in scientific literature, retracing it from the initial conceptualization in the aerospace field, to the most recent interpretations in the manufacturing domain and more specifically in Industry 4.0 and smart manufacturing research. DT provides virtual representations of systems along their lifecycle. Optimizations and decisions making would then rely on the same data that are updated in real-time with the physical system, through synchronization enabled by sensors. The paper also proposes the definition of DT for Industry 4.0 manufacturing, elaborated by the European H2020 project MAYA, as a contribution to the research discussion about DT concept.";Elsevier B.V.;Conference Proceeding;Procedia Manufacturing;2017-01-01;https://api.elsevier.com/content/abstract/scopus_id/85029833606
79;Digital twin and virtual reality based methodology for multi-robot manufacturing cell commissioning;"Intelligent automation, including robotics, is one of the current trends in the manufacturing industry in the context of ""Industry 4.0"", where cyber-physical systems control the production at automated or semi-automated factories. Robots are perfect substitutes for a skilled workforce for some repeatable, general, and strategically-important tasks. However, this transformation is not always feasible and immediate, since certain technologies do not provide the required degree of flexibility. The introduction of collaborative robots in the industry permits the combination of the advantages of manual and automated production. In some processes, it is necessary to incorporate robots from different manufacturers, thus the design of these multi-robot systems is crucial to guarantee the maximum quality and efficiency. In this context, this paper presents a novel methodology for process automation design, enhanced implementation, and real-time monitoring in operation based on creating a digital twin of the manufacturing process with an immersive virtual reality interface to be used as a virtual testbed before the physical implementation. Moreover, it can be efficiently used for operator training, real-time monitoring, and feasibility studies of future optimizations. It has been validated in a use case which provides a solution for an assembly manufacturing process.";MDPI AGmembranes@mdpi.com;Journal;Applied Sciences (Switzerland);2020-05-01;https://api.elsevier.com/content/abstract/scopus_id/85085616035
80;Model Validation of Large Wind Power Plants Through Field Testing;Recently, through efforts lead by the Western Electricity Coordinating Council's Renewable Energy Modeling Task Force, the so-called second-generation generic models were developed for modeling inverter based energy sources such as wind and photovoltaic generation. These models have been industry vetted, and approved, and now reside in several commercial software platforms used throughout North America, and elsewhere in the world. The benefit of these models is their generic nature and being open and publicly available. In this paper, we present a detailed account of field testing five large wind power plants in order to build and validate the dynamic models for the plants using these generic models. Several key insights are highlighted relative to the process and approach to model validation, including some unique circumstances for a few of these wind power plants, which are in close electrical proximity to each other. It is shown that these simplified models are able to match the field tests and yield reasonable and well-behaved models for use in large scale stability studies.;Institute of Electrical and Electronics Engineers Inc.;Journal;IEEE Transactions on Sustainable Energy;2018-07-01;https://api.elsevier.com/content/abstract/scopus_id/85035810209
81;Knowledge map and forecast of digital twin in the construction industry: State-of-the-art review using scientometric analysis;"With the advancement of Industry 4.0 and the development of science and technology, industries have seen the advantages of digital twin (DT) implementations and concepts. Although the construction industry is still in the early stage of DT development and implementation, it is urgent for the industry to have a DT agenda. This paper identifies 1158 related bibliographic records from the Web of Science and 745 records from the Derwent Patent Database and quantitatively analyzed them to describe patterns of publications. Two knowledge mapping tools were selected to visualize and analyze the literature in the relevant scientific domain. Then, we integrated clustering, knowledge mapping, and network analysis methods to show the current foci and future directions of DT-MT derived from the data of time distribution, journal areas, and subject distribution. The results indicate that DT is a cross-disciplinary information technology with great development potential, and its development in the construction industry is scattered with the leading advancement in the manufacturing of construction materials and components. The derived knowledge map shows three stages of construction DT, including (1) data collection and creation of DT framework and database; (2) implementation of planning and design modules in detail and control of construction sites; and (3) carrying out asset management and fault prediction in the operation and maintenance phase of projects. The novelty of this paper is the specific analysis of DT in the construction industry. This study adds value to the architecture, engineering, and construction industries by shedding light on the focus of DT practical applications and the formation of a development path of construction DT.";Elsevier Ltd;Journal;Journal of Cleaner Production;2023-01-10;https://api.elsevier.com/content/abstract/scopus_id/85143719809
82;A cyber-physical system-based approach for industrial automation systems;"Industrial automation systems (IASs) are commonly developed using the languages defined by the IEC 61131 standard and are executed on programmable logic controllers (PLCs). Their software part is commonly considered only after the development and integration of mechanics and electronics. However, this approach narrows the solution space for software; thus, it is considered inadequate to address the complexity of today's systems. In this paper, we adopt a system-based approach for the development of IASs. Based on this, the UML model of the software part of the system is extracted from the SysML system model and it is then refined to get the implementation code. Two implementation alternatives are considered to exploit both PLCs and the recent deluge of embedded boards in the market. For PLC targets, the new version of IEC 61131 that supports object-orientation is adopted, while Java is used for embedded boards. The case study used to illustrate our approach was developed as a lab exercise, which aims to introduce to students a number of technologies used to address challenges in the domain of cyber-physical systems and highlights the role of the Internet of Things (IoT) as a glue for their cyber interfaces.";Elsevier B.V.;Journal;Computers in Industry;2015-09-01;https://api.elsevier.com/content/abstract/scopus_id/84931572168
83;Implementation of SSL/TLS Security with MQTT Protocol in IoT Environment;Internet of Things (IoT) is the interconnection of devices with the internet to deliver its tasks. Nowadays, security is the main concern relating to these devices. Low in power storage, low in processing capabilities and low in data storage make it hard to provide a strong set of security protocols to protect the vulnerable devices “things”. Having internet as its backbone, allows the devices to communicate seamlessly. However, without any form of protection, it would open the door for hackers or middleman to hijack the connection, steal data and sabotage the information. In this paper, Secure Socket Layer and Transport Layer Security (SSL/TLS) protocol is implemented on top of Message Queuing Telemetry Transport (MQTT) IoT application protocol and the performance of the network is evaluated and analyzed in a typical IoT testbed comprising Raspberry Pi4 and ESP32 nodes. This work focuses on energy consumption, generated overhead, system complexity and required data storage resources. Experimental results of stress testing the system indicates that SSL/TLS encryption, operating with MQTT Quality of Service (QoS) level 2, while increasing the traffic rate 3.5 orders of magnitude yields more than two thousand times the amount of overhead generated and results in 73.25 J of consumed energy. Whereas operating without the SSL/TLS encryption under the same stress testing conditions yields only 140 times the amount of overhead generated and results in a mere 18.76 J of consumed energy. This difference of 4 folds on consumed energy indicates that the SSL/TLS -enabled node battery can only last a quarter of the lifespan of the TLS-free node and concluding the SSL/TLS encryption is not a viable solution for battery-operated IoT nodes.;Springer;Journal;Wireless Personal Communications;2023-09-01;https://api.elsevier.com/content/abstract/scopus_id/85165684814
84;Rational design of AgGaS/ZnS/ZnS quantum dots with a near-unity photoluminescence quantum yield via double shelling scheme;In this study, a two-step method was used to synthesize highly luminescent AgGaS/ZnS/ZnS quantum dots (QDs). In the first step, an inner ZnS shell was formed via a one-pot method, which resulted in a smaller lattice mismatch between the AgGaS core and the outer ZnS shell, thereby facilitating the formation of a thick outer shell. After the two-step shelling process, the synthesized AgGaS/ZnS/ZnS QDs showed an excellent photoluminescence quantum yield (PLQY) of 96.4% with a peak wavelength of 508 nm, representing the highest PLQY reported thus far for AgGaS QDs. Furthermore, the effect of halogen ions in Zn precursors on the shelling process was investigated. It was proposed that the capacity of halogen ions to coordinate with the QDs influenced the balance between Zn cation diffusion and ZnS shelling reaction. Specifically, the ZnS shelling reaction was dominant when ZnCl2 was employed, while Zn cation diffusion was the dominant process under the I−-rich environment. This work provides insights into the interfacial restructuring during the ZnS shelling and offers a clear map for the tailored synthesis of core/shell QDs.;Chinese Society of Metals;Journal;Journal of Materials Science and Technology;2024-01-10;https://api.elsevier.com/content/abstract/scopus_id/85166620988
85;Context-aware driver behavior detection system in intelligent transportation systems;Vehicular ad hoc networks (VANETs) have emerged as an application of mobile ad hoc networks (MANETs), which use dedicated short-range communication (DSRC) to allow vehicles in close proximity to communicate with each other or to communicate with roadside equipment. Applying wireless access technology in vehicular environments has led to the improvement of road safety and a reduction in the number of fatalities caused by road accidents through development of road safety applications and facilitation of information sharing between moving vehicles regarding the road. This paper focuses on developing a novel and nonintrusive driver behavior detection system using a context-aware system in VANETs to detect abnormal behaviors exhibited by drivers and to warn other vehicles on the road to prevent accidents from happening. A five-layer context-aware architecture is proposed, which is able to collect contextual information about the driving environment, to perform reasoning about certain and uncertain contextual information, and to react upon that information. A probabilistic model based on dynamic Bayesian networks (DBNs) in real time, inferring four types of driving behavior (normal, drunk, reckless, and fatigue) by combining contextual information about the driver, the vehicle, and the environment, is presented. The dynamic behavior model can capture the static and the temporal aspects related to the behavior of the driver, thus leading to robust and accurate behavior detection. The evaluation of behavior detection using synthetic data proves the validity of our model and the importance of including contextual information about the driver, the vehicle, and the environment. © 1967-2012 IEEE.;Institute of Electrical and Electronics Engineers Inc.;Journal;IEEE Transactions on Vehicular Technology;2013-01-01;https://api.elsevier.com/content/abstract/scopus_id/84888035580
86;A survey on driver behavior detection techniques for intelligent transportation systems;Driver behavior is an essential component of the driver-vehicle-environment system and plays a key role in the design of the transport and vehicle systems in order to improve the efficiency and safety of human agility. The most important factors that influence driver behavior are the environment, vehicle and the driver itself. Experience, distraction, fatigue, drowsiness etc. are so me of the other factors that have an impact on driver behavior. Improper driving behavior is the leading cause of the accidents and thus, detection of driver behavior is an emerging area of research interest. This paper discusses the various techniques used for monitoring driver behavior and also classifies them into real-time and non-real time techniques. A comparative analysis was performed on the basis of advantages, disadvantages and methodology applied by various techniques for detecting driver's behavior for Intelligent Transportation Systems (ITS).;Institute of Electrical and Electronics Engineers Inc.;Conference Proceeding;Proceedings of the 7th International Conference Confluence 2017 on Cloud Computing, Data Science and Engineering;2017-06-07;https://api.elsevier.com/content/abstract/scopus_id/85021743353
87;Enhancement of K nearest neighbour approach to solve the issue of pattern classification;k Nearest Neighbour (kNN) method is a frequently implemented method for improve the pattern classification and solve the problem of imbalanced dataset that has discovered so many classification and clustering applications. In this paper, issues of classification were focused and changes to the nearest neighbouring method were suggested which exploit data from a dataset structure. The issue of class imbalance has recently attracted considerable attention from researchers in data mining strategies. The results of the present experiments using unique client identifier (UCI) repository datasets show that the classifiers generated the work much effective when compared with classic kNN & are much accurate, but not dramatically slow. This produced some fascinating and encouraging outcomes, which inspired more work to develop the kNN process. The purpose of this paper is to suggest versions of kNN which are appropriate for categorization of issues, since it leverage intrinsic of knowledge within data sets, like the composition of the datasets.;American Institute of Physics Inc.;Conference Proceeding;AIP Conference Proceedings;2023-02-27;https://api.elsevier.com/content/abstract/scopus_id/85149925286
88;Contributions to driver fatigue detection based on eye-tracking;In recent years, one of the most important factors in road accidents is the drowsiness of drivers and the distraction while driving. In this paper, we describe a system that monitors the detection of fatigue or drowsiness. The proposed solutions follow the driver's gaze, and if the system identifies the closed eyes, it triggers an alarm signal intended to alert against losing control of the car and causing traffic accidents. Eye-tracking is the process that measuring the eye position and eye movement. The proposed method is structured in three phases. In the first phase, eye images are captured at constant time intervals and converted into grayscale images. In the second phase these images are fed to Haar algorithm to identify the driver eyes. In the third phase, based on the previous phase the system can now take action to continue monitoring or trigger alarm to alert the driver if the drowsiness has been detected.;North Atlantic University Union NAUN;Journal;International Journal of Circuits, Systems and Signal Processing;2021-01-01;https://api.elsevier.com/content/abstract/scopus_id/85099783674
89;Analysis of mode choice affects from the introduction of Doha Metro using machine learning and statistical analysis;The aim of this study was to investigate the possible influences of the operation of the new Doha Metro on the travel mode choice behavior in Doha City, Qatar. Revealed preference (RP) and stated preference (SP) survey questionnaires were designed to collect the necessary data. The questions considered different trip conditions and socioeconomic factors of travelers. Three different mode choices were considered in this study: private cars, taxi services, and metro. Two statistical models and one machine learning model were used to analyze the current and future mode choices: discrete choice binary logit (BL) and multinomial logit (MNL) models as well as extreme gradient boosting (XGBoost). Furthermore, the SHapley Additive exPlanations (SHAP) method was used to rank the input features based on their importance according to the mean SHAP value. The results showed that the XGBoost model outperforms the other two models in terms of predicting the travel mode choice as well as in terms of its accuracy. The results showed that various trip characteristics are significant in determining the mode choice, including the number of travelers and bags, journey time, and reimbursement of parking fees. Furthermore, different socioeconomic characteristics proved to be significant for the current and future mode choices, including nationality, income, age, employment status, and vehicle ownership.;Elsevier Ltd;Journal;Transportation Research Interdisciplinary Perspectives;2023-07-01;https://api.elsevier.com/content/abstract/scopus_id/85160412137
90;Prediction of human driving behavior using dynamic bayesian networks;This paper presents a method of predicting future human driving behavior under the condition that its resultant behavior and past observations are given. The proposed method makes use of a dynamic Bayesian network and the junction tree algorithm for probabilistic inference. The method is applied to behavior prediction for a vehicle assumed to stop at an intersection. Such a predictive system would facilitate warning and assistance to prevent dangerous activities, such as red-light violations, by allowing detection of a deviation from normal behavior. Copyright © 2006 The Institute of Electronics, Information and Communication Engineers.;Institute of Electronics, Information and Communication, Engineers, IEICE;Journal;IEICE Transactions on Information and Systems;2006-01-01;https://api.elsevier.com/content/abstract/scopus_id/33645236955
91;Comparing forward and backward reachability as tools for safety analysis;Using only the existence and uniqueness of trajectories for a generic dynamic system with inputs, we define and examine eight types of forward and backward reachability constructs. If the input is treated in a worst-case fashion, any forward or backward reach set or tube can be used for safety analysis, but if the input is treated in a best-case fashion only the backward reach tube always provides the correct results. Fortunately, forward and backward algorithms can be exchanged if well-posed reverse time trajectories can be defined. Unfortunately, backward reachability constructs are more likely to suffer from numerical stability issues, especially in systems with significant contraction-the very systems where forward simulation and reachability are most effective. © Springer-Verlag Berlin Heidelberg 2007.;Springer Verlag;Book Series;Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics);2007-01-01;https://api.elsevier.com/content/abstract/scopus_id/38049127366
92;Driver modeling based on driving behavior and its evaluation in driver identification;All drivers have habits behind the wheel. Different drivers vary in how they hit the gas and brake pedals, how they turn the steering wheel, and how much following distance they keep to follow a vehicle safely and comfortably. In this paper, we model such driving behaviors as car-following and pedal operation patterns. The relationship between following distance and velocity mapped into a two-dimensional space is modeled for each driver with an optimal velocity model approximated by a nonlinear function or with a statistical method of a Gaussian mixture model (GMM). Pedal operation patterns are also modeled with GMMs that represent the distributions of raw pedal operation signals or spectral features extracted through spectral analysis of the raw pedal operation signals. The driver models are evaluated in driver identification experiments using driving signals collected in a driving simulator and in a real vehicle. Experimental results show that the driver model based on the spectral features of pedal operation signals efficiently models driver individual differences and achieves an identification rate of 76.8% for a field test with 276 drivers, resulting in a relative error reduction of 55% over driver models that use raw pedal operation signals without spectral analysis © 2007 IEEE.;Institute of Electrical and Electronics Engineers Inc.;Journal;Proceedings of the IEEE;2007-01-01;https://api.elsevier.com/content/abstract/scopus_id/56149104498
93;An Optimization-Based Human Behavior Modeling and Prediction for Human-Robot Collaborative Disassembly;To achieve a safe and seamless human-robot collaboration in intelligent remanufacturing, robot agents should be able to understand human behaviors, predict human future motion, and incorporate motion prediction into their planning process. While most existing human prediction algorithms suffer from poor generalization and huge training data requirements, this paper models the human agent as a rational model seeking to minimize an unknown cost function along the motion trajectory. With such modeling, we design a set of features, such as collision avoidance, maintaining comfort during the motion, and reaching the goal point without too much detour, that could capture human intents during HRC. Maximum-Entropy inverse reinforcement learning is then leveraged to learn the underlying cost function from noisy human demonstrations. The human motion prediction is obtained by solving an optimization problem with a learned cost function. We particularly build an HRC dataset for human-robot-collaborative disassembly tasks and applied the proposed algorithm to this new dataset. Experimental studies are extensively conducted to validate our human motion prediction model.;Institute of Electrical and Electronics Engineers Inc.;Conference Proceeding;Proceedings of the American Control Conference;2023-01-01;https://api.elsevier.com/content/abstract/scopus_id/85167829731
94;Driver and pedestrian awareness-based collision risk analysis;"We present a novel approach for vehicle-pedestrian collision risk analysis that incorporates mutual situational awareness, a degree of potential motion coupling and the spatial layout of the environment. The approach uses a Dynamic Bayesian Network (DBN) for modeling the individual object paths; collision risk is subsequently computed by an intersection operation.";Institute of Electrical and Electronics Engineers Inc.;Conference Proceeding;IEEE Intelligent Vehicles Symposium, Proceedings;2016-08-05;https://api.elsevier.com/content/abstract/scopus_id/84983250856
95;Research on trajectory planning in emergency situations with multiple objects;This paper answers the question of the latest possible evasive trajectory will be answered. Using a novel approach, it can be shown and proven that it has to be a combined braking and steering maneuver. Although [1] has already pointed out the advantages of a combination over an sole steering intervention, the question of the latest possible collision-free trajectory has not yet been answered. Based on the novel approach, a propagation method will be presented, which reveals unavoidable collisions in scenarios with multiple obstacles and which can be used to derivate an evasive trajectory. © 2006 IEEE.;Institute of Electrical and Electronics Engineers Inc.;Conference Proceeding;IEEE Conference on Intelligent Transportation Systems, Proceedings, ITSC;2006-01-01;https://api.elsevier.com/content/abstract/scopus_id/41849140056
96;Counting Processes with Piecewise-Deterministic Markov Conditional Intensity: Asymptotic Analysis, Implementation and Information-Theoretic Use;"Counting processes (CPs) are important mathematical models with a variety of applications in signal processing, telecommunication, queuing, neuroscience, systems and synthetic biology, meteorology, insurance and finance. A CP (<italic>Y<sub>t</sub></italic>)<sub><italic>t</italic>&#x2265;0</sub> can be characterized by its conditional intensity (CI), i.e., the &#x03C3;(<italic>Y<sub>s</sub></italic>, 0 &#x2264; <italic>s</italic> &#x2264; <italic>t</italic>)-intensity, for which the filtration is generated by (<italic>Y<sub>t</sub></italic>)<sub><italic>t</italic>&#x2265;0</sub>. The CI is the central quantity from which it is possible to compute information-theoretic measures, and the asymptotic of the CI suffices for their asymptotic versions. Two examples for such measures are the mutual information rate (MIR) of a signal and its Poisson channel output, and the relative entropy rate (RER) between CPs. In order to analytically access the asymptotic of the CI, we introduce the class of CP models for which the CI progresses as a piecewise-deterministic Markov process on an augmented state space and with deterministic jump sizes. This class includes Markov-modulated Poisson processes and self-exciting counting processes. For this class we derive the probability evolution equation of the CI and the fixed point equation for its embedded Markov chain. For the asymptotic CI distribution (ACID) we obtain an analytic description by stationary analysis methods for Markov processes. We present a simulation-free method to compute the ACID, when the dimension of the auxiliary state space is low. Using the ACID, we contribute a new method for the computation of the MIR of the Poisson channel as well as the RER between CPs; and suggest its use also for the empirical assessment of the similarity of CPs. We apply the technique to various naturally occurring CPs, such as the random telegraph modulated Poisson process and the Hawkes process.";Institute of Electrical and Electronics Engineers Inc.;Journal;IEEE Transactions on Information Theory;2023-01-01;https://api.elsevier.com/content/abstract/scopus_id/85164726182
97;Maximum likelihood estimation of diffusions by continuous time Markov chain;A novel method is presented for estimating the parameters of a parametric diffusion process. The approach is based on a closed-form Maximum Likelihood estimator for an approximating Continuous Time Markov Chain (CTMC) of the diffusion process. Unlike typical time discretization approaches, such as pseudo-likelihood approximations with Shoji-Ozaki or Kessler's method, the CTMC approximation introduces no time-discretization error during parameter estimation, and is thus well-suited for typical econometric situations with infrequently sampled data. Due to the structure of the CTMC, closed-form approximations are obtained for the sample likelihood which hold for general univariate diffusions. Comparisons of the state-discretization approach with approximate MLE (time-discretization) and Exact MLE (when applicable) demonstrate favorable performance of the CTMC estimator. Simulated examples are provided in addition to real data experiments with FX rates and constant maturity interest rates.;Elsevier B.V.;Journal;Computational Statistics and Data Analysis;2022-04-01;https://api.elsevier.com/content/abstract/scopus_id/85120986908
98;Modeling and recognizing driver behavior based on driving data: A survey;In recent years, modeling and recognizing driver behavior have become crucial to understanding intelligence transport systems, human-vehicle systems, and intelligent vehicle systems. A wide range of both mathematical identification methods and modeling methods of driver behavior are presented from the control point of view in this paper based on the driving data, such as the brake/throttle pedal position and the steering wheel angle, among others. Subsequently, the driver's characteristics derived from the driver model are embedded into the advanced driver assistance systems, and the evaluation and verification of vehicle systems based on the driver model are described. © 2014 Wenshuo Wang et al.;Hindawi Publishing Corporation410 Park Avenue, 15th Floor, 287 pmbNew YorkNY 10022;Journal;Mathematical Problems in Engineering;2014-01-01;https://api.elsevier.com/content/abstract/scopus_id/84896299010
99;Vehicle collision probability calculation for general traffic scenarios under uncertainty;Vehicle-to-vehicle (V2V) communication systems allow vehicles to share state information with one another to improve safety and efficiency of transportation networks. One of the key applications of such a system is in the prediction and avoidance of collisions between vehicles. If a method to do this is to succeed it must be robust to measurement uncertainty. The method should also be general enough that it does not rely on constraints on vehicle motion for the accuracy of its predictions. It should work for all interactions between vehicles and not just a select subset. This paper presents a method for collision probability calculation that addresses these problems. © 2014 IEEE.;Institute of Electrical and Electronics Engineers Inc.;Conference Proceeding;IEEE Intelligent Vehicles Symposium, Proceedings;2014-01-01;https://api.elsevier.com/content/abstract/scopus_id/84905364854
100;Modeling of Driver Behavior on Trajectory-Speed Decision Making in Minor Traffic Roadways with Complex Features;The selection of trajectory and speed is a crucial factor in automobile driving behavior. However, almost all the research objects of existing Driver models are developed for driving on city streets and general highways but are not applicable to driving simulations on minor traffic roads with complex shapes, in particular. Therefore, according to the practical automobile driving processes and characteristics in the real world, this paper proposed a trajectory calculation strategy, which we named as 'selecting trajectory point on a preview cross section.' According to this strategy, objective functions was established to describe the different selection patterns of 'trajectory and speed' of drivers. Constraint expressions were then designed on the basis of the roadway geometry and pavement condition, passenger car performance and ride comfort. A rolling-horizon algorithm for simultaneous solving, i.e., 'target trajectory-target speed,' was proposed. Eventually, validations of the proposed model were conducted using a race circuit and a complex mountain road as a simulation example, and the results indicated that the anticipated computational results could be obtained using the proposed decision-making model and algorithm. The result also showed a fairly good agreement with the trajectory and speed of a car on a practical circuit and mountain road.;Institute of Electrical and Electronics Engineers Inc.;Journal;IEEE Transactions on Intelligent Transportation Systems;2019-01-01;https://api.elsevier.com/content/abstract/scopus_id/85043762518
101;Collision Probability Computation for Road Intersections Based on Vehicle to Infrastructure Communication;In recent years, many probability models proposed to calculate the collision probability for each vehicle and those models used in collision avoidance algorithms and intersection management algorithms. In this paper, we introduce a method to calculate the collision probability of vehicles at an urban intersection. The proposed model uses the current position, speed, acceleration, and turning direction then each vehicle shares its required information to the roadside unit (RSU) via the Vehicle to Infrastructures (V2I). RSU can predict each vehicle's path in intersections by using the received data. By considering vehicle dimensions in our calculation, RSU will detect a possible collision point and time to collision (TTC) for moving vehicles at the intersection. Simulation results show that this model can detect collisions occurrence early, so it will decrease the probability of a collision occurs.;Institute of Electrical and Electronics Engineers Inc.;Conference Proceeding;Proceedings of the International Conference on Microelectronics, ICM;2020-12-14;https://api.elsevier.com/content/abstract/scopus_id/85100585747
102;Research on Digital Twin Technology for Production Line Design and Simulation;With the rapid development of big data, artificial intelligence and internet of things, digital twin technology becomes a new research hotspot in the field of intelligent manufacturing. In this paper, the digital twin technology for production line design and simulation is studied. Emphasis is laid on the building and fusion of production line model, virtual-real mapping and real-time interaction technology and virtual production line simulation and verification technology. The research content of this paper provides theoretical and technical reference for the application of digital twins in the design and implementation of manufacturing production line.;Springer;Book Series;Advances in Intelligent Systems and Computing;2020-01-01;https://api.elsevier.com/content/abstract/scopus_id/85076559011
103;OPC UA: From Automation Pyramid to Information Network;Fast Forward • The basic concepts for the use case controller-to-controller have been developed and have been incorporated into a first set of specifications and the initial release candidate completed. • Test specifications are being generated that will later be converted into corresponding test cases for the OPC UA Compliance Test Tool. • A second version of the specification will be extended for the use cases controller-to-device and device-to-device, so OPC UA can be used for uniform and consistent communication for vertical and horizontal integration.;ISA - Instrumentation, Systems, and Automation Society;Trade Journal;InTech;2021-06-01;https://api.elsevier.com/content/abstract/scopus_id/85132574244
104;Digital twin modeling for loaded contact pattern-based grinding of spiral bevel gears;To distinguish with the conventional tooth flank grinding only considering geometric accuracy, an innovative digital twin modeling is proposed for loaded contact pattern based grinding of spiral bevel gears. Where, data-driven grinding simulation, sensitivity analysis strategy, adaptive decision and control are developed. Focusing on loaded contact pattern optimization, numerical loaded tooth contact analysis (NLTCA) considering noncentrosymmetric problem and tooth flank roughness is developed for data-driven relationship establishment. Then, an adaptive data-driven tooth flank grinding decision and control model is established. Where, the universal motion concept (UMC) machine settings is selected as the optimal design variable. It is actually an infinite approximation to the target tooth flank in form of an adaptive control system. Moreover, with point-to-point material removal distribution, the different optimization strategies are proposed for accurate tooth flank grinding. In particular, the overcutting problem on the tooth flank grinding programming is investigated. Finally, Levenberg-Marquardt method is applied to solve the established nonlinear lease square model for the accurate machine tool settings having modification variations. Thus, this accurate data-driven digital twin modeling can achieve loaded contact pattern-based grinding. The provided numerical and test instances can verify the proposed digital twin modeling.;Elsevier Ltd;Journal;Advanced Engineering Informatics;2021-08-01;https://api.elsevier.com/content/abstract/scopus_id/85108381688
105;Handbook of re-engineering software intensive systems into software product lines;"This handbook distils the wealth of expertise and knowledge from a large community of researchers and industrial practitioners in Software Product Lines (SPLs) gained through extensive and rigorous theoretical, empirical, and applied research. It is a timely compilation of well-established and cutting-edge approaches that can be leveraged by those facing the prevailing and daunting challenge of re-engineering their systems into SPLs. The selection of chapters provides readers with a wide and diverse perspective that reflects the complementary and varied expertise of the chapter authors. This perspective covers the re-engineering processes, from planning to execution. SPLs are families of systems that share common assets, allowing a disciplined software reuse. The adoption of SPL practices has shown to enable significant technical and economic benefits for the companies that employ them. However, successful SPLs rarely start from scratch, but instead, they usually start from a set of existing systems that must undergo well-defined re-engineering processes to unleash new levels of productivity and competitiveness. Practitioners will benefit from the lessons learned by the community, captured in the array of methodological and technological alternatives presented in the chapters of the handbook, and will gain the confidence for undertaking their own re-engineering challenges. Researchers and educators will find a valuable single-entry point to quickly become familiar with the state-of-the-art on the topic and the open research opportunities; including undergraduate, graduate students, and R&D engineers who want to have a comprehensive understanding of techniques in reverse engineering and re-engineering of variability-rich software systems.";Springer International Publishing;Book;Handbook of Re-Engineering Software Intensive Systems into Software Product Lines;2022-11-22;https://api.elsevier.com/content/abstract/scopus_id/85160496931
106;Experimentable Digital Twins-Streamlining Simulation-Based Systems Engineering for Industry 4.0;"Digital twins represent real objects or subjects with their data, functions, and communication capabilities in the digital world. As nodes within the internet of things, they enable networking and thus the automation of complex value-added chains. The application of simulation techniques brings digital twins to life and makes them experimentable; digital twins become experimentable digital twins (EDTs). Initially, these EDTs communicate with each other purely in the virtual world. The resulting networks of interacting EDTs model different application scenarios and are simulated in virtual testbeds, providing new foundations for comprehensive simulation-based systems engineering. Its focus is on EDTs, which become more detailed with every single application. Thus, complete digital representations of the respective real assets and their behaviors are created successively. The networking of EDTs with real assets leads to hybrid application scenarios in which EDTs are used in combination with real hardware, thus realizing complex control algorithms, innovative user interfaces, or mental models for intelligent systems.";IEEE Computer Societyhelp@computer.org;Journal;IEEE Transactions on Industrial Informatics;2018-04-01;https://api.elsevier.com/content/abstract/scopus_id/85041837589
107;Application of systems modeling language (SysML) and discrete event simulation to address patient waiting time issues in healthcare;A robust health care system is crucial to reducing patient stress and contributing to economic growth. The future of the health care industry depends upon a reliable and efficient system to deal with the increasing number of patients. However, in today's healthcare system, patients face negative experiences as a result of long wait times. Now the pressing question is how to develop an effective healthcare system? To address this issue, this study uses Systems Modeling Language (SysML) coupled with a simulation approach to assess the performance of the healthcare system, identify the problem, and offer recommended alternatives. To elaborate, a systemic magic-grid methodology will be used to model and analyze the blood laboratory by using four pillars (structural, behavioral, requirement, and parametric) of SysML. To represent these pillars, a set of SysML diagrams will be used to visualize the layered system architecture, interactions, and activity between its different components. Furthermore, Discrete Event Simulation (DES) is utilized through Flexsim simulation software for the analysis of the parametric aspect of the system of interest. A blood laboratory within an outpatient clinic located at southern US State is considered a testing bed. The detailed architecture of the system of interest is studied, and required data are collected for modeling and simulation. The simulation results indicate that the combination of 50% Type I routes and 50% Type II routes resulted in the shortest wait times in the system of 22 min, the shortest wait times in the phlebotomist queue of 2 min, and the highest system throughput of 11369 patients per nine months. This article will provide a reference point for practitioners who want to apply the SysML approach to address health sector-related issues. More importantly, with this comprehensive approach, stakeholders of the blood laboratory system can utilize the hospital infrastructure in a more effective and optimized manner.;Elsevier B.V.;Journal;Smart Health;2023-09-01;https://api.elsevier.com/content/abstract/scopus_id/85163703822
108;Implementation of interactive control of a crane ship model in MATLAB/Simulink environment;The increased demand for performing crane ship modeling has led to the necessity for fast and accurate numerical experiments. This paper presents an approach for creating a numerical model of a crane vessel with a suspended load that allows for real-time control of crane parts. The model is developed in the MATLAB/Simulink environment, which makes it possible to extend it further to the user's needs. The authors describe the approach to the calculation of wave-induced ship motions, presents the Simulink block model and describes the features encountered during the simulation process. The possibility of real-time control of the position of crane parts is also shown, keeping the calculation speed of the ship hydrodynamics.;Springer Science and Business Media Deutschland GmbH;Journal;ROBOMECH Journal;2023-12-01;https://api.elsevier.com/content/abstract/scopus_id/85165726281
109;An integrated framework for automated simulation of SysML models using DEVS;System models are constructed to design, study, and understand complex systems. According to the systems modeling language (SysML) that is a standard for model-based system engineering, all engineering activities should be performed using a common model. To validate complex system models defined in SysML, simulation is usually employed. There are numerous efforts to simulate SysML models using different simulation methods and tools. However, the efficient support of automated generation of executable simulation code is still an issue tangled by the research community. This paper introduces DEVSys, an integrated framework for utilizing existing SysML models and automatically producing executable discrete event simulation code, according to model driven architecture (MDA) concepts. Although this approach is not simulation-specific, discrete event system specification (DEVS) was employed, due to the similarities between SysML and DEVS, mainly in system structure description, and the mature, yet ongoing research on expressing executable DEVS models in a simulator-neutral manner. DEVSys framework elements include (a) a SysML profile for DEVS, enabling integration of simulation capabilities into SysML models, (b) a meta-model for DEVS, allowing the utilization of MDA concepts and tools, (c) a transformation of SysML models to DEVS models, using a standard model transformation language as query/view/transform (QVT), and (d) the generation of DEVS executable code for a DEVS simulation environment with an extensible markup language (XML) interface. The definition and implementation of DEVSys elements, as well as the process for its application are demonstrated and discussed, with the aid of a simple working example. © 2014 The Society for Modeling and Simulation International.;SAGE Publications Ltdinfo@sagepub.co.uk;Journal;Simulation;2014-01-01;https://api.elsevier.com/content/abstract/scopus_id/84901825375
110;Simulating SysML models: Overview and challenges;SysML language, proposed by OMG, is a commonly accepted standard to model and study systems-of-systems (SoS). It provides the means to depict SoS components and their behavior in a multi-layer fashion and explore alterative architectures for their design. To validate SysML models in terms of performance criteria, simulation is usually the preferred method employed. To this end, different SysML diagrams are utilized, while numerous simulation methodologies and tools are employed. There are many efforts targeting simulation code generation from SysML models. Model-based system engineering concepts are adopted in most of them to generate simulation models from SysML models. Nevertheless, this process is not standardized, although most of current approaches tend to follow the same steps, even if they employ different tools. The scope of this paper is to provide a comprehensive understanding of the similarities and differences of existing approaches and identify current challenges in fully automating SysML models simulation process.;Institute of Electrical and Electronics Engineers Inc.;Conference Proceeding;2015 10th System of Systems Engineering Conference, SoSE 2015;2015-07-07;https://api.elsevier.com/content/abstract/scopus_id/84941126243
111;Effectiveness of Workstation Towards Production Performance in Manufacturing Company Using Arena Simulation;Workstation arrangement is one of the factors that can give a significant impact on the company’s performance. Poor implementation of the workstation can lead to an ineffective and inefficient work system which may limit the rate of productivity, poor utilization of resources, high rate of rejection, and many more. For the company of this study, which is XYZ Furniture Sdn Bhd, they face limited production over a certain period by using existing machines which can make the production rate become inefficient. Thus, this study is conducted to solve the problem by proposing several solutions to the company. Based on the findings obtained from this method, the researchers found that there are some shortcomings in the workstation layout that has been used in the factory that causes them to be unable to maximize daily production and cause them losses due to having to work overtime to ensure their daily targets are achieved. The use of this ARENA simulation method is important because this application can create an overview of the movement of a process with more organized and accurate simulation results. Thus, this study is conducted to solve the problem by proposing several solutions to the company. Based on the result, there was a positive increase of 34 units of wood pieces equivalent to 7.08%. This improvement is very good and gives a considerable impact on the company and can help the company in growing its business in the future.;American Institute of Physics Inc.;Conference Proceeding;AIP Conference Proceedings;2023-05-22;https://api.elsevier.com/content/abstract/scopus_id/85163131787
112;Modeling of polymer-enzyme conjugates formation: Thermodynamic perturbation theory and computer simulations;A simple model for the formation of the polymer-enzyme conjugates has been proposed and described using corresponding modification of the Wertheim's first-order thermodynamic perturbation theory (TPT) for the system of associating chain molecules. A set of computer simulation data for the polymer chains containing various number of functional groups was generated and used to testify the accuracy of the theoretical results. Predictions of the present theoretical approach are more accurate than that of the conventional TPT and are in a very good agreement with the computer simulation data. In particular, the theory is able to account for the difference in the position of the polymer functional groups along its backbone.;Elsevier B.V.;Journal;Journal of Molecular Liquids;2023-09-01;https://api.elsevier.com/content/abstract/scopus_id/85162090568
113;SysML in Action With Cameo Systems Modeler;System engineering (SE) using models (MBSE) is currently in vogue in the community of SE practitioners, whether they are analysts, architects, developers or testers. INCOSE has contributed greatly to the definition of a language for the community, henceforth standardized under ISO-19514: SysML. However, this language is not associated by default with any particular MBSE procedure. This is a major difficulty hampering its implementation. In order to overcome this difficulty, this book describes, in addition to the SysML notation, a generic approach based on the main principles of SE and relative standards, serving as the basis for a specific MBSE approach to be built. This is in order to respond to the specificities of the field of projects in which the practitioners evolve. In order to carry out the procedure in a pragmatic way, a simplified but realistic example serves as a guideline from the initial requirements to the validation of the system, putting into action the SysML modeling tool Cameo Systems Modeler by No Magic.;Elsevier;Book;SysML in Action with Cameo Systems Modeler;2017-11-17;https://api.elsevier.com/content/abstract/scopus_id/85054243292
114;Design and Development of a Geometric Calculator in CATIA;"In this article, an application in the field of engineering graphics is presented for the design of a geometric calculator generated as a macro in CATIA V5. The code of this macro is written in the CATVBA language and utilizes the CATIA internal editor while taking advantage of the capabilities offered by Visual Basic for Applications (VBA). The principal purpose of this application lies in the possibility of creating the three main geometric elements (point, line, and plane) and in solving five types of general geometric problems, and then comparing the results obtained with their equivalent problems from analytical geometry. In particular, within these types of general geometric problems, 34 possible cases are solved: definition of lines (nine cases), definition of planes (12 cases), intersection points (three cases), angles (three cases), and distances (seven cases). These new entities defined with the geometric calculator can serve as support for the generation of new three-dimensional volumes, the creation of auxiliary symmetries, and the dimensioning of various elements. It was verified that the results of the designed macro and the solutions of the analytical equations coincided; therefore, the procedure was validated. Likewise, the module employed herein in the CATIA V5 environment is “Wireframe and Surface Design”, since it enables handling the three basic geometric elements (point, line, and plane), which form the basis of the geometric calculator. Lastly, it is verified how the geometric calculator allows their integration with three-dimensional solids, which represents a notable advance as an aid in its geometric definition.";MDPI;Journal;Symmetry;2023-02-01;https://api.elsevier.com/content/abstract/scopus_id/85149242305
115;Environmental integrity as a modeler of the composition of the Odonata community;Human actions often alter natural environments, causing homogenization of micro-habitats and, consequently, the loss or replacement of species. Our research evaluates how the effects of environmental integrity and the physical and chemical characteristics of streams influence the adult Odonata community in a region of the Amazon, in western Pará. The data were obtained in 15 streams of first and second order in the municipality of Santarém, Pará, between October and December 2014 (dry season) and between March and May 2015 (rainy season). A total of 544 specimens were collected, distributed in 23 genera, 35 species. Significant differences were observed in the composition of Odonata based on the integrity of streams, and species are replaced as the habitat integrity gradient is reduced, with species that need more preserved conditions extinct locally, making room for generalist species. However, only Psaironeura tenuissima was an indicator of more preserved sites, while Argia sp.1 and Mnesarete smaragdina were indicative of altered sites. None of the variables had any influence on the richness or abundance of Odonata.;Springer Science and Business Media Deutschland GmbH;Journal;Environmental Monitoring and Assessment;2021-04-01;https://api.elsevier.com/content/abstract/scopus_id/85102101255
116;Correction to: Capacity and stability on some Cegrell classes of m − subharmonic functions (Collectanea Mathematica, (2023), 74, 3, (817-835), 10.1007/s13348-022-00374-5);Unfortunately, the affiliation numbers of the authors is not updated correctly in the online published article the correct author names and affiliations are as follows.;Springer Nature;Journal;Collectanea Mathematica;2023-09-01;https://api.elsevier.com/content/abstract/scopus_id/85141067275
117;Simulation of Photon-Counting Detector Based on OpenModelica;Photon-counting Computed Tomography (CT) is an emerging technology based on new energy-resolving X-ray detectors, providing promising performance compared to traditional methods which uses energy-integrating detectors (EID). Recent advances in photon-counting detectors (PCD) result in higher spatial resolution, less artefacts, improved contrast-to-noise ratio, and lower dose, which has led to significant research interest. At a proof-of-principle level, this paper covers the simulation for various processes during photon-counting CT to illustrate its basic architecture, detection mechanism, electrical signal response, and factors influencing the detection performance. The simulation work is implemented by using OpenModelica. In OpenModelica, components involved in photon-counting CT are built. Simulation results are provided for demonstrating characterization of photon-counting detectors, the obtained results of electronic signal responses are compared with the referenced studied, which showed satisfied agreement, and the principle of pile-up rejection circuit is demonstrated. This work of demonstration proved that the Modelica language as well as the OpenModelica platform are capable to be deployed in the study of nuclear medical imaging in a convenient and visualized way by taking advantage of reusable object-oriented models.;Springer Science and Business Media Deutschland GmbH;Conference Proceeding;Springer Proceedings in Physics;2023-01-01;https://api.elsevier.com/content/abstract/scopus_id/85161222528
118;Challenges in the modelling of SoS design alternatives with MBSE;A central promise of Model-Based Systems Engineering (MBSE) is to decrease the volume of information produced during the system design lifecycle through improved structure and smarter reuse across the numerous perspectives and architectural levels. The study of variants is a core challenge within MBSE due to associated exponential information growth, and the complexity created by this extra dimensionality. Variant management has been discussed extensively for the physical layer, but lacks research focusing on functional and logical layers, particularly relevant to concept phase engineering. In System of Systems (SoS) problems, the physical choices are typically constrained by the fact that the interacting systems already exist. This paper therefore studies the implications of variant modelling for the behaviour of the SoS. It explores the possibility of developing a functional architecture that can be extended to accommodate changes due to decisions made in the logical layer. It investigates the formal aspects of including variant modelling into the Systems Modelling Language (SysML) and discusses the potential need for a language extension.;Institute of Electrical and Electronics Engineers Inc.;Conference Proceeding;2016 11th Systems of Systems Engineering Conference, SoSE 2016;2016-08-12;https://api.elsevier.com/content/abstract/scopus_id/84985946863
119;Reducing the cost and complexity of variant exploration with MBSE & LSP;Variant exploration is often construed as synonymous with a significant increase in work and a decrease in clarity. Model-Based Systems Engineering (MBSE) has been heralded as a potential solution to streamline the process by supporting exploration of numerous parallel variants within a common multidimensional model. Successful decoupling of variants from their surroundings supports the parallel development of interfacing systems without delay or impediment, thereby permitting the deferment of a final decision on the preferred technology choice. Still, a number of challenges remain. Some have been addressed by previous research, though, not focusing on the effect of variant technology choices in the logical tier, nor on embedding variant management as a native part of the core MBSE model. This paper addresses those challenges via application of Liskov's Substitution Principle (LSP). It defines an MBSE based approach that explicitly documents variance points, identifying their source and scope, and integrates their management into the standard Systems Engineering (SE) workflow. Emphasis is given on providing a pragmatic approach that promotes consistency, visibility, and simplicity. The result is a model that embeds the required information artifacts without adding significant complexity. The proposed strategy does not prescribe excessive variant-specific activities, with the majority of the effort being already part of standard Requirement, Interface, and Logical SE processes. It is tolerant to an iterative approach, and proactively supports concurrent engineering through decoupling.;Institute of Electrical and Electronics Engineers Inc.;Conference Proceeding;ISSE 2016 - 2016 International Symposium on Systems Engineering - Proceedings Papers;2016-11-22;https://api.elsevier.com/content/abstract/scopus_id/85006427764
120;IMPLEMENTATION OF ARTIFICIAL POTENTIAL FIELDS AND LYAPUNOV STABILITY AND CONTROL IN OBSTACLES AVOIDANCE OF MOBILE ROBOT USING ROS GAZEBO;"This paper presents two intelligent methods of robotics; the artificial potential field (APF) and Lyapunov stability methods as they are both designed to ensure that the robot stays clear of immovable objects or obstacles and moves in the most effective way possible toward the target. Furthermore, to address robot path planning issues in real-time using these methods, the robot can move to the target in an optimal environment while avoiding obstacles. It can also reach the target point in a limited time and choose the best and the shortest possible path. Additionally, when it calculates the best path, the robot would be obliged to move to the chosen target as the control and stability algorithm guarantees that efficiency. Moreover, the error percentage of the Lyapunov stability method would be almost zero. Ros (Robotic Operating System) Gazebo with robot waffle_Pi type was used for the simulation results demonstration.";Little Lion Scientific;Journal;Journal of Theoretical and Applied Information Technology;2023-02-28;https://api.elsevier.com/content/abstract/scopus_id/85152738315
121;Generalised Regression Neural Network (GRNN) Architecture-Based Motion Planning and Control of an E-Puck Robot in V-REP Software Platform;This article focuses on the motion planning and control of an automated differential-driven two-wheeled E-puck robot using Generalized Regression Neural Network (GRNN) architecture in the Virtual Robot Experimentation Platform (V-REP) software platform among scattered obstacles. The main advantage of this GRNN over the feedforward neural network is that it provides accurate results in a short period with minimal error. First, the designed GRNN architecture receives real-time obstacle information from the Infra-Red (IR) sensors of an E-puck robot. According to IR sensor data interpretation, this architecture sends the left and right wheel velocities command to the E-puck robot in the V-REP software platform. In the present study, the GRNN architecture includes the MIMO system, i.e., multiple inputs (IR sensors data) and multiple outputs (left and right wheel velocities). The three-dimensional (3D) motion and orientation results of the GRNN architecture-controlled E-puck robot are carried out in the V-REP software platform among scattered and wall-type obstacles. Further on, compared with the feedforward neural network, the proposed GRNN architecture obtains better navigation path length with minimum error results.;Sciendo;Journal;Acta Mechanica et Automatica;2021-12-01;https://api.elsevier.com/content/abstract/scopus_id/85121033510
122;A virtual testbed for underwater robotics application to control design for auvs;Developing and operating autonomous underwater vehicles (AUVs) is a complex, hazardous, and expensive task. In order to help engineers reduce the involved costs and risks, we establish a Virtual Testbed for underwater robotics. We provide a concept for holistic and realistic 3D simulation systems including dynamic rigid body simulation for underwater scenarios as well as actuator and sensor simulation, which can be used for development, testing, and evaluation of AUV models. The real-world problem of control design for AUV reference tracking is utilized as example and tackled by developing three different linear controllers for the REMUS AUV. Finally, we perform simulated experiments to evaluate and compare the different controllers.;EUROSIS;Conference Proceeding;12th International Industrial Simulation Conference 2014, ISC 2014;2014-01-01;https://api.elsevier.com/content/abstract/scopus_id/84922167841
123;Robust fault tolerant manoeuvring and rudder control of marine vehicles with state saturation;In this paper, the problem of rudder-roll damping control for a class of cruise keeping ships subject to both actuator fault and state saturation is investigated. A manoeuvring and rudder control model of marine vessels is firstly reviewed based on the existing literature. Actuator faults which including partial loss of rudder effectiveness and actuator-bias faults are considered in the model. Then a switching control strategy is developed to compensate for the actuator faults and to guarantee the stability of the rudder-roll damping control system. Moreover, some relationship regarding state saturation bounds, actuator fault limits and control parameters are analyzed in this paper. Finally, simulation results are given to illustrate the proposed procedures and their effectiveness.;Institute of Electrical and Electronics Engineers Inc.;Conference Proceeding;Proceedings of the IEEE International Conference on Industrial Technology;2016-05-19;https://api.elsevier.com/content/abstract/scopus_id/84974588273
124;Software-defined Cloud Manufacturing with Edge Computing for Industry 4.0;Industrial trends and new generation information and communication technologies have become driving forces for advancement in the process control and manufacturing industry. This paper thoroughly investigates the future industrial trends from the perspectives of market, engineering system, product, innovation, etc., then incorporates the concept of software defined networking and proposes a new cloud based manufacturing model, Software Defined Cloud Manufacturing (SDCM). The key characteristics, reference architecture and emerging enabling technologies of SDCM are presented to support the SDCM's advantages in terms of real-time response, reconfiguration and operations of the manufacturing system. Resource virtualization and function programmability lie at the core of SDCM to empower the manufacturing sector. The paper is concluded with remarks and future work.;Institute of Electrical and Electronics Engineers Inc.;Conference Proceeding;2020 International Wireless Communications and Mobile Computing, IWCMC 2020;2020-06-01;https://api.elsevier.com/content/abstract/scopus_id/85089679811
125;Digital Twin in manufacturing: A categorical literature review and classification;The Digital Twin (DT) is commonly known as a key enabler for the digital transformation, however, in literature is no common understanding concerning this term. It is used slightly different over the disparate disciplines. The aim of this paper is to provide a categorical literature review of the DT in manufacturing and to classify existing publication according to their level of integration of the DT. Therefore, it is distinct between Digital Model (DM), Digital Shadow (DS) and Digital Twin. The results are showing, that literature concerning the highest development stage, the DT, is scarce, whilst there is more literature about DM and DS.;Elsevier B.V.;Conference Proceeding;IFAC-PapersOnLine;2018-01-01;https://api.elsevier.com/content/abstract/scopus_id/85052915281
126;A Cross-Domain Systematic Mapping Study on Software Engineering for Digital Twins;Digital Twins are currently investigated as the technological backbone for providing an enhanced understanding and management of existing systems as well as for designing new systems in various domains, e.g., ranging from single manufacturing components such as sensors to large-scale systems such as smart cities. Given the diverse application domains of Digital Twins, it is not surprising that the characterization of the term Digital Twin, as well as the needs for developing and operating Digital Twins are multi-faceted. Providing a better understanding what the commonalities and differences of Digital Twins in different contexts are, may allow to build reusable support for developing, running, and managing Digital Twins by providing dedicated concepts, techniques, and tool support. In this paper, we aim to uncover the nature of Digital Twins based on a systematic mapping study which is not limited to a particular application domain or technological space. We systematically retrieved a set of 1471 unique publications of which 356 were selected for further investigation. In particular, we analyzed the types of research and contributions made for Digital Twins, the expected properties Digital Twins have to fulfill, how Digital Twins are realized and operated, as well as how Digital Twins are finally evaluated. Based on this analysis, we also contribute a novel feature model for Digital Twins from a software engineering perspective as well as several observations to further guide future software engineering research in this area.;Elsevier Inc.;Journal;Journal of Systems and Software;2022-11-01;https://api.elsevier.com/content/abstract/scopus_id/85135797712
127;Knowledge map and forecast of digital twin in the construction industry: State-of-the-art review using scientometric analysis;"With the advancement of Industry 4.0 and the development of science and technology, industries have seen the advantages of digital twin (DT) implementations and concepts. Although the construction industry is still in the early stage of DT development and implementation, it is urgent for the industry to have a DT agenda. This paper identifies 1158 related bibliographic records from the Web of Science and 745 records from the Derwent Patent Database and quantitatively analyzed them to describe patterns of publications. Two knowledge mapping tools were selected to visualize and analyze the literature in the relevant scientific domain. Then, we integrated clustering, knowledge mapping, and network analysis methods to show the current foci and future directions of DT-MT derived from the data of time distribution, journal areas, and subject distribution. The results indicate that DT is a cross-disciplinary information technology with great development potential, and its development in the construction industry is scattered with the leading advancement in the manufacturing of construction materials and components. The derived knowledge map shows three stages of construction DT, including (1) data collection and creation of DT framework and database; (2) implementation of planning and design modules in detail and control of construction sites; and (3) carrying out asset management and fault prediction in the operation and maintenance phase of projects. The novelty of this paper is the specific analysis of DT in the construction industry. This study adds value to the architecture, engineering, and construction industries by shedding light on the focus of DT practical applications and the formation of a development path of construction DT.";Elsevier Ltd;Journal;Journal of Cleaner Production;2023-01-10;https://api.elsevier.com/content/abstract/scopus_id/85143719809
128;Collaborative Model-Driven Software Engineering — A systematic survey of practices and needs in industry;The engineering of modern software-intensive systems is carried out in collaboration among stakeholders with specialized expertise. The complexity of such systems often also necessitates employing more rigorous approaches, such as Model-Driven Software Engineering (MDSE). Collaborative MDSE is the combination of the two disciplines, with its specific opportunities and challenges. The rapid expansion and maturation of the field started attracting tool builders from outside of academia. However, available systematic studies on collaborative MDSE focus exclusively on mapping academic research and fail to identify how academic research aligns with industry practices and needs. To address this shortcoming, we have carried out a mixed-method survey on the practices and needs concerning collaborative MDSE. First, we carried out a qualitative survey in two focus group sessions, interviewing seven industry experts. Second, based on the results of the interviews, we constructed a questionnaire and carried out a questionnaire survey with 41 industry expert participants. In this paper, we report the results of our study, investigate the alignment of academic research with the needs of practitioners, and suggest directions on research and development of the supporting techniques of collaborative MDSE.;Elsevier Inc.;Journal;Journal of Systems and Software;2023-05-01;https://api.elsevier.com/content/abstract/scopus_id/85147541731
129;Comparing Modeling Approaches for the Multi-Level Capacitated Lot-Sizing and Scheduling Problem;Determining lot sizes is an essential step during the material requirements planning phase influencing total production cost and total throughput time of a production system. It is well-known that lot-sizing and scheduling decisions are intertwined. Neglecting this relation, as it is done in the classical hierarchical planning approach, leads to inefficient and sometimes infeasible plans. In this work we compare different approaches for integrating the lot-sizing and the scheduling decisions in multi-stage systems. We show their abilities and limitations in describing relevant aspects of a production environment. By applying the models to benchmark instances we analyze their computational behavior. The structural and numerical comparisons show that there are considerable differences between the approaches although all models aim to describe the same planning problem. The results provide a guideline for selecting the right modeling approach for different planning situations.;Springer Science and Business Media Deutschland GmbH;Book Series;Dynamic Modeling and Econometrics in Economics and Finance;2016-01-01;https://api.elsevier.com/content/abstract/scopus_id/85131015319
130;Multi-dimensional multi-level modeling;The growth of multi-level modeling has resulted in an increase of level-organization alternatives which significantly differ from each other with respect to their underlying foundations and the well-formedness rules they enforce. Alternatives substantially diverge with respect to how level boundaries should govern instance-of relationships, what modeling mechanisms they employ, and what modeling principles they establish. In this article, I analyze how a number of multi-level modeling approaches deal with certain advanced modeling scenarios. In particular, I identify linear domain metamodeling, i.e., the requirement that all domain-induced instance-of relationships align with a single global level-hierarchy, as a source of accidental complexity. I propose a novel multi-dimensional multi-level modeling approach based on the notion of orthogonal ontological classification that supports modeling of domain scenarios with minimal complexity while supporting separation of concerns and sanity-checking to avoid inconsistent modeling choices.;Springer Science and Business Media Deutschland GmbH;Journal;Software and Systems Modeling;2022-04-01;https://api.elsevier.com/content/abstract/scopus_id/85123068734
131;Model-Driven Development of a Digital Twin for Injection Molding;Digital Twins (DTs) of Cyber-Physical Production Systems (CPPSs) enable the smart automation of production processes, collection of data, and can thus reduce manual efforts for supervising and controlling CPPSs. Realizing DTs is challenging and requires significant efforts for their conception and integration with the represented CPPS. To mitigate this, we present an approach to systematically engineering DTs for injection molding that supports domain-specific customizations and automation of essential development activities based on a model-driven reference architecture. In this approach, reactive CPPS behavior is defined in terms of a Domain-Specific Language (DSL) for specifying events that occur in the physical system. The reference architecture connects to the CPPS through a novel DSL for representing OPC-UA bindings. We have evaluated this approach with a DT of an injection molding machine that controls the machine to optimize the Design of Experiment (DoE) parameters between experiment cycles before the products are molded. Through this, our reference implementation of the DT facilitates the time-consuming setup of a DT and the subsequent injection molding activities. Overall, this facilitates to systematically engineer digital twins with reactive behavior that help to optimize machine use.;Springer;Book Series;Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics);2020-01-01;https://api.elsevier.com/content/abstract/scopus_id/85086228557
132;Model-driven digital twin construction: Synthesizing the integration of cyber-physical systems with their information systems;Digital twins emerge in many disciplines to support engineering, monitoring, controlling, and optimizing cyber-physical systems, such as airplanes, cars, factories, medical devices, or ships. There is an increasing demand to create digital twins as representation of cyber-physical systems and their related models, data traces, aggregated data, and services. Despite a plethora of digital twin applications, there are very few systematic methods to facilitate the modeling of digital twins for a given cyber-physical system. Existing methods focus only on the construction of specific digital twin models and do not consider the integration of these models with the observed cyber-physical system. To mitigate this, we present a fully model-driven method to describe the software of the cyber-physical system, its digital twin information system, and their integration. The integration method relies on MontiArc models of the cyber-physical system's architecture and on UML/P class diagrams from which the digital twin information system is generated. We show the practical application and feasibility of our method on an IoT case study. Explicitly modeling the integration of digital twins and cyber-physical systems eliminates repetitive programming activities and can foster the systematic engineering of digital twins.;Association for Computing Machinery, Inc;Conference Proceeding;Proceedings - 23rd ACM/IEEE International Conference on Model Driven Engineering Languages and Systems, MODELS 2020;2020-10-16;https://api.elsevier.com/content/abstract/scopus_id/85096992990
133;Digital Twin Platforms: Requirements, Capabilities, and Future Prospects;Digital twins (DTs) have emerged as a paradigm for the virtual representation of complex systems alongside their underlying hardware. We investigate the benefits of Amazon, Eclipse, and Microsoft DT platforms and assess the extent to which they meet standard requirements.;IEEE Computer Society;Journal;IEEE Software;2022-01-01;https://api.elsevier.com/content/abstract/scopus_id/85121363965
134;AML4DT: A Model-Driven Framework for Developing and Maintaining Digital Twins with AutomationML;As technologies such as the Internet of Things (IoT) and Cyber-Physical Systems (CPS) are becoming ubiquitous, systems adopting these technologies are getting increasingly complex. Digital Twins (DTs) provide comprehensive views on such systems, the data they generate during runtime, as well as their usage and evolution over time. Setting up the required infrastructure to run a Digital Twin is still an ambitious task that involves significant upfront efforts from domain experts, although existing knowledge about the systems, such as engineering models, may be already available for reuse. To address this issue, we present AML4DT, a model-driven framework supporting the development and maintenance of Digital Twin infrastructures by employing AutomationML (AML) models. We automatically establish a connection between systems and their DTs based on dedicated DT models. These DT models are automatically derived from existing AutomationML models, which are produced in the engineering phases of a system. Additionally, to alleviate the maintenance of the DTs, AML4DT facilitates the synchronization of the AutomationML models with the DT infrastructure for several evolution cases. A case study shows the benefits of developing and maintaining DTs based on AutomationML models using the proposed AML4DT framework. For this particular study, the effort of performing the required tasks could be reduced by about 50%.;Institute of Electrical and Electronics Engineers Inc.;Conference Proceeding;IEEE International Conference on Emerging Technologies and Factory Automation, ETFA;2021-01-01;https://api.elsevier.com/content/abstract/scopus_id/85122957318
135;Language engineering with the GEMOC studio;This tutorial provides a practical approach for developing and integrating various Domain-Specific (modeling) Languages (DSLs) used in the development of modern complex software-intensive systems, with the main objective to support abstraction and separation of concerns. The tutorial leverages on the tooling provided by the GEMOC studio to present the various facilities offered by the Eclipse platform (incl., EMF/Ecore, Xtext, Sirius) and introduces the advanced features to extend a DSL with a well-defined execution semantics, possibly including formal concurrency constraints and coordination patterns. From such a specification, we demonstrate the ability of the studio to automatically support model execution, graphical animation, omniscient debugging, concurrency analysis and concurrent execution of heterogeneous models. The tutorial is composed of both lectures and hands-on sessions. Hands-on sessions allow participants to experiment on a concrete use case of an architecture description language used to coordinate heterogeneous behavioral and structural components.;Institute of Electrical and Electronics Engineers Inc.;Conference Proceeding;Proceedings - 2017 IEEE International Conference on Software Architecture Workshops, ICSAW 2017: Side Track Proceedings;2017-06-23;https://api.elsevier.com/content/abstract/scopus_id/85025599751
136;Design patterns: Abstraction and reuse of object-oriented design;We propose design patterns as a new mechanism for expressing object-oriented design experience. Design patterns identify, name, and abstract common themes in object-oriented design. They capture the intent behind a design by identifying objects, their collaborations, and the distribution of responsibilities. Design patterns play many roles in the object-oriented development process: they provide a common vocabulary for design, they reduce system complexity by naming and defining abstractions, they constitute a base of experience for building reusable software, and they act as building blocks from which more complex designs can be built. Design patterns can be considered reusable micro-architectures that contribute to an overall system architecture. We describe how to express and organize design patterns and introduce a catalog of design patterns. We also describe our experience in applying design patterns to the design of object-oriented systems.;Springer Verlag;Book Series;Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics);1993-01-01;https://api.elsevier.com/content/abstract/scopus_id/37149024045
137;TemporalEMF: A temporal metamodeling framework;Existing modeling tools provide direct access to the most current version of a model but very limited support to inspect the model state in the past. This typically requires looking for a model version (usually stored in some kind of external versioning system like Git) roughly corresponding to the desired period and using it to manually retrieve the required data. This approximate answer is not enough in scenarios that require a more precise and immediate response to temporal queries like complex collaborative co-engineering processes or runtime models. In this paper, we reuse well-known concepts from temporal languages to propose a temporal metamodeling framework, called TemporalEMF, that adds native temporal support for models. In our framework, models are automatically treated as temporal models and can be subjected to temporal queries to retrieve the model contents at different points in time. We have built our framework on top of the Eclipse Modeling Framework (EMF). Behind the scenes, the history of a model is transparently stored in a NoSQL database. We evaluate the resulting TemporalEMF framework with an Industry 4.0 case study about a production system simulator. The results show good scalability for storing and accessing temporal models without requiring changes to the syntax and semantics of the simulator.;Springer Verlagservice@springer.de;Book Series;Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics);2018-01-01;https://api.elsevier.com/content/abstract/scopus_id/85054785176
138;Refactoring the anemic domain model using pattern of enterprise application architecture and its impact on maintainability: A case study;Design pattern is a set of solutions that is used to solve software development common problems. The purpose of design pattern utilization is to improve software quality. Various design patterns have been proposed. One of them is Patterns of Enterprise Application Architecture (PoEAA) which are specified for enterprise application. However, there are lacks of literature that discuss these patterns. This research conducts a quantitative study to assess the impact of design pattern on software maintainability. We use Academic Information System of Institut Teknologi Sepuluh Nopember as a case study. It is an enterprise software which has Anemic Domain Model. We perform refactoring into the existing systems using suitable PoEAA. We measure its maintainability using C&K and three additional metrics, prior and after the refactoring process. The measurement results are then evaluated to obtain the impact. Based on the experiments, we clearly observe that PoEAA utilization could significantly restructure the anemic domain model of AIS. The maintainability is increased especially in presentation layer. PoEAA also eliminates duplicated methods in service and repository layer of the existing version of AIS. However, there are several drawbacks of the improvements.;International Association of Engineersijam@iaeng.org;Journal;IAENG International Journal of Computer Science;2019-05-01;https://api.elsevier.com/content/abstract/scopus_id/85066278598
139;Temporal Models on Time Series Databases;With the emergence of Cyber-Physical Systems (CPS), several sophisticated runtime monitoring solutions have been proposed in order to deal with extensive execution logs. One promising development in this respect is the integration of time series databases that support the storage of massive amounts of historical data as well as to provide fast query capabilities to reason about runtime properties of such CPS. In this paper, we discuss how conceptual modeling can benefit from time series databases, and vice versa. In particular, we present how metamodels and their instances, i.e., models, can be partially mapped to time series databases. Thus, the traceability between design and simulation/runtime activities can be ensured by retrieving and accessing runtime information, i.e., time series data, in design models. On this basis, the contribution of this paper is four-fold. First, a dedicated profile for annotating design models for time series databases is presented. Second, a mapping for integrating the metamodeling framework EMF with InfluxDB is introduced as a technology backbone enabling two distinct mapping strategies for model information. Third, we demonstrate how continuous time series queries can be combined with the Object Constraint Language (OCL) for navigation through models, now enriched with derived runtime properties. Finally, we also present an initial evaluation of the different mapping strategies with respect to data storage and query performance. Our initial results show the efficiency of applying derived runtime properties as time series queries also for large model histories.;Association Internationale pour les Technologies Objets;Journal;Journal of Object Technology;2020-01-01;https://api.elsevier.com/content/abstract/scopus_id/85100020731
140;Transformation of UML and OCL models into filmstrip models;This contribution presents an automatic transformation from UML and OCL models into enriched UML and OCL models, so-called filmstrip models, which embody temporal information when employing OCL while maintaining the same functionality as the original model. The approach uses a combination of object and sequence diagrams that allows for a wide range of possible OCL constraints about sequences of operation calls and their temporal properties. The modeler does not need to account for such properties while creating the original model. Errors found by constraints for the filmstrip model can easily be related back to the original model, as the elements of the filmstrip model are synchronized with the original model and the backwards calculation is generally simple. The approach is implemented in a UML and OCL modeling tool. © 2014 Springer International Publishing Switzerland.;Springer Verlagservice@springer.de;Book Series;Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics);2014-01-01;https://api.elsevier.com/content/abstract/scopus_id/84958553065
141;Systems architecture design pattern catalog for developing digital twins;A digital twin is a digital replica of a physical entity to which it is remotely connected. A digital twin can provide a rich representation of the corresponding physical entity and enables sophisticated control for various purposes. Although the concept of the digital twin is largely known, designing digital twins based systems has not yet been fully explored. In practice, digital twins can be applied in different ways leading to different architectural designs. To guide the architecture design process, we provide a pattern-oriented approach for architecting digital twinbased Internet of Things (IoT) systems. To this end, we propose a catalog of digital twin architecture design patterns that can be reused in the broad context of systems engineering. The patterns are described using the well-known documentation template and support the various phases in the systems engineering life cycle process. For illustrating the application of digital twin patterns, we adopt a case study in the agriculture and food domain.;MDPI AGPostfachBaselCH-4005rasetti@mdpi.com;Journal;Sensors (Switzerland);2020-09-02;https://api.elsevier.com/content/abstract/scopus_id/85090276654
142;A DEVS-based engine for building digital quadruplets;Development of Embedded Real-Time Systems is prone to error, and developing bug-free applications is expensive and no guarantees can be provided. We introduce the concept of Digital Quadruplet which includes: a 3D virtual representation of the physical world (a Digital Twin), a Discrete-Event formal model of the system of interest (called the “Digital Triplet”), which can be used for formal analysis as well as simulation studies, and a physical model of the real system under study for experimentation (called the “Digital Quadruplet”). We focus on the definition of the idea of a Digital Quadruplet and how to make these four apparati consistent and reusable. To do so, we use the Discrete-Event formal model as a center for both simulation and execution of the real-time embedded components with timing constraints, as well as a common mechanism for interfacing with the digital counterparts, providing model continuity throughout the process. Here we focus on a principal part of the Digital Quadruplet idea: the provision of an environment to allow models to be used for simulation (in virtual time), visualization, or execution in real-time. A Discrete-EVent Systems specifications (DEVS) kernel runs on bare-metal hardware platforms, avoiding the use of an Operating RTOS in the platform, and the combination with discrete-event modeling engineering.;SAGE Publications Ltd;Journal;Simulation;2021-07-01;https://api.elsevier.com/content/abstract/scopus_id/85105525097
143;Enhancing digital twins through reinforcement learning;Digital Twins are core enablers of smart and autonomous manufacturing systems. Although they strive to represent their physical counterpart as accurately as possible, slight model or data errors will remain. We present an algorithm to compensate for those residual errors through Reinforcement Learning (RL) and data fed back from the manufacturing system. When learning, the Digital Twin acts as teacher and safety policy to ensure minimal performance. We test the algorithm in a sheet metal assembly context, in which locators of the fixture are optimally adjusted for individual assemblies. Our results show a fast adaption and improved performance of the autonomous system.;IEEE Computer Societyhelp@computer.org;Conference Proceeding;IEEE International Conference on Automation Science and Engineering;2019-08-01;https://api.elsevier.com/content/abstract/scopus_id/85072946861
144;Development of Digital Twin for Load Center on the Example of Distribution Network of an Urban District;The paper proposes a concept of building a digital twin based on the reinforcement learning method. This concept allows implementing an accurate digital model of an electrical network with bidirectional automatic data exchange, used for modeling, optimization, and control. The core of such a model is an agent (potential digital twin). The agent, while constantly interacting with a physical object (electrical grid), searches for an optimal strategy for active network management, which involves short-term strategies capable of controlling the power supplied by generators and/or consumed by the load to avoid overload or voltage problems. Such an agent can verify its training with the initial default policy, which can be considered as a teacher's advice. The effectiveness of this approach is demonstrated on a test 77-node scheme and a real 17-node network diagram of the Akademgorodok microdistrict (Irkutsk) according to the data from smart electricity meters.;EDP Sciences;Conference Proceeding;E3S Web of Conferences;2020-11-23;https://api.elsevier.com/content/abstract/scopus_id/85097646193
145;Behavior-Centered Digital-Twin Design for Dynamic Cyber-Physical System Development;Digital Twins are digital models of Cyber-Physical Systems to enable not only continuous monitoring but also active functional improvement of networked services, physical products, machines and devices. This capacity is of utmost importance when recognizing and exploring business opportunities in terms of organizational and technology innovations, as well as enriching the scope of system-relevant applications. Before being operated in their target ecosystems, such as smart cities, Cyber-Physical Systems can be validated and be run as Digital Twin through executable behavior models. The development of these models captures both, the horizontal, and the vertical integration of CPS components, thus allowing to consider specific system qualities, such as pollution effects of traffic. This article investigates methodological and technological aspects of developing and operating Digital Twins along system transformation processes. We consider integration depth and breadth, connectivity, organizational intelligence, validation, and implementation variability in the context of human-centered modeling and development. The approach enriches the understanding of digital twins towards digital representation of Cyber-Physical Systems allowing for dynamic allocation of physical and digital parts according to operational conditions. An exemplary case study in traffic management demonstrates the feasibility and practicability of the communication-centered approach.;Riga Technical University;Journal;Complex Systems Informatics and Modeling Quarterly;2022-01-01;https://api.elsevier.com/content/abstract/scopus_id/85141817900
146;Advancement of Digital Twin in Irrigation and Smart Farming;"Agriculture and food production has been immensely impacted by digitalization; paving the way for advanced data processing techniques and technologies possible in the field of agriculture. The aim of Smart farming is to extract information from agricultural entities to resolve issues and challenges faced with regard to rising demand, food security, and climate change. Digital Twin is a concept that has the potential to increase production and efficiency while reducing the use of energy and other materials. The potential for digital twins to succeed in sustainable agriculture is enormous. Since the agriculture sector is dynamic and complicated, it needs an advanced management system. The necessity for automatic and self-reliant agriculture set up at the initial level is critical due to the frequent occurrence of natural disasters like floods and diseases. Due to problems with soil-based systems such as erosion, heavy manual labor, water availability, and productivity issues, soilless agriculture is becoming more and more popular. Digital techniques are expected to increase the optimization of processes and assist in agricultural decision-making.";Institute of Electrical and Electronics Engineers Inc.;Conference Proceeding;2nd International Conference on Sustainable Computing and Data Communication Systems, ICSCDS 2023 - Proceedings;2023-01-01;https://api.elsevier.com/content/abstract/scopus_id/85159082967
147;Towards Model-Driven Digital Twin Engineering: Current Opportunities and Future Challenges;Digital Twins have emerged since the beginning of this millennium to better support the management of systems based on (real-time) data collected in different parts of the operating systems. Digital Twins have been successfully used in many application domains, and thus, are considered as an important aspect of Model-Based Systems Engineering (MBSE). However, their development, maintenance, and evolution still face major challenges, in particular: (i) the management of heterogeneous models from different disciplines, (ii) the bi-directional synchronization of digital twins and the actual systems, and (iii) the support for collaborative development throughout the complete life-cycle. In the last decades, the Model-Driven Engineering (MDE) community has investigated these challenges in the context of software systems. Now the question arises, which results may be applicable for digital twin engineering as well. In this paper, we identify various MDE techniques and technologies which may contribute to tackle the three mentioned digital twin challenges as well as outline a set of open MDE research challenges that need to be addressed in order to move towards a digital twin engineering discipline.;Springer Science and Business Media Deutschland GmbHinfo@springer-sbm.com;Book Series;Communications in Computer and Information Science;2020-01-01;https://api.elsevier.com/content/abstract/scopus_id/85094117399
148;Using UML and OCL Models to Realize High-Level Digital Twins;Digital twins constitute virtual representations of physically existing systems. However, their inherent complexity makes them difficult to develop and prove correct. In this paper we explore the use of UML and OCL, complemented with an executable language, SOIL, to build and test digital twins at a high level of abstraction. We also show how to realize the bidirectional connection between the UML models of the digital twin in the USE tool with the physical twin, using an architectural framework centered on a data lake. We have built a prototype of the framework to demonstrate our ideas, and validated it by developing a digital twin of a Lego Mindstorms car. The results allow us to show some interesting advantages of using high-level UML models to specify virtual twins, such as simulation, property checking and some other types of tests.;Institute of Electrical and Electronics Engineers Inc.;Conference Proceeding;Companion Proceedings - 24th International Conference on Model-Driven Engineering Languages and Systems, MODELS-C 2021;2021-01-01;https://api.elsevier.com/content/abstract/scopus_id/85123990225
149;Automated Model Transformation in modeling Digital Twins of Industrial Internet-of-Things Applications utilizing AutomationML;In recent years, the manufacturing industry sector has undergone major changes. Better known by the term Industry 4.0, this trend describes the transformation of centrally managed production systems towards decentralized value creation networks. As this leads to a strong increase regarding complexity, engineering such current or future manufacturing systems becomes a difficult task. An example for such a challenging engineering process is the selection of technical implementations fulfilling all requirements and performing the needed functions. In order to support this process, model-based systems engineering (MBSE) and its expressions like model-driven architecture (MDA) proved to be promising approaches. However, semi or fully automated model transformations, which could deal with the aforementioned issue, are still yet widely unexplored in the industrial area. Therefore, this paper introduces an approach transforming the logical architecture of a system into its technical implementation by utilizing AutomationML. As the goal is to perform this step automatically, additional tool support is provided by developing specific software. The results of this approach are thereby evaluated by a real-world case study, which is applied according to the concepts of the Reference Architecture Model Industrie 4.0 (RAMI 4.0).;Institute of Electrical and Electronics Engineers Inc.;Conference Proceeding;IEEE International Conference on Emerging Technologies and Factory Automation, ETFA;2021-01-01;https://api.elsevier.com/content/abstract/scopus_id/85122922012
150;Information modeling for cyber-physical production system based on digital twin and AutomationML;Production systems play an important role in intelligent manufacturing. A large number of manufacturing resources are designed and developed with virtual (digital) ones, which will be associated with the physical ones throughout their lifecycle. With the recent emergence of information and communications technologies (ICTs), such as internet of things, big data, virtual reality, artificial intelligence, and 5G, the interconnection and interaction between physical resources and virtual ones become possible in production systems. Digital twin (DT) shows great potential to realize the cyber-physical production system (CPPS) in the era of Industry 4.0. In this paper, we present our vision on integrating various physical resources into CPPS via DT and AutomationML. To elaborate on how to apply ICTs, this paper firstly explores a generic architecture of CPPS based on DT. DT is a virtual and authoritative representation of physical manufacturing resource, since DT includes various models and manufacturing big data of resource. The proposed architecture is illustrated in detail as follows: (1) physical layer, (2) network layer, (3) virtual layer, and (4) application layer. A case of expert fault diagnose for aircraft engine is presented using the proposed information fusion in the architecture. Secondly, this paper proposes an approach of information modeling for CPPS based on AutomationML. Various manufacturing services can be encapsulated and defined in the standardized format (AutomationML), and then the corresponding virtual manufacturing resources (DTs) will be integrated into CPPS. Finally, this paper describes a case of information modeling for blisk machining and demonstrates the modeling approach in real-life scenarios for support manufacturing resource sharing via DT. Furthermore, the conclusion and further work is briefly summarized.;Springer;Journal;International Journal of Advanced Manufacturing Technology;2020-03-01;https://api.elsevier.com/content/abstract/scopus_id/85081887346
151;Model-based generation of run-time data collection systems exploiting AutomationML;Production system operators need support for collecting and pre-processing data on production systems consisting of several system components, as foundation for optimization and defect detection. Traditional approaches based on hard-coded programming of such run-time data collection systems take time and effort, and require both domain and technology knowledge. In this article, we introduce the AML-RTDC approach, which combines the strengths of AutomationML (AML) data modeling and model-driven engineering, to reduce the manual effort for realizing the run-time data collection (RTDC) system. We evaluate the feasibility of the AML-RTDC approach with a demonstration case about a lab-sized production system and a use case based on real-world requirements.;De Gruyter Oldenbourghbear@kams.or.kr;Journal;At-Automatisierungstechnik;2018-10-25;https://api.elsevier.com/content/abstract/scopus_id/85055572325
152;Digital Twin Data Modeling with AutomationML and a Communication Methodology for Data Exchange;In the context of the Cyber Physical Systems toward the realization of a Digital Twin system for future manufacturing and product service systems we propose the use of AutomationML to model attributes related to the Digital Twin. Also, we propose that this model is very useful for data exchange between different systems that are connected with the Digital Twin. We present a case study where a industrial component was modeled and simulated to prove that our methodology works.;Elsevier B.V.;Conference Proceeding;IFAC-PapersOnLine;2016-01-01;https://api.elsevier.com/content/abstract/scopus_id/85006391498
153;A Methodology for Digital Twin Modeling and Deployment for Industry 4.0;"The digital twin (DT) is a virtual representation of a physical object, which has been proposed as one of the key concepts for Industry 4.0. The DT provides a virtual representation of products along their lifecycle that enables the prediction and optimization of the behavior of a production system and its components. A methodology design using model-driven engineering (MDE) is proposed that strives toward being both flexible and generic. This approach is presented at two levels: first, a DT is modeled as a composition of basic components that provide basic functionalities, such as identification, storage, communication, security, data management, human-machine interface (HMI), and simulation; second, an aggregated DT is defined as a hierarchical composition of other DTs. A generic reference architecture based on these concepts and a concrete implementation methodology are proposed using AutomationML. This methodology follows an MDE approach that supports most of the DT features currently proposed in the literature. A case study has been developed, the proposed ideas are being evaluated with industrial case studies, and some of the preliminary results are described in this article. With the case study, it is possible to verify that the proposed methodology supports the creation and the deployment process of a DT.";Institute of Electrical and Electronics Engineers Inc.;Journal;Proceedings of the IEEE;2021-04-01;https://api.elsevier.com/content/abstract/scopus_id/85098780861
154;Taming the Complexity of Digital Twins;Current digital twin standards are based on architectures that have been shown to produce unnecessary accidental complexity. New standards are therefore needed to support the efficient implementation of digital twins and their seamless integration with other systems.;IEEE Computer Society;Journal;IEEE Software;2022-01-01;https://api.elsevier.com/content/abstract/scopus_id/85120072713
155;A machine learning digital twin approach for critical process parameter prediction in a catalyst manufacturing line;Digital twins (DTs) are rapidly changing how manufacturing companies leverage the large volumes of data they generate daily to gain a competitive advantage and optimize their supply chains. When coupled with recent developments in machine learning (ML), DTs have the potential to generate invaluable insights for process manufacturing companies to help them optimize their manufacturing processes. However, this potential has yet to be fully exploited due to the challenges that process manufacturing companies face in developing and implementing DTs in their organizations. Although DTs are receiving increasing attention in both industry and academia, there is limited literature on how to apply them in the process industry. To address this gap, this paper presents a framework for developing ML-based DTs to predict critical process parameters in real time. The proposed framework is tested through a case study at an international process manufacturing company in which it was used to collect and process plant data, build accurate predictive models for two critical process parameters, and develop a DT application to visualize the models’ predictions. The case study demonstrated the usefulness of the proposed DT–ML framework in the sense that it provided the company with more accurate predictions than the models it previously applied. The study provides insights into the value of applying ML-based DT in the process industry and sheds light on some of the challenges associated with the application of this technology.;Elsevier B.V.;Journal;Computers in Industry;2023-10-01;https://api.elsevier.com/content/abstract/scopus_id/85164238444
156;Enabling semantic interoperability of asset administration shells through an ontology-based modeling method;Digital twin technology establishes the future development vision for Industry 4.0, and is also an important exploration direction for the Model-Driven Engineering (MDE) paradigm. Because it builds a more flexible and communicative production system through models that spans life cycle, hierarchy and architecture. The standard proposed under the concept of Industry 4.0, the Asset Administration Shell (AAS), provides a syntactic interoperability interface for all assets involved in smart factories. However, there is still a need to fill the gap regarding semantic interoperability, in order to allow efficient interactions between Industry 4.0 components. Ontologies are a good candidate because they provide formal semantics expressed using a knowledge representation language, and in addition, there are many associated mature tools for reasoning and inference. Therefore, we propose a modeling approach that provides semantic interoperability for AAS-based digital twins using ontologies.;Association for Computing Machinery, Inc;Conference Proceeding;Proceedings - ACM/IEEE 25th International Conference on Model Driven Engineering Languages and Systems, MODELS 2022: Companion Proceedings;2022-10-23;https://api.elsevier.com/content/abstract/scopus_id/85142934810
157;KNOWLEDGE STRUCTURES OVER SIMULATION UNITS;Modern cyber-physical applications, such as those adopting the Digital Twin paradigm, typically connect simulators with data-rich components and domain knowledge, both often formalized as knowledge graphs. Engineering such applications poses challenges to developers. This paper presents a language-based integration of knowledge graphs and simulators for object-oriented languages. We use Functional Mock-Up Objects (FMOs) as a programming layer to encapsulate simulators compliant with the FMI standard into OO structures and integrate FMOs into the class and type systems. We show how FMOs can be integrated into knowledge graphs by means of semantical lifting, and used to ensure structural properties of cyber-physical applications. We provide a prototype implementation of the proposed integration and discuss how it can be realized in other languages. Finally, the use of FMOs in practice is illustrated by two case studies.;The Society for Modeling and Simulation International;Conference Proceeding;Simulation Series;2022-01-01;https://api.elsevier.com/content/abstract/scopus_id/85146968117
158;Digital Twin Reconfiguration Using Asset Models;Digital twins need to adapt to changes in the physical system they reflect. In this paper, we propose a solution to dynamically reconfigure simulators in a digital twin that exploits formalized asset models for this purpose. The proposed solution uses (1) semantic reflection in the programs orchestrating the simulators of the digital twin, and (2) semantic web technologies to formalize domain constraints and integrate asset models into the digital twin, as well as to validate semantically reflected digital twin configurations against these domain constraints on the fly. We provide an open-source proof-of-concept implementation of the proposed solution.;Springer Science and Business Media Deutschland GmbH;Book Series;Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics);2022-01-01;https://api.elsevier.com/content/abstract/scopus_id/85142758544
159;Programming and Debugging with Semantically Lifted States;We propose a novel integration of programming languages with semantic technologies. We create a semantic reflection mechanism by a direct mapping from program states to RDF knowledge graphs. This mechanism enables several promising novel applications including the use of semantic technology, including reasoning, for debugging and validating the sanity of program states, and integration with external knowledge graphs. Additionally, by making the knowledge graph accessible from the program, method implementations can refer to state semantics rather than objects, establishing a deep integration between programs and semantics. This allows the programmer to use domain knowledge formalized as, e.g., an ontology directly in the program’s control flow. We formalize this integration by defining a core object based programming language that incorporates these features. A prototypical interpreter is available for download.;Springer Science and Business Media Deutschland GmbH;Book Series;Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics);2021-01-01;https://api.elsevier.com/content/abstract/scopus_id/85111141938
160;A data-level fusion approach for degradation modeling and prognostic analysis under multiple failure modes;Operating units, in practice, often suffer from multiple modes of failure, and each failure mode has a distinct influence on the service life cycle path of a unit. The rapid development of sensor and communication technologies has enabled multiple sensors to simultaneously monitor and track the health status of a unit in real time. However, one challenging question that remains to be resolved is how to leverage data from multiple sensors for better degradation modeling and prognostic analysis, especially when there are multiple failure modes. Currently, many of the existing approaches in prognostics either (a) fail to capture the dependency between sensors and instead focus on analyzing each sensor independently or (b) fail to incorporate the failure-mode diagnosis for better degradation modeling and prognostics during condition monitoring. To address the limitations in the existing literature, we propose a data-level fusion methodology to construct a composite failure-mode index, named FM-INDEX, via the fusion of multiple sensor data. Our goal is to utilize the FM-INDEX to better characterize the failure mode of an operating unit in real time, thus leading to better degradation modeling and prognostic analysis. A case study that involves the degradation data set of an aircraft gas turbine engine with two potential failure modes is conducted to numerically evaluate the performance of our proposed method compared to other techniques in the related literature.;American Society for Qualityhelp@asq.org;Journal;Journal of Quality Technology;2018-01-01;https://api.elsevier.com/content/abstract/scopus_id/85055025482
161;A digital twin-driven approach for the assembly-commissioning of high precision products;"High precision products (HPPs) with multidisciplinary coupling are widely used in aerospace, marine, chemical and other fields. Since the internal structure of HPPs is complex and compact, the assembly process requires high precision and involves multidisciplinary coupling. Traditional assembly process of HPPs is based on manual experience, which results in low assembly efficiency and poor-quality consistency. Given the above problems, this research proposes a digital twin-driven assembly-commissioning approach for HPPs. Firstly, this paper introduces the theoretical framework of digital twin-driven assembly-commissioning. Secondly, we introduce the construction method of assembly-commissioning total factor information model based on digital twin technology; the fusion method of twin data and the interoperability method between digital twin models; in addition, the assembliability prediction and assembly-commissioning process optimization methods. Finally, a case study product is used to verify the effectiveness and feasibility of the proposed method.";Elsevier Ltd;Journal;Robotics and Computer-Integrated Manufacturing;2020-02-01;https://api.elsevier.com/content/abstract/scopus_id/85069044841
162;A Digital Twin approach based on nonparametric Bayesian network for complex system health monitoring;This paper proposes a Digital Twin approach for health monitoring. In this approach, a Digital Twin model based on nonparametric Bayesian network is constructed to denote the dynamic degradation process of health state and the propagation of epistemic uncertainty. Then, a real-time model updating strategy based on improved Gaussian particle filter (GPF) and Dirichlet process mixture model (DPMM) is presented to enhance the model adaptability. On one hand, for those parameters in the nonparametric Bayesian network with prior models, the improved GPF is used to update them in real time. On the other hand, for parameters lacking a prior model, DPMM is proposed to learn hidden variables, which adaptively update the model structure and greatly reduce uncertainty. Experiments on the electro-optical system are conducted to validate the feasibility of the Digital Twin approach and verify the effectiveness of the nonparametric Bayesian network. The results of comparative experiments prove that the Digital Twin approach based on nonparametric Bayesian Network has a good model self-learning ability, which improves the accuracy of health monitoring.;Elsevier B.V.;Journal;Journal of Manufacturing Systems;2021-01-01;https://api.elsevier.com/content/abstract/scopus_id/85089534296
163;Remaining useful life estimation - A review on the statistical data driven approaches;Remaining useful life (RUL) is the useful life left on an asset at a particular time of operation. Its estimation is central to condition based maintenance and prognostics and health management. RUL is typically random and unknown, and as such it must be estimated from available sources of information such as the information obtained in condition and health monitoring. The research on how to best estimate the RUL has gained popularity recently due to the rapid advances in condition and health monitoring techniques. However, due to its complicated relationship with observable health information, there is no such best approach which can be used universally to achieve the best estimate. As such this paper reviews the recent modeling developments for estimating the RUL. The review is centred on statistical data driven approaches which rely only on available past observed data and statistical models. The approaches are classified into two broad types of models, that is, models that rely on directly observed state information of the asset, and those do not. We systematically review the models and approaches reported in the literature and finally highlight future research challenges. © 2011 Elsevier B.V. All rights reserved.;Elsevier B.V.;Journal;European Journal of Operational Research;2011-08-16;https://api.elsevier.com/content/abstract/scopus_id/79955584653
164;Design and Implementation of Smart Manufacturing Systems Through AR for Data-Driven Digital Twin System;Modification of size, residual stress, and surface roughness have an enormous impact on a complex mechanical product’s final machining quality. Machine quality can be ensured using Digital Twin (DT) technology by checking the real-time machining process. The virtual–real separation display method is the most modern DT System (DTS). It results in the ineffective transmission of the necessary restricting the use of the DTS by processing data on-site technicians to support field processing. Augmented Reality (AR) monitoring the manufacturing process approach to solve this problem is proposed based on the DT. First, the dynamic multi-view for AR is built using data from multiple sources. Second, real-time monitoring of complex product’s intermediate processes incorporates AR to encourage communication between the users of the DT machining system. The outcome of the system can prevent errors that cannot be fixed. An application case for observing will be used to confirm the viability and the efficacy of the proposed method.;Springer;Journal;SN Computer Science;2023-09-01;https://api.elsevier.com/content/abstract/scopus_id/85166225404
165;Digital twin-driven product design, manufacturing and service with big data;Nowadays, along with the application of new-generation information technologies in industry and manufacturing, the big data-driven manufacturing era is coming. However, although various big data in the entire product lifecycle, including product design, manufacturing, and service, can be obtained, it can be found that the current research on product lifecycle data mainly focuses on physical products rather than virtual models. Besides, due to the lack of convergence between product physical and virtual space, the data in product lifecycle is isolated, fragmented, and stagnant, which is useless for manufacturing enterprises. These problems lead to low level of efficiency, intelligence, sustainability in product design, manufacturing, and service phases. However, physical product data, virtual product data, and connected data that tie physical and virtual product are needed to support product design, manufacturing, and service. Therefore, how to generate and use converged cyber-physical data to better serve product lifecycle, so as to drive product design, manufacturing, and service to be more efficient, smart, and sustainable, is emphasized and investigated based on our previous study on big data in product lifecycle management. In this paper, a new method for product design, manufacturing, and service driven by digital twin is proposed. The detailed application methods and frameworks of digital twin-driven product design, manufacturing, and service are investigated. Furthermore, three cases are given to illustrate the future applications of digital twin in the three phases of a product respectively.;Springer London;Journal;International Journal of Advanced Manufacturing Technology;2018-02-01;https://api.elsevier.com/content/abstract/scopus_id/85015707925
166;A hybrid framework combining data-driven and model-based methods for system remaining useful life prediction;Remaining useful life prediction is one of the key requirements in prognostics and health management. While a system or component exhibits degradation during its life cycle, there are various methods to predict its future performance and assess the time frame until it does no longer perform its desired functionality. The proposed data-driven and model-based hybrid/fusion prognostics framework interfaces a classical Bayesian model-based prognostics approach, namely particle filter, with two data-driven methods in purpose of improving the prediction accuracy. The first data-driven method establishes the measurement model (inferring the measurements from the internal system state) to account for situations where the internal system state is not accessible through direct measurements. The second data-driven method extrapolates the measurements beyond the range of actually available measurements to feed them back to the model-based method which further updates the particles and their weights during the long-term prediction phase. By leveraging the strengths of the data-driven and model-based methods, the proposed fusion prognostics framework can bridge the gap between data-driven prognostics and model-based prognostics when both abundant historical data and knowledge of the physical degradation process are available. The proposed framework was successfully applied on lithium-ion battery remaining useful life prediction and achieved a significantly better accuracy compared to the classical particle filter approach.;Elsevier Ltd;Journal;Applied Soft Computing Journal;2016-07-01;https://api.elsevier.com/content/abstract/scopus_id/84964614154
167;A hybrid predictive maintenance approach for CNC machine tool driven by Digital Twin;As a typical manufacturing equipment, CNC machine tool (CNCMT) is the mother machine of industry. Fault of CNCMT might cause the loss of precision and affect the production if troubleshooting is not timely. Therefore, the reliability of CNCMT has a big significance. Predictive maintenance is an effective method to avoid faults and casualties. Due to less consideration of the status variety and consistency of CNCMT in its life cycle, current methods cannot achieve accurate, timely and intelligent results. To realize reliable predictive maintenance of CNCMT, a hybrid approach driven by Digital Twin (DT) is studied. This approach is DT model-based and DT data-driven hybrid. With the proposed framework, a hybrid predictive maintenance algorithm based on DT model and DT data is researched. At last, a case study on cutting tool life prediction is conducted. The result shows that the proposed method is feasible and more accurate than single approach.;Elsevier Ltd;Journal;Robotics and Computer-Integrated Manufacturing;2020-10-01;https://api.elsevier.com/content/abstract/scopus_id/85082683374
168;Connotation, architecture and trends of product digital twin;Digital twin technology is considered as the core and crucial technology to fulfill cyber-physical systems. Product digital twin is one of applications of digital twin technology in the process of product development. However, the research on it is still in its infancy and there are only few unsystematic research findings presented. The concept of digital twin technology based on review of digital twin background and the architecture for product digital twin based on systematic analysis of its connotation were proposed. The implementation approach of product digital twin in the stage of product design, manufacturing and service was investigated. The evolving trends of product digital twin concerning its features in the future were predicted.;CIMS;Journal;Jisuanji Jicheng Zhizao Xitong/Computer Integrated Manufacturing Systems, CIMS;2017-04-01;https://api.elsevier.com/content/abstract/scopus_id/85021844855
169;Shaping the digital twin for design and production engineering;The digitalization of manufacturing fuels the application of sophisticated virtual product models, which are referred to as digital twins, throughout all stages of product realization. Particularly, more realistic virtual models of manufactured products are essential to bridge the gap between design and manufacturing and to mirror the real and virtual worlds. In this paper, we propose a comprehensive reference model based on the concept of Skin Model Shapes, which serves as a digital twin of the physical product in design and manufacturing. In this regard, model conceptualization, representation, and implementation as well as applications along the product life-cycle are addressed.;Elsevier USA;Journal;CIRP Annals - Manufacturing Technology;2017-01-01;https://api.elsevier.com/content/abstract/scopus_id/85018723536
170;Digital twin workshop: a new paradigm for future workshop;With the integration and extensive applications for new generation of information technologies (such as cloud computing, internet of things, big data, mobile internet, artificial intelligence) in manufacturing industry, a number of countries had put forward their national advanced manufacturing development strategies, such as Industry 4.0 in Germany, Industrial Internet and manufacturing system based on Cyber-Physical Systems (CPS) in USA, as well as Made in China 2025 and Internet Plus Manufacturing in China. Although each of these strategies was proposed under different circumstances, one of the common purposes of these strategies was to achieve the interconnection, interoperability between physical world and the information world of manufacturing and the intelligent operation of manufacturing. As one of bottlenecks to achieve this purpose, the communication and interaction between the physical world and the information world of manufacturing must be solved. To solve the problem, a novel concept of Digital Twin Workshop (DTW) based on digital twin was proposed. Complementary to the concept, the characteristics, architecture, system composition, operating mechanism and enabling key technologies were also elaborated and discussed respectively. On this basis, the theory and implementation of the communication and interaction between physical and information world of workshop based on workshop digital twin data were discussed.;CIMS;Journal;Jisuanji Jicheng Zhizao Xitong/Computer Integrated Manufacturing Systems, CIMS;2017-01-01;https://api.elsevier.com/content/abstract/scopus_id/85016456422
171;A generic tri-model-based approach for product-level digital twin development in a smart manufacturing environment;Smart manufacturing, as an emerging manufacturing paradigm, leverages massive in-context data from manufacturing systems for intelligent decision makings. In such context, Cyber-Physical Systems (CPS) play a key role in digitizing manufacturing systems and integrating multiple systems together for collaborative works. Amongst different levels of smartness and connectedness of CPS, Digital Twin (DT), as an exact digital copy of a physical object or system including its properties and relationship with the environment, has a significant impact on realizing smart manufacturing. A DT constantly synchronizes with its physical system and provides real-time high-fidelity simulations of the system and offers ubiquitous control over the system. Despite its great advantages, few works have been discussed about DT reference models, let alone a generic manner to establish it for smart manufacturing. Aiming to fill the gap, this research introduces a generic CPS system architecture for DT establishment in smart manufacturing with a novel tri-model-based approach (i.e. digital model, computational model and graph-based model) for product-level DT development. The tri-model works concurrently to simulate real-world physical behaviour and characteristics of the digital model. To validate the proposed architecture and approach, a case study of an open source 3D printer DT establishment is further conducted. Conclusions and future works are also highlighted to provide insightful knowledge to both academia and industries at last.;Elsevier Ltd;Journal;Robotics and Computer-Integrated Manufacturing;2020-08-01;https://api.elsevier.com/content/abstract/scopus_id/85079352073
172;In-process tool condition forecasting based on a deep learning method;It is widely acknowledged that machining precision and surface integrity are greatly affected by cutting tool conditions. In order to enable early cutting tool replacement and proactive actions, tool wear conditions should be estimated in advance and updated in real-time. In this work, an approach to in-process tool condition forecasting is proposed based on a deep learning method. A long short-term memory network is designed to forecast multiple flank wear values based on historical data. A residual convolutional neural network is built to enable in-process tool condition monitoring, using raw signals acquired during the machining process. The integration of them enables in-process tool condition forecasting. Median-based correction and mean-based correction are adopted to improve the accuracy. IEEE PHM 2010 challenge data has been used to illustrate and validate this approach. Experimental study and quantitative comparisons showed that future flank wear values could be precisely forecasted during the machining process. The proposed approach contributes to prompt and reliable cutting tool condition forecasting, which will support the decision-making about cutting tool replacement in data-driven smart manufacturing.;Elsevier Ltd;Journal;Robotics and Computer-Integrated Manufacturing;2020-08-01;https://api.elsevier.com/content/abstract/scopus_id/85078671065
173;Digital Twin-driven smart manufacturing: Connotation, reference model, applications and research issues;This paper reviews the recent development of Digital Twin technologies in manufacturing systems and processes, to analyze the connotation, application scenarios, and research issues of Digital Twin-driven smart manufacturing in the context of Industry 4.0. To understand Digital Twin and its future potential in manufacturing, we summarized the definition and state-of-the-art development outcomes of Digital Twin. Existing technologies for developing a Digital Twin for smart manufacturing are reviewed under a Digital Twin reference model to systematize the development methodology for Digital Twin. Representative applications are reviewed with a focus on the alignment with the proposed reference model. Outstanding research issues of developing Digital Twins for smart manufacturing are identified at the end of the paper.;Elsevier Ltd;Journal;Robotics and Computer-Integrated Manufacturing;2020-02-01;https://api.elsevier.com/content/abstract/scopus_id/85070213247
174;A Reliability Allocation Method for Mechanical Product Based on Meta-Action;"For a long time, the machinery's reliability allocation methods have followed the electronics', and thus all of the interactions among moving components and parts in machines are neglected. In fact, the allocated results are not reasonable for mechanical products; even the allocation granularity is not appropriate to guide the reliability design. To overcome these problems, this paper proposes an allocation method based on meta-action. By using the function-movement-action method, we decompose the mechanical product into meta-actions. An action is used as the basic unit of reliability control, and all the parts contributing to the action are treated as a whole. The interactions of the parts which contribute to the same action are treated as the internal forces of meta-actions, and others are considered as the interactions between meta-actions by the consideration of common parts in different units. An illustrative example shows the proposed method makes the results more reasonable and more convenient to guide mechanical product designs.";Institute of Electrical and Electronics Engineers Inc.;Journal;IEEE Transactions on Reliability;2020-03-01;https://api.elsevier.com/content/abstract/scopus_id/85068263566
175;Employing Knowledge on Causal Relationship to Assist Multidisciplinary Design Optimization;With the increasing design dimensionality, it is more difficult to solve multidisciplinary design optimization (MDO) problems. Many MDO decomposition strategies have been developed to reduce the dimensionality. Those strategies consider the design problem as a black-box function. However, practitioners usually have certain knowledge of their problem. In this paper, a method leveraging causal graph and qualitative analysis is developed to reduce the dimensionality of the MDO problem by systematically modeling and incorporating the knowledge about the design problem into optimization. Causal graph is created to show the input-output relationships between variables. A qualitative analysis algorithm using design structure matrix (DSM) is developed to automatically find the variables whose values can be determined without resorting to optimization. According to the impact of variables, an MDO problem is divided into two subproblems, the optimization problem with respect to the most important variables, and the other with variables of lower importance. The novel method is used to solve a power converter design problem and an aircraft concept design problem, and the results show that by incorporating knowledge in form of causal relationship, the optimization efficiency is significantly improved.;American Society of Mechanical Engineers (ASME)infocentral@asme.org;Journal;Journal of Mechanical Design, Transactions of the ASME;2019-04-01;https://api.elsevier.com/content/abstract/scopus_id/85059942742
176;Digital twin driven prognostics and health management for complex equipment;Prognostics and health management (PHM) is crucial in the lifecycle monitoring of a product, especially for complex equipment working in a harsh environment. In order to improve the accuracy and efficiency of PHM, digital twin (DT), an emerging technology to achieve physical–virtual convergence, is proposed for complex equipment. A general DT for complex equipment is first constructed, then a new method using DT driven PHM is proposed, making effective use of the interaction mechanism and fused data of DT. A case study of a wind turbine is used to illustrate the effectiveness of the proposed method.;Elsevier USA;Journal;CIRP Annals;2018-01-01;https://api.elsevier.com/content/abstract/scopus_id/85047291024
177;An event-condition-action logic programming language;Event-Condition-Action (ECA) languages are an intuitive and powerful paradigm for programming reactive systems. Usually, important features for an EGA language are reactive and reasoning capabilities, the possibility to express complex actions and events, and a declarative semantics. In this paper, we introduce ERA, an ECA language based on, and extending the framework of logic programs updates that, together with these features, also exhibits capabilities to integrate external updates and perform self updates to its knowledge (data and classical rules) and behaviour (reactive rules). © Springer-Verlag Berlin Heidelberg 2006.;Springer Verlag;Book Series;Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics);2006-01-01;https://api.elsevier.com/content/abstract/scopus_id/33750058269
178;A Survey on Development and Applications of the Multi-criteria Decision Making Method MULTIMOORA;This paper focuses on the MULTIMOORA method. Specifically, it discusses its development as well as extensions alongside with an overview of their applications. Indeed, the MULTIMOORA is a relatively new multi-criteria decision making method consisting of the three parts. Therefore, it is specific with peculiarities of both complete and partial aggregation techniques. The MULTIMOORA method consists of the three parts, namely the Ratio System, the Reference Point, and the Full Multiplicative Form. The paper discussed the extensions of MULTIMOORA into the fuzzy environment and group decision making. The carried out survey identified the two major areas of application of MOORA and MULTIMOORA, namely engineering decision support for technological development and economic researches. The economic researches can further be classified into those related to micro-level or macro-level analyses. © 2013 John Wiley & Sons, Ltd.;John Wiley and Sons LtdSouthern GateChichester, West SussexPO19 8SQ;Journal;Journal of Multi-Criteria Decision Analysis;2014-01-01;https://api.elsevier.com/content/abstract/scopus_id/84904904655
179;Actor based simulation for closed loop control of supply chain using reinforcement learning;Reinforcement Learning (RL) has achieved a degree of success in control applications such as online gameplay and robotics, but has rarely been used to manage operations of business-critical systems such as supply chains. A key aspect of using RL in the real world is to train the agent before deployment, so as to minimise experimentation in live operation. While this is feasible for online gameplay (where the rules of the game are known) and robotics (where the dynamics are predictable), it is much more difficult for complex systems due to associated complexities, such as uncertainty, adaptability and emergent behaviour. In this paper, we describe a framework for effective integration of a reinforcement learning controller with an actor-based simulation of the complex networked system, in order to enable deployment of the RL agent in the real system with minimal further tuning.;International Foundation for Autonomous Agents and Multiagent Systems (IFAAMAS)info@ifaamas.org;Conference Proceeding;Proceedings of the International Joint Conference on Autonomous Agents and Multiagent Systems, AAMAS;2019-01-01;https://api.elsevier.com/content/abstract/scopus_id/85074360666
180;Ambidexterity and organizational learning: revisiting and reconnecting the literatures;Purpose: The purpose of the study is to investigate how the processes of exploration and exploitation have developed in parallel in the literature of organizational ambidexterity and organizational learning, since James March published his seminal paper in 1991. The goal of the paper is to provide a synthesis of exploration and exploitation based on the two areas of literature. Design/methodology/approach: The study is conceptual and no empirical data have been used. Findings: The study advances current understanding of exploration and exploitation by building a new model for organizational ambidexterity that takes into account multiple levels of learning, perspectives from absorptive capacity and inter-organizational learning. Originality/value: The study’s novelty lies in the creation and discussion of a synthesis of exploration and exploitation stemming from organizational ambidexterity and organizational learning.;Emerald Group Holdings Ltd.;Journal;Learning Organization;2019-08-12;https://api.elsevier.com/content/abstract/scopus_id/85066841858
181;Combining DEVS with multi-agent concepts to design and simulate multi-models of complex systems (WIP);We are interested in the multi-modeling and simulation of complex systems, that is representing a complex system as a set of interacting models and simulating it with a cosimulation approach. Representing and simulating a complex system multi-model requires to integrate heterogeneity at several levels (representations, formalisms, simulation software, models' interactions. . . ). In this article, we present our approach that consists of combining the Discrete EVent System Specification (DEVS) formalism and multi-agent concepts in order to achieve these requirements. The use of the DEVS formalism enables a rigourous integration of models described with heterogeneous formalisms and a rigourous simulation protocol. Multi-agent concepts ease the description of multi-perspective integration and the reuse of existing heterogeneous simulators. We detail the combination of both in the Agent & Artifact for Multi-Modeling (AA4MM) approach and illustrate its use in a proof of concept.;The Society for Modeling and Simulation Internationalwww.scs.org;Conference Proceeding;Simulation Series;2015-01-01;https://api.elsevier.com/content/abstract/scopus_id/84928136665
182;Querying histories of organisation simulations;Industrial Dynamics involves system modelling, simulation and evaluation leading to policy making. Traditional approaches to industrial dynamics use expert knowledge to build top-down models that have been criticised as not taking into account the adaptability and sociotechnical features of modern organisations. Furthermore, such models require a-priori knowledge of policy-making theorems. This paper advances recent research on bottom-up agent-based organisational modelling for Industrial Dynamics by presenting a framework where simulations produce histories that can be used to establish a range of policy-based theorems. The framework is presented and evaluated using a case study that has been implemented using a toolset called ESL.;Association for Information Systems;Conference Proceeding;Information Systems Development: Advances in Methods, Tools and Management - Proceedings of the 26th International Conference on Information Systems Development, ISD 2017;2017-01-01;https://api.elsevier.com/content/abstract/scopus_id/85081127542
183;Why managers still matter as applied organization (design) theory;Core organization design issues have emerged in recent popular and influential discussions of managers and organizations, specifically in a genre of writing—the “bossless company narrative”—that declares that the classic managerial hierarchy is dead. In this article, we review our critical discussion of this genre in our book, Why Managers Still Matter, arguing that the narrative manifests bad empiricism and half-baked organization theory. However, we also raise the possibility of a charitable reading of the genre: it points to themes in organization design theory that are currently underdeveloped, notably with respect to, for example, the impact of organizational structure and control on employee motivations and the importance of contingencies such as the characteristics of knowledge for organization design.;Springer Science and Business Media B.V.;Journal;Journal of Organization Design;2023-06-01;https://api.elsevier.com/content/abstract/scopus_id/85143687793
184;Dimensions of digital twin applications - A literature review;The use of Digital Twins has gained attraction in research and practice in recent years. Digital Twins are virtual representations of physical objects and they can be connected with their physical counterparts. Through this connection, Digital Twins contribute to the convergence of the real and the virtual world. While existing literature reviews focus strongly on the manufacturing industry, this paper analyzes Digital Twin applications across industries. Based on a systematic literature review, this paper examines 87 Digital Twin applications and proposes a classification scheme with six dimensions to describe the applications identified. The concept of Digital Twins is currently still underrepresented in Information Systems research, which opens up further research opportunities.;Association for Information Systemspublications@aisnet.org;Conference Proceeding;25th Americas Conference on Information Systems, AMCIS 2019;2019-01-01;https://api.elsevier.com/content/abstract/scopus_id/85084019743
185;Digital twin: Mitigating unpredictable, undesirable emergent behavior in complex systems;Systems do not simply pop into existence. They progress through lifecycle phases of creation, production, operations, and disposal. The issues leading to undesirable and unpredicted emergent behavior are set in place during the phases of creation and production and realized during the operational phase, with many of those problematic issues due to human interaction. We propose that the idea of the Digital Twin, which links the physical system with its virtual equivalent can mitigate these problematic issues. We describe the Digital Twin concept and its development, show how it applies across the product lifecycle in defining and understanding system behavior, and define tests to evaluate how we are progressing. We discuss how the Digital Twin relates to Systems Engineering and how it can address the human interactions that lead to “normal accidents.” We address both Digital Twin obstacles and opportunities, such as system replication and front running. We finish with NASA's current work with the Digital Twin.;Springer International Publishing;Book;Transdisciplinary Perspectives on Complex Systems: New Findings and Approaches;2016-01-01;https://api.elsevier.com/content/abstract/scopus_id/85006339863
186;A/B Testing Intuition Busters: Common Misunderstandings in Online Controlled Experiments;"A/B tests, or online controlled experiments, are heavily used in industry to evaluate implementations of ideas. While the statistics behind controlled experiments are well documented and some basic pitfalls known, we have observed some seemingly intuitive concepts being touted, including by A/B tool vendors and agencies, which are misleading, often badly so. Our goal is to describe these misunderstandings, the ""intuition""behind them, and to explain and bust that intuition with solid statistical reasoning. We provide recommendations that experimentation platform designers can implement to make it harder for experimenters to make these intuitive mistakes.";Association for Computing Machinery;Conference Proceeding;Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining;2022-08-14;https://api.elsevier.com/content/abstract/scopus_id/85137142752
187;Decision Making under Deep Uncertainty: From Theory to Practice;This open access book focuses on both the theory and practice associated with the tools and approaches for decisionmaking in the face of deep uncertainty. It explores approaches and tools supporting the design of strategic plans under deep uncertainty, and their testing in the real world, including barriers and enablers for their use in practice. The book broadens traditional approaches and tools to include the analysis of actors and networks related to the problem at hand. It also shows how lessons learned in the application process can be used to improve the approaches and tools used in the design process. The book offers guidance in identifying and applying appropriate approaches and tools to design plans, as well as advice on implementing these plans in the real world. For decisionmakers and practitioners, the book includes realistic examples and practical guidelines that should help them understand what decisionmaking under deep uncertainty is and how it may be of assistance to them. Decision Making under Deep Uncertainty: From Theory to Practice is divided into four parts. Part I presents five approaches for designing strategic plans under deep uncertainty: Robust Decision Making, Dynamic Adaptive Planning, Dynamic Adaptive Policy Pathways, Info-Gap Decision Theory, and Engineering Options Analysis. Each approach is worked out in terms of its theoretical foundations, methodological steps to follow when using the approach, latest methodological insights, and challenges for improvement. In Part II, applications of each of these approaches are presented. Based on recent case studies, the practical implications of applying each approach are discussed in depth. Part III focuses on using the approaches and tools in real-world contexts, based on insights from real-world cases. Part IV contains conclusions and a synthesis of the lessons that can be drawn for designing, applying, and implementing strategic plans under deep uncertainty, as well as recommendations for future work. The publication of this book has been funded by the Radboud University, the RAND Corporation, Delft University of Technology, and Deltares.;Springer International Publishing;Book;Decision Making under Deep Uncertainty: From Theory to Practice;2019-01-01;https://api.elsevier.com/content/abstract/scopus_id/85133596013
188;A Continuous Process for Validation, Verification, and Accreditation of Simulation Models;A simulation model, and more generically, a model, is founded on its assumptions. Assurance of the model’s correctness and correct use is needed to achieve accreditation. Often the exercise of working with a specific code misunderstands the overall process, focusing the resources on the model coding and forgetting the needed resources to ensure the validation of every step of the model definition and coding. The goal of this work is to present a methodology to help in the definition and use of the assumptions in the modeling process. To do so, we present a process to conduct a simulation project, an assumptions taxonomy, and a method that simplifies working with those assumptions. We propose to extend the traditional Validation, Verification, and Accreditation processes to a process composed of eight Validation, Verification, and Accreditation phases that cover the overall life cycle of a model. Although this paper is focused on a simulation model, we can extend the proposed method to a more general modeling approach.;MDPI;Journal;Mathematics;2023-02-01;https://api.elsevier.com/content/abstract/scopus_id/85148955919
189;Dynamic capabilities and organizational agility: Risk, uncertainty, and strategy in the innovation economy;"""Organizational agility"" is often treated as an immutable quality, implying that firms need to be in a constant state of transformation. However, this ignores that such transformations, while often essential, come at a cost. They are not always necessary, and may not even be possible. This article explores agility at a more fundamental level and relates it more specifically to dynamic capabilities. It demonstrates that it is first essential to understand deep uncertainty, which is ubiquitous in the innovation economy. Uncertainty is very different from risk, which can be managed using traditional tools and approaches. Strong dynamic capabilities are necessary for fostering the organizational agility necessary to address deep uncertainty, such as that generated by innovation and the associated dynamic competition. This article explores the mechanisms by which managers may calibrate the required level of organizational agility, deliver it cost effectively, and relate it to strategy.";University of California Press;Journal;California Management Review;2016-06-01;https://api.elsevier.com/content/abstract/scopus_id/84989291196
190;TOPSISort-L: An extended likelihood-based interval-valued intuitionistic fuzzy TOPSIS-sort method and its application to multi-criteria group decision-making;In real-world multi-criteria group decision making challenges, decision makers typically provide imprecise or ambiguous information due to a lack of knowledge, time constraints, or restrictions regarding information disclosure. In order to deal with this sort of imprecision caused by experts' subjective evaluation, fuzzy sets have been suggested. When compared to traditional fuzzy sets, interval-valued intuitionistic fuzzy sets, also known as IVIFSs, are superior when it comes to dealing with subjective ambiguity and incomplete information. For these reasons, this work provides a novel extension for the TOPSIS technique by employing likelihoods of IVIFSs, and introduces a methodology named as TOPSISort-L that is capable of classifying alternatives under a variety of circumstances. We begin by developing the conventional fuzzy TOPSIS technique by using a newly proposed decision matrix, a novel selection mechanism for ideal solutions, and a generalized likelihood-based closeness metric for the purpose of alternative ranking. After that, the TOPSISort-L algorithms are presented to obtain an accurate classification for the alternatives when there are information about the characteristic profiles, and also obtain an approximate classification when information about the characteristic profiles is missing. Eventually, by contrasting the approach with various different methodologies now in use, we exhibit the validity and adaptability of the method.;Elsevier Ltd;Journal;Expert Systems with Applications;2023-12-15;https://api.elsevier.com/content/abstract/scopus_id/85165534442
191;A simulation approach in process mining conformance analysis. the introduction of a brand new BPMN element;"The computerization of organizational processes provides several tools for evaluating the quality of mapping. An executed process produces, in each instance, log files that can be used to reconstruct the actual procedure carried out by the system as well as to highlight deviations or ""move"" from the mapped path. These ""moves"" represent a loss that we consider unacceptable for organizations, which manifests its effects in various modes and weights. From this arises, in our view, the need for tools and logic approach to manage and limit the ""conformance risk"", including as well as a proper consideration and methodological evaluation of the same risk, but also practical solutions and mapping tools that might influence the occurrence. In this paper we propose a methodology and a simulation model of ""Conformance Risk Aware Desing"" in order to support the modeler in moving from a diagnostic to a preventive and design view of the conformance's matter. Purpose In this paper we have analyzed the Process Mining theme and give a new interpretation of the ""conformance analysis"" treating this issue not as an inevitable diagnostic phenomenon but as a real and necessary design parameter. This turning point is realized increasing the process mapping's quality through two main purposes. Our first purpose is the introduction of a brand new modeling elements in the world famous BPMN Modelling Notation with the implicit and objective meaning of ""conformance controller"" that indicates the need for the agent to communicate about the execution of the preceding task. The second part of the purpose pass through the systematic inclusion of a ""conformance controller"" pattern after each task whose deviation's risk assessment exceeds a certain threshold value. This methodological approach led to a stronger process design as for the task considered critical by the organization and to a lower need of process model repairing efforts and costs once that the process is released and log files are available. Design/methodology/approach In order to demonstrate the usefulness and value of our proposal we got served of a simulation based on the System Dynamic logic. We've modeled a human resource's behavior facing a number of task to perform under the influence and the documentation of a process model including both the ""conformance controller"" and the controller pattern. We used a combination of software including Matlab and Powersim Studio in order to getting results about a numeric estimation of the strength of the mapped task, basing our argument on the fuzzy logic rules and techniques, as for Matlab, and simulating hundred runs of a generic process in different operational scenarios, as for Powersim Studio. Collecting all the simulated data, we produced a series of statistical considerations about the effectiveness of the proposal and about the scenarios that increased its economical efficiency. Originality/value We open the discussion concerning the need by the organizations, increasingly constrained by regulatory guidelines and outside interests, of managing and preventing the risk of deviation from the process model especially regarding the critical and most added valued tasks, proposing a specific methodology. We also propose the integration of the BPMN Modelling Notation with a new implicit element and pattern to be used for the aim above. Another important aspect of the proposal is the building of a provisional model able to predict the impact of deviation from the mapped process using the defuzzyfication and implementable by every kind of organization that owns datasets about its processes.";AIDI - Italian Association of Industrial Operations Professors;Conference Proceeding;Proceedings of the Summer School Francesco Turco;2013-01-01;https://api.elsevier.com/content/abstract/scopus_id/84982976050
192;A problem-solving ontology for human-centered cyber physical production systems;Cyber physical social systems (CPSS) tend to integrate computation with physical processes as well as human and social characteristics. The fusion of cyber, physical, and socio spaces through Industry 4.0 emerges a new type of production systems known as cyber physical production systems (CPPS). CPPS enriches communications among cyber-physical-socio space in the production environment. Utilizing human-centered CPPS in smart factories (ideally) results in a mutual transition from human-machine cooperation to active collaboration, which is characterized by cyber-physical-socio interactions, knowledge exchange and reciprocal learning. The shift from data workers or producers to problem-solver is, therefore, triggered to both humans and CPPS, respectively. Hence, their job roles and responsibilities cannot be independently defined. This paper approaches the collaboration of human and CPPS in problem-solving from the angle of complementarity whereby “human competences” and “CPPS autonomy” together derive supplementary capability and reciprocal learning. In this research, “Problem” is an umbrella term that refers to both categories of “human-CPPS task” (i.e. a specific piece of work required to be done) and “failure event” (i.e. a state of difficulty that needs to be resolved). A holistic ontological framework is proposed, entitled PSP Ontology (Problem, Solution, Problem-Solver Ontology), which represents the logical relations between the three super-concepts of “Problem Profile” “Problem-Solver Profile” and “Solution Profile”. Related entities are formalized by introducing (i) contingency vector, (ii) vector of competence and autonomy, and (iii) solution maturity index, respectively. PSP Ontology is utilized for semantic representation of the super-concepts and reasoning out the competence questions, i.e. in which situation and under which conditions human and/or CPPS is dominant or eligible to solve a problem (to accomplish a given task and/or to detect or eliminate a failure), which is qualitatively exemplified in the use-case of maintenance 4.0.;Elsevier Ltd;Journal;CIRP Journal of Manufacturing Science and Technology;2018-08-01;https://api.elsevier.com/content/abstract/scopus_id/85050807312
193;Cyber-Physical Systems and Human Action: A Re-Definition of Distributed Agency Between Humans ND Technology, Using the Example of Explicit and Implicit Knowledge;"This chapter focuses on the challenge of understanding the social embeddedness of cyber-physical systems (CPSs). It distinguishes different forms of relations between CPS and the social environment in which they operate by pointing out that both the communication in social relationships and the communication between humans and technology can assume different forms. These different forms of communication are based on explicit or on implicit (tacit) knowledge. From this distinction, extensions as well as limits of human-technology communication can be derived. An extended concept of distributed agency is suggested, with a readjusted division of labor between humans and technology along the lines of the distinction between explicit and implicit knowledge. This aims at a ""reloaded"" concept of distributed agency to provide an orientation guide for future development.";Elsevier Inc.;Book;Cyber-Physical Systems: Foundations, Principles and Applications;2017-01-01;https://api.elsevier.com/content/abstract/scopus_id/85024135138
194;Joint Workshop on Model-Driven Engineering for Software Architecture (MDE4SA) and International Workshop on Automotive System/Software Architectures (WASA);Current society heavily relies on software and software systems. Due to its increasing complexity, the design and operation of software systems are becoming challenging. In the last decades, a great deal of effort has been put into addressing software systems design, development, and maintenance challenges. Empirical evidence shows that one of the most critical success factors when developing software systems is their Software Architecture (SA). A SA describes software systems in terms of software components, their interactions, and critical quality attributes. Among other benefits, SAs improve the overall communication among different stakeholders, are the carriers of significant design decisions, promote the use of different abstraction levels, and allow for the early assessment of the software under development.;Institute of Electrical and Electronics Engineers Inc.;Conference Proceeding;Proceedings - IEEE 20th International Conference on Software Architecture Companion, ICSA-C 2023;2023-01-01;https://api.elsevier.com/content/abstract/scopus_id/85159042133
195;Empowering humans in a cyber-physical production system: Human-in-the-loop perspective;In the Industry 4.0 context, humans are taking up new roles in Cyber-Physical Systems (CPS) since it was realized that they constitute the most flexible part of an automated system. With this new reality, humans are becoming integral parts of CPS, entering in the realm of the Human-in-the-Loop of Cyber-Physical Systems (HiLCPS). In this paper, the challenges and technologies for the integration of humans in CPS environments are briefly discussed, and a practical application of the HiLCPS concept is presented. For this purpose, a human operation workbench is empowered with intelligent decision and assistant systems that use, among others, reasoning algorithms, image processing and speech recognition, to improve the productivity and quality of the tasks performed by humans, particularly focusing the assembly of highly customized products, as well as the training of operators to perform complex assembly, repairing or maintenance tasks.;Institute of Electrical and Electronics Engineers Inc.;Conference Proceeding;Proceedings - 2019 IEEE International Conference on Industrial Cyber Physical Systems, ICPS 2019;2019-05-01;https://api.elsevier.com/content/abstract/scopus_id/85070875894
196;Role of humans in complexity of a system-of-systems;This paper pursues three primary objectives. First, a brief introduction to system-of-systems is presented in order to establish a foundation for exploration of the role of human system modeling in this context. Second, the sources of complexity related to human participation in a system-of-systems are described and categorized. Finally, special attention is placed upon how this complexity might be better managed by greater involvement of modeling of human behavior and decision-making. The ultimate objective of the research thrust is to better enable success in the various system-of-systems that exist in society. © Springer-Verlag Berlin Heidelberg 2007.;Springer Verlag;Book Series;Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics);2007-01-01;https://api.elsevier.com/content/abstract/scopus_id/38149088669
197;Exploring the integration of the human as a flexibility factor in CPS enabled manufacturing environments: Methodology and results;Cyber Physical Systems (CPS) are expected to shape the evolution of production towards the fourth industrial revolution named Industry 4.0. The increasing integration of manufacturing processes and the strengthening of the autonomous capabilities of manufacturing systems make investigating the role of humans a primary research objective in view of emerging social and demographic megatrends. Understanding how the employees can be better integrated to enable increased flexibility in manufacturing systems is a prerequisite to allow technological solutions, as well as humans, to harness their full potential. Humans can supervise and adjust the settings, be a source of knowledge and competences, can diagnose situations, take decisions and several other activities influencing manufacturing performances, overall providing additional degrees of freedom to the systems. This paper, studies two different integration models: Human-in-the-Loop and Human-in-the-Mesh. They are both analysed in the context of four industrial cases of deployment of cyber physical systems in production.;IEEE Computer Societyhelp@computer.org;Conference Proceeding;IECON Proceedings (Industrial Electronics Conference);2016-12-21;https://api.elsevier.com/content/abstract/scopus_id/85010069986
198;Framework for Rapid Development of Embedded Human-in-The-Loop Cyber-Physical Systems;Human-in-The-Loop Cyber-Physical Systems (HiLCPS) offers assistive technology that augments human interaction with the physical world, such as self-feeding, communication and mobility for functionally locked-in individuals. HiLCPS applications are typically implemented as networked embedded systems interfacing both human and the physical environment. Developing HiLCPS applications is challenging due to interfacing with hardware with different specifications and physical location (local/remote). Also, while algorithm designers prototype applications in MATLAB benefiting from an algorithm design environment, the gap from prototyping MATLAB application to embedded solution traditionally requires significant manual implementation. In this paper, we propose a HiLCPS Framework for the rapid development of embedded HiLCPS applications. The framework groups similar hardware types to classes, unifying their access and with this offering both hardware and location transparent access. The framework furthermore incorporates a domain-specific synthesis tool, called Hsyn. Hsyn empowers algorithm designers to prototype a portable, hardware-Agnostic application in MATLAB while offering an automatic path to embedded deployment without requiring embedded knowledge. We demonstrate the benefit of the framework with a brain-controlled wheelchair application prototyped in MATLAB that transparently accesses a variety of EEG acquisition systems with local or remote connections. Then, by using Hsyn, the application is automatically deployed to a BeagleBone Black equipped with a custom-designed electrophysiological acquisition cape. Hsyn shows six orders of magnitude of productivity gain compared to manual embedded deployment. The wheelchair performs stepwise navigated based on human intent inference with 91% accuracy at 0.9 confidence threshold every 4 seconds on average over 9 users.;Institute of Electrical and Electronics Engineers Inc.;Conference Proceeding;Proceedings - 2016 IEEE 16th International Conference on Bioinformatics and Bioengineering, BIBE 2016;2016-12-16;https://api.elsevier.com/content/abstract/scopus_id/85011022658
199;Integration of a digital twin as human representation in a scheduling procedure of a cyber-physical production system;Cyber-physical production systems comprise the idea of connected, self-controlling devices. Through communication among the devices holistic information about the production system shall be gathered and for example used for production planning and control. Often the integration of human personnel in this kind of automated planning, control and execution of production processes is combined with an assumption of tasks by the computer system and thereby degradation of employees. The interconnection and negotiation among the devices open up possibilities for a new kind of integration of human employees interacting in such a technically autonomously working environment. This paper presents an approach, where a digital twin is developed, which assumes the communication and coordination tasks of the employee with the production system and acts as a representative. Thereby employees on the shop floor are able to take part in computational decision-making. The digital twin uses a database which emulates user behavior.;IEEE Computer Societyhelp@computer.org;Conference Proceeding;IEEE International Conference on Industrial Engineering and Engineering Management;2018-02-09;https://api.elsevier.com/content/abstract/scopus_id/85045258714
200;Framing blockchain-integrated digital twins for emergent healthcare management at local and city levels: A proof of concept;It has been witnessed that digital technology has the potential to improve the efficiency of emergent healthcare management in COVID-19, which however has not been widely adopted due to unclear definition and configuration. This research aims to propose a proof of concept of digital twins for emergent healthcare management through configuring the cyber and functional interdependencies of healthcare systems at local and city levels. Critical interdependencies of healthcare systems have been firstly identified at both levels, then the information and associated cyber and functional interdependencies embedded in seven critical hospital information systems (HISs) have been identified and mapped. The proposed conceptual digital twin-based approach has been then developed for information coordination amongst these critical HISs at both local and city levels based on permissioned blockchain to (1) integrate and manage the information from seven critical HISs, and further (2) predict the demands of medical resources according to patient trajectory. A case study has been finally conducted at three hospitals in London during the COVID-19 period, and the results showed that the developed framework of blockchain-integrated digital twins is a promising way to provide more accurate and timely procurement information to decision-makers and can effectively support evidence-based decisions on medical resource allocation in the pandemic.;ICE Publishing;Journal;Proceedings of the Institution of Civil Engineers: Engineering Sustainability;2023-01-01;https://api.elsevier.com/content/abstract/scopus_id/85159583161
201;Towards a human-centred approach in modelling and testing of cyber-physical systems;The ability to capture different levels of abstraction in a system model is especially important for remote integration, testing/verification, and manufacturing of cyber-physical systems (CPSs). However, the complexity of modelling and testing of CPSs makes these processes extremely prone to human error. In this paper we present our ongoing work on introducing human-centred considerations into modelling and testing of CPSs, which allow for agile iterative refinement processes of different levels of abstraction when errors are discovered or missing information is completed.;IEEE Computer Societyhelp@computer.org;Conference Proceeding;Proceedings of the International Conference on Parallel and Distributed Systems - ICPADS;2016-01-15;https://api.elsevier.com/content/abstract/scopus_id/84964627169
202;Reference Framework for Digital Twins within Cyber-Physical Systems;Cyber-Physical Systems (CPSs) represent systems which integrate physical units and processes with computational entities over Internet and allow ubiquitous access of information and services. Although the application of CPSs promise to positively transform many application fields, there are still many open questions and challenges on how to design and realize a CPS. As indicated in the third level of the 5-level CPS architecture, the so-called cyber level, one of the challenges addresses the need for digital twins as high-fidelity mirroring images of CPSs entities. This is a prerequisite to realize the upper levels of the 5-level CPS architecture-the cognition and configuration level. In the scientific literature, the concept of a Digital Twin is introduced as a concrete realization for mirroring physical entities in the virtual world. However, a reference framework for the main building blocks of a Digital Twin framework is missing. This hinders a reuse of best practices and proven solutions for concrete realizations of a Digital Twin. In order to tackle this problem, we have established a reference framework for Digital Twins within a CPS. Our framework specifies the main building blocks of a Digital Twin in terms of structure and interrelations. To achieve this goal, we performed a systematic literature review, where we evaluated existing Digital Twin realizations used in different application domains of CPSs and we applied Grounded Theory and Framework Analysis as underlying methodologies. This reference framework serves a blueprint for developing Digital Twins of physical entities which are part of a CPS.;Institute of Electrical and Electronics Engineers Inc.;Conference Proceeding;Proceedings - 2019 IEEE/ACM 5th International Workshop on Software Engineering for Smart Cyber-Physical Systems, SEsCPS 2019;2019-05-01;https://api.elsevier.com/content/abstract/scopus_id/85072842816
203;Digital Twin in manufacturing: A categorical literature review and classification;The Digital Twin (DT) is commonly known as a key enabler for the digital transformation, however, in literature is no common understanding concerning this term. It is used slightly different over the disparate disciplines. The aim of this paper is to provide a categorical literature review of the DT in manufacturing and to classify existing publication according to their level of integration of the DT. Therefore, it is distinct between Digital Model (DM), Digital Shadow (DS) and Digital Twin. The results are showing, that literature concerning the highest development stage, the DT, is scarce, whilst there is more literature about DM and DS.;Elsevier B.V.;Conference Proceeding;IFAC-PapersOnLine;2018-01-01;https://api.elsevier.com/content/abstract/scopus_id/85052915281
204;A complementary Cyber-Human Systems framework for Industry 4.0 Cyber-Physical Systems;"Humans are a vital element to automotive manufacturing; however, skilled production personnel have largely been designated as data receivers in Cyber-Physical Systems (CPS) of Industry 4.0. A renewed focus on the human worker who completes significant portions of manual value-added content in automotive assembly through Cyber-Human Systems (CHS) is allowing humans to perform their jobs more safely, efficiently, and supporting enhanced control and quality monitoring of manual manufacturing tasks. There is a need for a unified complementary framework of CHS and CPS to guide the implementation of future smart manufacturing systems.";Elsevier Ltd;Journal;Manufacturing Letters;2018-01-01;https://api.elsevier.com/content/abstract/scopus_id/85041009096
205;The past, present and future of cyber-physical systems: A focus on models;This paper is about better engineering of cyber-physical systems (CPSs) through better models. Deterministic models have historically proven extremely useful and arguably form the kingpin of the industrial revolution and the digital and information technology revolutions. Key deterministic models that have proven successful include differential equations, synchronous digital logic and single-threaded imperative programs. Cyber-physical systems, however, combine these models in such a way that determinism is not preserved. Two projects show that deterministic CPS models with faithful physical realizations are possible and practical. The first project is PRET, which shows that the timing precision of synchronous digital logic can be practically made available at the software level of abstraction. The second project is Ptides (programming temporally-integrated distributed embedded systems), which shows that deterministic models for distributed cyber-physical systems have practical faithful realizations. These projects are existence proofs that deterministic CPS models are possible and practical.;MDPI AGPostfachBaselCH-4005membranes@mdpi.com;Journal;Sensors (Switzerland);2015-02-26;https://api.elsevier.com/content/abstract/scopus_id/84928667408
206;A Cyber-Physical Systems architecture for Industry 4.0-based manufacturing systems;Recent advances in manufacturing industry has paved way for a systematical deployment of Cyber-Physical Systems (CPS), within which information from all related perspectives is closely monitored and synchronized between the physical factory floor and the cyber computational space. Moreover, by utilizing advanced information analytics, networked machines will be able to perform more efficiently, collaboratively and resiliently. Such trend is transforming manufacturing industry to the next generation, namely Industry 4.0. At this early development phase, there is an urgent need for a clear definition of CPS. In this paper, a unified 5-level architecture is proposed as a guideline for implementation of CPS.;Elsevier Ltd;Journal;Manufacturing Letters;2015-01-01;https://api.elsevier.com/content/abstract/scopus_id/84921300723
207;Calculi of complexity: How phenomena emerge from rules a review of complexity: A guided tour by melanie Mitchell;A complex system is one that generates emergent structures, usually by the repeated application of relatively simple rules. The structures are emergent in the sense that they are not specified in, and cannot be predicted from, the rules that produce them. Mitchell argues for a unified science of complexity but does not ultimately succeed in providing a coherent account. Her book is nevertheless useful as an introduction to some systems and algorithms that have been studied in complexity research and that may be applicable to natural phenomena, including the behavior of organisms. The book also contains interesting historical and biographical details about complexity research and some of the mathematicians and scientists who have contributed to it. A sample of the specific complex systems and algorithms Mitchell introduces is examined in this review, including iterated maps, cellular automata, genetic algorithms, and small world and random Boolean networks. The relevance of complexity theory for behavior analysis is also considered, including applications of simple rules, networks, cellular automata, and genetic algorithms.;Blackwell Publishing Ltd;Journal;Journal of the Experimental Analysis of Behavior;2013-03-01;https://api.elsevier.com/content/abstract/scopus_id/84881643962
208;Integrating the manufacturer usage description standard in the modelling of cyber–physical systems;The continuous growth of cyber–physical systems (CPS) attacks, especially due to the conflict in Ukraine, has highlighted the need for cybersecurity management mechanisms, due to the catastrophic consequences that a failure or attack on critical infrastructures such as power plants. Indeed, Gartner predicts that by 2025, 30% of critical infrastructures will suffer a cyberattack. In this context, defining the expected behaviour of the system is key to detecting and mitigating possible vulnerabilities both in the design and runtime phases. Modelling emerges as a tool that facilitates the analysis of the security offered by the system even before the system is implemented, allowing an early risk analysis. However, creating such a model is usually challenging due to its intrinsic complexity, or the reconfiguration needed after a security assessment due to a new vulnerability. The situation gets even worse when the system is a complex CPS-of-Systems, where different Constituent Systems (CS) are interconnected since cascade effects and dependencies are stronger and we might not have all the information from the third-party CS. Also, the results of the evaluation are typically used only during the design phase, thus missing out on potential security policies and mitigations that could be used during the system operation. In this sense, the Manufacturer Usage Description (MUD) allows the manufacturer to define access control policies that reduce the attack surface of a device. However, the limited expressiveness of this standard reduces the possibilities of its application in systems with more complex policies beyond the network level. We propose the usage of the MUD standard as a source of information for CPS modelling, providing information on interactions about third-party components of the system. In addition, we define an extended MUD model that deals with the expressiveness problems of the MUD and allows to automatically generate a behavioural profile that integrates the recommendations obtained from the assessment and modelling processes. The extended MUD could be used during runtime to reduce the attack surface of the system, enforce security configuration or even discern if a component is secure enough to be part of the ecosystem. Our approach has been validated in a real use case in the context of smart grid, to show its applicability.;Elsevier B.V.;Journal;Computer Standards and Interfaces;2024-01-01;https://api.elsevier.com/content/abstract/scopus_id/85166306008
209;A Review of the Roles of Digital Twin in CPS-based Production Systems;"The Digital Twin (DT) is one of the main concepts associated to the Industry 4.0 wave. This term is more and more used in industry and research initiatives; however, the scientific literature does not provide a unique definition of this concept. The paper aims at analyzing the definitions of the DT concept in scientific literature, retracing it from the initial conceptualization in the aerospace field, to the most recent interpretations in the manufacturing domain and more specifically in Industry 4.0 and smart manufacturing research. DT provides virtual representations of systems along their lifecycle. Optimizations and decisions making would then rely on the same data that are updated in real-time with the physical system, through synchronization enabled by sensors. The paper also proposes the definition of DT for Industry 4.0 manufacturing, elaborated by the European H2020 project MAYA, as a contribution to the research discussion about DT concept.";Elsevier B.V.;Conference Proceeding;Procedia Manufacturing;2017-01-01;https://api.elsevier.com/content/abstract/scopus_id/85029833606
210;Challenges in digital twin development for cyber-physical production systems;The recent advancement of information and communication technology makes digitalisation of an entire manufacturing shop-floor possible where physical processes are tightly intertwined with their cyber counterparts. This led to an emergence of a concept of digital twin, which is a realistic virtual copy of a physical object. Digital twin will be the key technology in Cyber-Physical Production Systems (CPPS) and its market is expected to grow significantly in the coming years. Nevertheless, digital twin is still relatively a new concept that people have different perspectives on its requirements, capabilities, and limitations. To better understand an effect of digital twin’s operations, mitigate complexity of capturing dynamics of physical phenomena, and improve analysis and predictability, it is important to have a development tool with a strong semantic foundation that can accurately model, simulate, and synthesise the digital twin. This paper reviews current state-of-art on tools and developments of digital twin in manufacturing and discusses potential design challenges.;Springer Verlagservice@springer.de;Book Series;Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics);2019-01-01;https://api.elsevier.com/content/abstract/scopus_id/85069172464
211;A unified digital twin framework for real-time monitoring and evaluation of smart manufacturing systems;Digital Twin (DT) is one of the key enabling technologies for realizing the promise of Smart Manufacturing (SM) and Industry 4.0 to improve production systems operation. Driven by the generation and analysis of high volume data coming from interconnected cyber and physical spaces, DTs are real-time digital images of physical systems, processes or products that help evaluate and improve business performance. This paper proposes a novel DT architecture for the real-time monitoring and evaluation of large-scale SM systems. An application to a manufacturing flow-shop is presented to illustrate the usefulness of the proposed methodology.;IEEE Computer Societyhelp@computer.org;Conference Proceeding;IEEE International Conference on Automation Science and Engineering;2019-08-01;https://api.elsevier.com/content/abstract/scopus_id/85072984851
212;Cyber Physical Systems Oriented Robot Development Platform;The development of systems, of various levels of complexity that can integrate physical with virtual components has become a priority for research in the context of emerging paradigms such as Cyber-Physical Systems or Internet for the Future. The authors propose a Robotic Development Platform architecture that integrates principles of Cyber-Physical Systems. The proposed architecture, is scalable, by facilitating the integration of different existing development and simulation tools and will allow robot systems to be tested in different environments, with different characteristics, and facilitate the integration of real world simulation with virtual environment simulation.;Elsevier;Conference Proceeding;Procedia Computer Science;2015-01-01;https://api.elsevier.com/content/abstract/scopus_id/84953252985
213;Task planning of cyber-human systems;Internet of Things (IoT) allows for cyber-physical applications to be created and composed to provide intelligent support or automation of end-user tasks. For many of such tasks, human participation is crucial to the success and the quality of the tasks. The cyber systems should proactively request help from the humans to accomplish the tasks when needed. However, the outcome of such system-human synergy may be affected by factors external to the systems. Failure to consider those factors when involving human participants in the tasks may result in suboptimal performance and negative experience on the humans. In this paper, we propose an approach for automated generation of control strategies of cyber-human systems. We investigate how explicit modeling of human participant can be used in automated planning to generate cooperative strategy of human and system to achieve a given task, by means of which best and appropriately utilize the human. Specifically, our approach consists of: (1) a formal framework for modeling cooperation between cyber system and human, and (2) a formalization of system-human cooperative task planning as strategy synthesis of stochastic multiplayer game. We illustrate our approach through an example of indoor air quality control in smart homes.;Springer Verlagservice@springer.de;Book Series;Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics);2015-01-01;https://api.elsevier.com/content/abstract/scopus_id/84944613076
214;Current status and advancement of cyber-physical systems in manufacturing;This paper presents the current status and the latest advancement of cyber-physical systems (CPS) in manufacturing. In order to understand CPS and its future potential in manufacturing, definitions and characteristics of CPS are explained and compared with cloud manufacturing concept. Research and applications are outlined to highlight the latest advancement in the field. CPS shows great promise in factories of the future in the areas of future trends as identified at the end of this paper.;Elsevier B.V.;Journal;Journal of Manufacturing Systems;2015-10-01;https://api.elsevier.com/content/abstract/scopus_id/84928606583
215;Value Creation with Digital Twins: Application-Oriented Conceptual Framework and Case Study;The internet of things, digital twins of smart connected products, and thereby enabled smart services are topics of great interest and have been gaining traction for many years. However, many questions concerning the application-oriented usage of digital twins still need to be scrutinized. Therefore, this paper examines the question of an application-oriented framework for value creation with digital twins using design science research approaches. A conceptual reference framework is presented based on earlier research and iteratively developed within workshops with three companies. The framework incorporates primary dimensions of external and internal value creation and data resources. Further, it discusses the product life cycle, the real-world counterpart, value creation in the ecosystem, and the generational aspect of the digital twins. Furthermore, applying the framework to a use case with an industrial research partner helps to show the contributions to the industrial sector. The framework provides utility to practitioners as a means of creating a common sense in interdisciplinary teams, communicating digital twin projects to internal and external stakeholders, and as a toolbox for specific challenges concerning digital twins. In addition, the framework distinguishes itself from existing approaches by including the service ecosystem and its actors while considering the principles of product life cycle management. Therefore, using the framework in other use cases will test the approach on different industries and products. Furthermore, there is a need to develop approaches for implementing and developing an existing case.;MDPI;Journal;Applied Sciences (Switzerland);2023-03-01;https://api.elsevier.com/content/abstract/scopus_id/85151514485
216;Updaticator: Updating billions of devices by an efficient, scalable and secure software update distribution over untrusted cache-enabled networks;Secure and fast distribution of software updates and patches is essential for improving functionality and security of computer systems. Today, each device downloads updates individually from a software provider distribution server. Unfortunately, this approach does not scale to large systems with billions of devices where the network bandwidth of the server and the local Internet gateway become bottlenecks. Cache-enabled Network (CN) services (either proprietary, as Akamai, or open Content-Distribution Networks) can reduce these bottlenecks. However, they do not offer security guarantees against potentially untrusted CN providers that try to threaten the confidentiality of the updates or the privacy of the users. In this paper, we propose Updaticator, the first protocol for software updates over Cache-enabled Networks that is scalable to billions of concurrent device updates while being secure against malicious networks. We evaluate our proposal considering Named-Data Networking, a novel instance of Cache-enabled overlay Networks. Our analysis and experimental evaluation show that Updaticator removes the bottlenecks of individual device-update distribution, by reducing the network load at the distribution server: from linear in the number of devices to a constant, even if billions of devices are requesting updates. Furthermore, when compared to the state-of-the-art individual device-update mechanisms, the download time with Updaticator is negligible, due to local caching. © 2014 Springer International Publishing Switzerland.;Springer Verlagservice@springer.de;Book Series;Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics);2014-01-01;https://api.elsevier.com/content/abstract/scopus_id/84906519005
217;An efficient and scalable vaccine passport verification system based on ciphertext policy attribute-based encryption and blockchain;Implementing a trust and secure immunity or vaccine passport verification system is now crucial for many countries. The system typically aims to enable the secure access control and verification of vaccination records which will be used by trusted parties. However, the issues related to the system scalability in supporting a large number of data access requests, the enforcement of the user consent for data sharing, and the flexibility in delegating the access capability to trusted parties have not been resolved by existing works. In this paper, we propose a Universal Vaccine Passport Verification System (UniVAC) to support a decentralized, scalable, secure, and fine-grained, access control for Covid-19 vaccine passport data sharing and verification. At a core of our scheme, we employ the ciphertext policy attribute-based encryption (CP-ABE) to support secure and fine-grained access control and use the blockchain to record access transactions and provide data indexing. Furthermore, we propose a ciphertext retrieval method based on regional blockchain segmentation and introduce the outsourced CP-ABE decryption as a part of the proxy re-encryption (PRE) process to enable scalable and secure ciphertext delivery of the encrypted vaccine passport under the requestor’s public key. Finally, we conducted the extensive experiments in real cloud environment and the results showed that our proposed scheme is more efficient and scalable than related works.;Springer Science and Business Media Deutschland GmbH;Journal;Journal of Cloud Computing;2023-12-01;https://api.elsevier.com/content/abstract/scopus_id/85167453968
218;Integrating Wearable and Camera Based Monitoring in the Digital Twin for Safety Assessment in the Industry 4.0 Era;The occurrence of human errors in work processes reduces the quality of results, increases the costs due to compensatory actions, and may have heavy repercussions on the workers’ safety. The definition of rules and procedures that workers have to respect has shown to be not enough to guarantee their safety, as negligence and opportunistic behaviours can unfortunately lead to catastrophic consequences. In the Industry 4.0 era, with the advent of the digital twin in smart factories, advanced systems can be exploited for automatic risk prediction and avoidance. By leveraging the new opportunities provided by the digital twin and, in particular, the introduction of wearable sensors and computer vision, we propose an automatic system for monitoring human behaviours in a smart factory in real time. The final goal is to feed cloud-based safety assessment tools that evaluate human errors and raise consequent alerts when required.;Springer Science and Business Media Deutschland GmbH;Book Series;Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics);2022-01-01;https://api.elsevier.com/content/abstract/scopus_id/85142691307
219;DIME: A programming-less modeling environment for web applications;We present DIME, an integrated solution for the rigorous model-driven development of sophisticated web applications based on the Dynamic Web Application (DyWA) framework, that is designed to accelerate the realization of requirements in agile development environments. DIME provides a family of Graphical Domain-Specific Languages (GDSLs), each of which is tailored towards a specific aspect of typical web applications, including persistent entities (i.e., a data model), business logic in form of various types of process models, the structure of the user interface, and access control. They are modeled on a high level of abstraction in a simplicity-driven fashion that focuses on describing what application is sought, instead of how the application is realized. The choice of platform, programming language, and frameworks is moved to the corresponding (full) code generator.;Springer Verlagservice@springer.de;Book Series;Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics);2016-01-01;https://api.elsevier.com/content/abstract/scopus_id/84993929512
220;The Road to European Digital Sovereignty with Gaia-X and IDSA;Digitalization is chewing the world with strong economic and social impacts. Recently, the management of the COVID 19 crisis highlighted the power of digital tools and their impact on stakes such as privacy, surveillance, transparency, and censorship. How nations deal with massive digitalization and master the technologies, and applications that are deployed and used on their soils by their companies and citizens is vividly raised by the U.S. ban on Huawei enforced by the 'clean network' strategy. This ban could have set the ground for a technology war between '2 blocks: the digital democracies and the techno-authoritarian regimes' [1], [2].;Institute of Electrical and Electronics Engineers Inc.;Journal;IEEE Network;2021-03-01;https://api.elsevier.com/content/abstract/scopus_id/85103448059
221;Low-Code Internet of Things Application Development for Edge Analytics;Internet of Things (IoT) applications combined with edge analytics are increasingly developed and deployed across a wide range of industries by engineers who are non-expert software developers. In order to enable them to build such IoT applications, we apply low-code technologies in this case study based on Model Driven Development. We use two different frameworks: DIME for the application design and implementation of IoT and edge aspects as well as analytics in R, and Pyrus for data analytics in Python, demonstrating how such engineers can build innovative IoT applications without having the full coding expertise. With this approach, we develop an application that connects a range of heterogeneous technologies: sensors through the EdgeX middleware platform with data analytics and web based configuration applications. The connection to data analytics pipelines can provide various kinds of information to the application users. Our innovative development approach has the potential to simplify the development and deployment of such applications in industry.;Springer Science and Business Media Deutschland GmbH;Book Series;IFIP Advances in Information and Communication Technology;2022-01-01;https://api.elsevier.com/content/abstract/scopus_id/85142727704
222;Smart and ecofriendly intelligent house based on iot and simulation using a Cisco networking simulator;Currently, the whole world is witnessing an era of technology. At this point, everyone is fully set up on digital devices. You can use the digital devices to protect your home and at the same time, you can look and manage your home through online (e.g., fan, air conditioner, television, home window, door, shutter, cooling, etc.). So, in this chapter, we will understand how we can control everything (electrical) in your home, and how we can adapt to the home environment. We have compiled this whole chapter with Cisco Packet Tracer Networking Simulator to help you and protect your home and also you can watch your home from anywhere. With the help of the IoT concept, you can manage and monitor all devices running on all batteries and the flowing machines running with the heat of the wire. You can also manage and monitor them.;Apple Academic Press;Book;Intelligent Sensor Node-Based Systems: Applications in Engineering and Science;2023-10-06;https://api.elsevier.com/content/abstract/scopus_id/85167528226
223;Model-Driven Edge Analytics: Practical Use Cases in Smart Manufacturing;In the Internet of Things (IoT) era, devices and systems generate enormous amounts of real-time data, and demand real-time analytics in an uninterrupted manner. The typical solution, a cloud-centred architecture providing an analytics service, cannot guarantee real-time responsiveness because of unpredictable workloads and network congestion. Recently, edge computing has been proposed as a solution to reduce latency in critical systems. For computation processing and analytics on edge, the challenges include handling the heterogeneity of devices and data, and achieving processing on the edge in order to reduce the amount of data transmitted over the network. In this paper, we show how low-code, model-driven approaches benefit a Digital Platform for Edge analytics. The first solution uses EdgeX, an IIoT framework for supporting heterogeneous architectures with the eKuiper rule-based engine. The engine schedules fully automatically tasks that retrieve data from the Edge, as the infrastructure near the data is generated, allowing us to create a continuous flow of information. The second solution uses FiWARE, an IIoT framework used in industry, using IoT agents to accomplish a pipeline for edge analytics. In our architecture, based on the DIME LC/NC Integrated Modelling Environment, both integrations of EdgeX/eKuyper and FiWARE happen by adding an External Native DSL to this Digital Platform. The DSL comprises a family of reusable Service-Independent Building blocks (SIBs), which are the essential modelling entities and (service) execution capabilities in the architecture’s modelling layer. They provide users with capabilities to connect, control and organise devices and components, and develop custom workflows in a simple drag and drop manner.;Springer Science and Business Media Deutschland GmbH;Book Series;Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics);2022-01-01;https://api.elsevier.com/content/abstract/scopus_id/85142698926
224;Feature-based Modelling of a Complex, Online-Reconfigurable Decision Support Service;In this paper, we show how the concepts of components, features and services are used today in the Online Conference System (OCS) in order to marry the modelling of functionally complex, online reconfigurable internet services at the application level with the needs of a model-driven development amenable to analyze and verify the models. Characteristic of the approach is the coarse-grained approach to modelling and design of features and services, which guarantees the scalability to capture large complex systems. The interplay of the different features and components is realized via a coordination-based approach, which is an easily understandable modelling paradigm of system-wide business processes, and thus adequate for the needs of industrial application developers. © 2006 Elsevier B.V. All rights reserved.;Elsevier;Journal;Electronic Notes in Theoretical Computer Science;2006-05-22;https://api.elsevier.com/content/abstract/scopus_id/33646354619
225;Service-orientation: Conquering complexity with XMDD;We advocate a new direction for mastering complexity in service-oriented design of complex applications: eXtreme Model-Driven Development (XMDD). It is a new application development paradigm that is extreme in that it is designed to involve the customer/application expert continuously throughout the whole systems' life cycle, and it is model-driven because it is based on the ‘One-Thing Approach’, which works by successively enriching and refining one single artifact that is a rich model. With XMDD, system development becomes in essence a pathway to user-centric orchestration of intuitive service functionality. XMDD differs radically from classical software development, which in our opinion, is no longer adequate for the bulk of application programming, because the profile of todays's applications has changed and demands agility and a leaner development style. This need is particularly evident when it comes to heterogeneous, cross-organizational systems, which must adapt to rapidly changing market requirements. XMDD addresses the needs via a model-driven, lightweight, and cooperative development paradigm that puts the user process at the center of development and the application expert in control of the process evolution.;Springer-Verlag London Ltd;Book;Conquering Complexity;2012-01-01;https://api.elsevier.com/content/abstract/scopus_id/84955352764
226;CINCO: a simplicity-driven approach to full generation of domain-specific graphical modeling tools;Even with the help of powerful metamodeling frameworks, the development of domain-specific graphical modeling tools is usually a complex, repetitive, and tedious task, which introduces substantial upfront costs often prohibiting such approaches in practice. In order to reduce these costs, the presented Cinco meta tooling suite is designed to provide a holistic approach that greatly simplifies the development of such domain-specific tools. Our solution is based on the idea to apply the concept of domain specialization also to the (meta-)domain of “domain-specific modeling tools”. Important here is our focus on complex graph-based models, comprising various kinds of nodes and edges together with their individual representation, correlations, and interpretation. This focus allows for high-level specifications of the model structures and functionalities as the prerequisite for push-button tool generation.;Springer Verlagservice@springer.de;Journal;International Journal on Software Tools for Technology Transfer;2018-06-01;https://api.elsevier.com/content/abstract/scopus_id/85019270516
227;Evaluating the FiwarE platform: A case-study on implementing smart application with fiware;This paper describes the result of a thorough analysis and evaluation of the so-called FIWARE platform from a smart application development point of view. FIWARE is the result of a series of well-funded EU projects that is currently intensively promoted throughout public agencies in Europe and world-wide. The goal was to figure out how services provided by FIWARE facilitate the development of smart applications. It was conducted first by an analysis of the central components that make up the service stack, followed by the implementation of a pilot project that aimed on using as many of these services as possible.;IEEE Computer Society;Conference Proceeding;Proceedings of the Annual Hawaii International Conference on System Sciences;2018-01-01;https://api.elsevier.com/content/abstract/scopus_id/85061767196
228;Low-code as enabler of digital transformation in manufacturing industry;Currently, enterprises have to make quick and resilient responses to changing market requirements. In light of this, low-code development platforms provide the technology mechanisms to facilitate and automate the development of software applications to support current enterprise needs and promote digital transformation. Based on a theory-building research methodology through the literature and other information sources review, the main contribution of this paper is the current characterisation of the emerging low-code domain following the foundations of the computer-aided software engineering field. A context analysis, focused on the current status of research related to the low-code development platforms, is performed. Moreover, benchmarking among the existing low-code development platforms addressed to manufacturing industry is analysed to identify the current lacking features. As an illustrative example of the emerging low-code paradigm and respond to the identified uncovered features, the virtual factory open operating system (vf-OS) platform is described as an open multi-sided low-code framework able to manage the overall network of a collaborative manufacturing and logistics environment that enables humans, applications, and Internet of Things (IoT) devices to seamlessly communicate and interoperate in the interconnected environment, promoting resilient digital transformation.;MDPI AGindexing@mdpi.com;Journal;Applied Sciences (Switzerland);2020-01-01;https://api.elsevier.com/content/abstract/scopus_id/85077580109
229;A review on edge computing: Working, comparisons, benefits, vision, instances and illustrations along with challenges;The number of commercial devices that can run fully-featured operating systems has expanded dramatically as a consequence of impressive developments in embedded ‘systems-on-a-chip’. The potential of the Internet of Things has expanded as a result of this change. Several earlier Internet of Things devices were essentially data collectors and transmitters, with no ability to carry out analysis. When it comes to edge computing, the ability to do sophisticated calculations at the location where they are being performed is facilitated by the rising processing power of today’s gadgets. By moving cloud computing services closer to the network’s border, edge computing allows for the creation of new services and products. This allows for a wide range of new possibilities. On the basis of a comparison between cloud computing and edge computing technologies, we evaluate, highlight, and report on recent advancements in edge computing technologies in this study. We also consider the field of maintenance and the benefits of edge computing. This presentation also includes a few illustrations and photographs. This investigation also considers the importance of this phenomenon in the functioning of 5G technologies.;CRC Press;Book;Internet of Things in Modern Computing: Theory and Applications;2023-01-01;https://api.elsevier.com/content/abstract/scopus_id/85164871325
230;MPDS-RCA: Multi-level privacy-preserving data sharing for resisting collusion attacks based on an integration of CP-ABE and LDP;In the ciphertext-policy attribute-based encryption (CP-ABE), once malicious users have gained access to the sharing data, they are able to obtain real private data, leading to serious privacy leakage issues. Thus, if the data user does not access the original private data, but instead accesses the perturbed data, while guaranteeing certain statistical characteristics, this would greatly improve the privacy of the CP-ABE technology as well as enhance its security. Motivated by this, an integration of basic CP-ABE and local differential privacy (LDP) based on randomized response for achieving multi-level privacy-preserving data sharing (MPDS), which has a relatively lower complexity and higher data utility, is constructed to provide double privacy protection for data owners at the source. To prevent different trusted data users from colluding and gaining more privacy beyond their trust levels, a random perturbation strategy is elaborately designed for resisting collusion attacks (RCA) while guaranteeing that the output of RCA perturbation strategy is the same as that of the original perturbation, which has been proved from the theoretical level. Finally, the proposed MPDS-RCA scheme is simulated and verified on both synthetic and real data sets, which indicates that the proposed scheme outperforms the existing MPPDS scheme in terms of the average error rate, the KL-divergence and the JS-divergence while reducing the computational cost by approximately 50%.;Elsevier Ltd;Journal;Computers and Security;2022-01-01;https://api.elsevier.com/content/abstract/scopus_id/85118537351
231;Convergence of Edge Computing and Deep Learning: A Comprehensive Survey;"Ubiquitous sensors and smart devices from factories and communities are generating massive amounts of data, and ever-increasing computing power is driving the core of computation and services from the cloud to the edge of the network. As an important enabler broadly changing people's lives, from face recognition to ambitious smart factories and cities, developments of artificial intelligence (especially deep learning, DL) based applications and services are thriving. However, due to efficiency and latency issues, the current cloud computing service architecture hinders the vision of 'providing artificial intelligence for every person and every organization at everywhere'. Thus, unleashing DL services using resources at the network edge near the data sources has emerged as a desirable solution. Therefore, edge intelligence, aiming to facilitate the deployment of DL services by edge computing, has received significant attention. In addition, DL, as the representative technique of artificial intelligence, can be integrated into edge computing frameworks to build intelligent edge for dynamic, adaptive edge maintenance and management. With regard to mutually beneficial edge intelligence and intelligent edge, this paper introduces and discusses: 1) the application scenarios of both; 2) the practical implementation methods and enabling technologies, namely DL training and inference in the customized edge computing framework; 3) challenges and future trends of more pervasive and fine-grained intelligence. We believe that by consolidating information scattered across the communication, networking, and DL areas, this survey can help readers to understand the connections between enabling technologies while promoting further discussions on the fusion of edge intelligence and intelligent edge, i.e., Edge DL.";Institute of Electrical and Electronics Engineers Inc.;Journal;IEEE Communications Surveys and Tutorials;2020-04-01;https://api.elsevier.com/content/abstract/scopus_id/85085658645
232;Pyrus: An Online Modeling Environment for No-Code Data-Analytics Service Composition;We present Pyrus, a domain-specific online modeling environment for building graphical processes for data analysis, machine learning and artificial intelligence. Pyrus aims at bridging the gap between de facto (often Python-based) standards as established by the Jupyter platform, and the tradition to model data analysis workflows in a dataflow-driven fashion. Technically, Pyrus integrates established online IDEs like Jupyter and allows users to graphically combine available functional components to dataflow-oriented workflows in a collaborative fashion without writing a single line of code. Following a controlflow/dataflow conversion and compilation, the execution is then delegated to the underlying platforms. Both the inputs to a modeled workflow and the results of its execution can be specified and viewed without leaving Pyrus which supports a seamless cooperation between data science experts and programmers. The paper illustrates the fundamental concepts, the employed domain-specific language, and, in particular, the role of the integrated IDE’s in an example-driven fashion which can be reproduced in the available online modeling environment.;Springer Science and Business Media Deutschland GmbH;Book Series;Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics);2021-01-01;https://api.elsevier.com/content/abstract/scopus_id/85118145576
233;Facebook's cyber-cyber and cyber-physical digital twins;A cyber-cyber digital twin is a simulation of a software system. By contrast, a cyber-physical digital twin is a simulation of a non-software (physical) system. Although cyber-physical digital twins have received a lot of recent attention, their cyber-cyber counterparts have been comparatively overlooked. In this paper we show how the unique properties of cyber-cyber digital twins open up exciting opportunities for research and development. Like all digital twins, the cyber-cyber digital twin is both informed by and informs the behaviour of the twin it simulates. It is therefore a software system that simulates another software system, making it conceptually truly a twin, blurring the distinction between the simulated and the simulator. Cyber-cyber digital twins can be twins of other cyber-cyber digital twins, leading to a hierarchy of twins. As we shall see, these apparently philosophical observations have practical ramifications for the design, implementation and deployment of digital twins at Facebook.;Association for Computing Machinery;Conference Proceeding;ACM International Conference Proceeding Series;2021-06-21;https://api.elsevier.com/content/abstract/scopus_id/85108913162
234;Inferring hierarchical motifs from execution traces;Program comprehension is a necessary step for performing many software engineering tasks. Dynamic analysis is effective in producing execution traces that assist comprehension. Traces are rich sources of information regarding the behaviour of a program. However, it is challenging to gain insight from traces due to their overwhelming amount of data and complexity. We propose a generic technique for facilitating comprehension by inferring recurring execution motifs. Inspired by bioinformatics, motifs are patterns in traces that are flexible to small changes in execution, and are captured in a hierarchical model. The hierarchical nature of the model provides an overview of the behaviour at a high-level, while preserving the execution details and intermediate levels in a structured manner. We design a visualization that allows developers to observe and interact with the model. We implement our approach in an open-source tool, called Sabalan, and evaluate it through a user experiment. The results show that using Sabalan improves developers' accuracy in performing comprehension tasks by 54%.;IEEE Computer Societyhelp@computer.org;Conference Proceeding;Proceedings - International Conference on Software Engineering;2018-05-27;https://api.elsevier.com/content/abstract/scopus_id/85049390034
235;Species Survey of Iranian Barberry Genotypes Using ITS2 Sequences and Basic Local Alignment Search Tools;The taxonomy of Iranian wild barberry and the unique seedless cultivar (CV) ‘Zereshk Bidaneh’ based on morphological characteristics is debated. The nuclear internal transcribed spacer (ITS) regions have high accuracy and efficiency for plant species identification and DNA barcoding. In this study, 17 wild barberry genotypes from northern and northeastern regions of Iran, along with CV ‘Zereshk Bidaneh’ and Japonica barberry, were classified by ITS2 sequence analysis. Genetic distance and phylogenetic relationships among Berberis genotypes were assessed with MEGA X and the NCBI database. The results showed that the genetic distance between the studied barberry accessions varied from 0.012 to 0.154. Cluster analysis classified Iranian barberry genotypes into six groups, and the ‘JA’, ES4 and R9N3 genotypes were placed in a separate clade. In BLAST analysis, 93 DNA sequences from different barberry species in the NCBI database had over 90% identity with ITS sequences of the studied barberries. Seventeen of the studied Berberis genotypes were classified into two groups in an NCBI similarity dendrogram. The first group included 11 genotypes that were placed in the B. crataegina clade, and six genotypes of the second group along with the other NCBI Berberis genotypes belong to B. integerrima. The R9N3 genotype was separated from other studied barberry genotypes and inserted alongside the Berberis species, which are all unlisted species in Iran. ‘Zereshk Bidaneh’ was in the B. integerrima clade. ITS2 sequence analysis can be used for true species identification based on DNA databases.;Springer Science and Business Media Deutschland GmbH;Journal;Erwerbs-Obstbau;2023-01-01;https://api.elsevier.com/content/abstract/scopus_id/85168460418
236;A sub-quadratic sequence alignment algorithm for unrestricted cost matrices;The classical algorithm for computing the similarity between two sequences [36, 39] uses a dynamic programming matrix, and compares two strings of size n in 0(n2) time. We address the challenge of computing the similarity of two strings in sub-quadratic time, for metrics which use a scoring matrix of unrestricted weights. Our algorithm applies to both local and global alignment computations. The speed-up is achieved by dividing the dynamic programming matrix into variable sized blocks, as induced by Lempel-Ziv parsing of both strings, and utilizing the inherent periodic nature of both strings. This leads to an O(n2/logn) algorithm for an input of constant alphabet size. For most texts, the time complexity is actually 0(hn21 logn) where h ≤ 1 is the entropy of the text.;Association for Computing Machineryacmhelp@acm.org;Conference Proceeding;Proceedings of the Annual ACM-SIAM Symposium on Discrete Algorithms;2002-01-01;https://api.elsevier.com/content/abstract/scopus_id/84968754816
237;Motion Similarity Evaluation between Human and a Tri-Co Robot during Real-Time Imitation with a Trajectory Dynamic Time Warping Model;Precisely imitating human motions in real-time poses a challenge for the robots due to difference in their physical structures. This paper proposes a human–computer interaction method for remotely manipulating life-size humanoid robots with a new metrics for evaluating motion similarity. First, we establish a motion capture system to acquire the operator’s motion data and retarget it to the standard bone model. Secondly, we develop a fast mapping algorithm, by mapping the BVH (BioVision Hierarchy) data collected by the motion capture system to each joint motion angle of the robot to realize the imitated motion control of the humanoid robot. Thirdly, a DTW (Dy-namic Time Warping)-based trajectory evaluation method is proposed to quantitatively evaluate the difference between robot trajectory and human motion, and meanwhile, visualization terminals render it more convenient to make comparisons between two different but simultaneous motion systems. We design a complex gesture simulation experiment to verify the feasibility and real-time performance of the control method. The proposed human-in-the-loop imitation control method addresses a prominent non-isostructural retargeting problem between human and robot, enhances robot interaction capability in a more natural way, and improves robot adaptability to uncertain and dynamic environments.;MDPI;Journal;Sensors;2022-03-01;https://api.elsevier.com/content/abstract/scopus_id/85125401290
238;Semantic model differencing utilizing behavioral semantics specifications;Identifying differences among models is a crucial prerequisite for several development and change management tasks in model-driven engineering. The majority of existing model differencing approaches focus on revealing syntactic differences which can only approximate semantic differences among models. Significant advances in semantic model differencing have been recently made by Maoz et al. [16] who propose semantic diff operators for UML class and activity diagrams. In this paper, we present a generic semantic differencing approach which can be instantiated to realize semantic diff operators for specific modeling languages. Our approach utilizes the behavioral semantics specification of the considered modeling language, which enables to execute models and capture execution traces representing the models’ semantic interpretation. Based on this semantic interpretation, semantic differences can be revealed.;Springer Verlag;Book Series;Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics);2014-01-01;https://api.elsevier.com/content/abstract/scopus_id/84921662127
239;Trace comprehension operators for executable DSLs;Recent approaches contribute facilities to breathe life into metamodels, thus making behavioral models directly executable. Such facilities are particularly helpful to better utilize a model over the time dimension, e.g., for early validation and verification. However, when even a small change is made to the model, to the language definition (e.g., semantic variation points), or to the external stimuli of an execution scenario, it remains difficult for a designer to grasp the impact of such a change on the resulting execution trace. This prevents accessible trade-off analysis and design-space exploration on behavioral models. In this paper, we propose a set of formally defined operators for analyzing execution traces. The operators include dynamic trace filtering, trace comparison with diff computation and visualization, and graph-based view extraction to analyze cycles. The operators are applied and validated on a demonstrative example that highlight their usefulness for the comprehension specific aspects of the underlying traces.;Springer Verlagservice@springer.de;Book Series;Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics);2018-01-01;https://api.elsevier.com/content/abstract/scopus_id/85048885130
240;Leveraging digital twin technology in model-based systems engineering;Digital twin, a concept introduced in 2002, is becoming increasingly relevant to systems engineering and, more specifically, to model-based system engineering (MBSE). A digital twin, like a virtual prototype, is a dynamic digital representation of a physical system. However, unlike a virtual prototype, a digital twin is a virtual instance of a physical system (twin) that is continually updated with the latter’s performance, maintenance, and health status data throughout the physical system’s life cycle. This paper presents an overall vision and rationale for incorporating digital twin technology into MBSE. The paper discusses the benefits of integrating digital twins with system simulation and Internet of Things (IoT) in support of MBSE and provides specific examples of the use and benefits of digital twin technology in different industries. It concludes with a recommendation to make digital twin technology an integral part of MBSE methodology and experimentation testbeds.;MDPI AG;Journal;Systems;2019-03-01;https://api.elsevier.com/content/abstract/scopus_id/85111581098
241;Distance measures for time series in r: The TSdist package;The definition of a distance measure between time series is crucial for many time series data mining tasks, such as clustering and classification. For this reason, a vast portfolio of time series distance measures has been published in the past few years. In this paper, the TSdist package is presented, a complete tool which provides a unified framework to calculate the largest variety of time series dissimilarity measures available in R at the moment, to the best of our knowledge. The package implements some popular distance measures which were not previously available in R, and moreover, it also provides wrappers for measures already included in other R packages. Additionally, the application of these distance measures to clustering and classification tasks is also supported in TSdist, directly enabling the evaluation and comparison of their performance within these two frameworks.;Technische Universitaet Wieninstitut@statistik.tuwien.ac.at;Journal;R Journal;2016-01-01;https://api.elsevier.com/content/abstract/scopus_id/85013170695
242;Model-driven Runtime State Identification;With new advances such as Cyber-Physical Systems (CPS) and Internet of Things (IoT), more and more discrete software systems interact with continuous physical systems. State machines are a classical approach to specify the intended behavior of discrete systems during development. However, the actual realized behavior may deviate from those specified models due to environmental impacts, or measurement inaccuracies. Accordingly, data gathered at runtime should be validated against the specified model. A first step in this direction is to identify the individual system states of each execution of a system at runtime. This is a particular challenge for continuous systems where system states may be only identified by listening to sensor value streams. A further challenge is to raise these raw value streams on a model level for checking purposes. To tackle these challenges, we introduce a model-driven runtime state identification approach. In particular, we automatically derive corresponding time-series database queries from state machines in order to identify system runtime states based on the sensor value streams of running systems. We demonstrate our approach for a subset of SysML and evaluate it based on a case study of a simulated environment of a five-axes grip-arm robot within a working station.;Gesellschaft fur Informatik (GI);Conference Proceeding;Lecture Notes in Informatics (LNI), Proceedings - Series of the Gesellschaft fur Informatik (GI);2020-01-01;https://api.elsevier.com/content/abstract/scopus_id/85127211145
243;On Digital Twins, Mirrors, and Virtualizations: Frameworks for Model Verification and Validation;"A powerful new idea in the computational representation of structures is that of the digital twin. The concept of the digital twin emerged and developed over the last decade, and has been identified by many industries as a highly desired technology. The current situation is that individual companies often have their own definitions of a digital twin, and no clear consensus has emerged. In particular, there is no current mathematical formulation of a digital twin. A companion paper to the current one will attempt to present the essential components of the desired formulation. One of those components is identified as a rigorous representation theory of models; most importantly, governing how they are verified and validated, and how validation information can be transferred between models. Unlike its companion, which does not attempt detailed specification of any twin components, this paper will attempt to outline a rigorous representation theory of models, based on the introduction of two new concepts: mirrors and virtualizations. The paper is not intended as a passive wish list; it is intended as a rallying call. The new theory will require the active participation of researchers across a number of domains including: pure and applied mathematics, physics, computer science, and engineering. The paper outlines the main objects of the theory and gives examples of the sort of theorems and hypotheses that might be proved in the new framework.";American Society of Mechanical Engineers (ASME) infocentral@asme.org;Journal;ASCE-ASME Journal of Risk and Uncertainty in Engineering Systems, Part B: Mechanical Engineering;2020-09-01;https://api.elsevier.com/content/abstract/scopus_id/85092429524
244;Faster than real-time simulation of mobile crane dynamics using digital twin concept;The paper discusses the problem of the mobile crane movement prediction in real-time during the operation of the crane. Dynamic and kinematic models of the crane are created to solve this problem. The models form the digital twin that can be used to facilitate the crane operation. As the crane can move rather fast, the crane dynamics, rather than kinematics, becomes more important in its movement prediction. However, in order to be calculated faster than real-time the crane model should be simplified. The paper considers an example of the mobile crane for which two models are developed: the detailed reference model and the simplified model for faster than real-time calculations, which includes the dynamic and kinematic equations. We implemented both models as C programs. Experiments are made in order to compare the accuracy and the calculation speed of the simplified model with respect to the reference model.;Institute of Physics Publishinghelen.craven@iop.org;Conference Proceeding;Journal of Physics: Conference Series;2018-12-11;https://api.elsevier.com/content/abstract/scopus_id/85058999096
245;The past, present and future of cyber-physical systems: A focus on models;This paper is about better engineering of cyber-physical systems (CPSs) through better models. Deterministic models have historically proven extremely useful and arguably form the kingpin of the industrial revolution and the digital and information technology revolutions. Key deterministic models that have proven successful include differential equations, synchronous digital logic and single-threaded imperative programs. Cyber-physical systems, however, combine these models in such a way that determinism is not preserved. Two projects show that deterministic CPS models with faithful physical realizations are possible and practical. The first project is PRET, which shows that the timing precision of synchronous digital logic can be practically made available at the software level of abstraction. The second project is Ptides (programming temporally-integrated distributed embedded systems), which shows that deterministic models for distributed cyber-physical systems have practical faithful realizations. These projects are existence proofs that deterministic CPS models are possible and practical.;MDPI AGPostfachBaselCH-4005membranes@mdpi.com;Journal;Sensors (Switzerland);2015-02-26;https://api.elsevier.com/content/abstract/scopus_id/84928667408
246;Cyber-Physical Systems: Foundations, Principles and Applications;Cyber-Physical Systems: Foundations, Principles and Applications explores the core system science perspective needed to design and build complex cyber-physical systems. Using Systems Science's underlying theories, such as probability theory, decision theory, game theory, organizational sociology, behavioral economics, and cognitive psychology, the book addresses foundational issues central across CPS applications, including System Design -- How to design CPS to be safe, secure, and resilient in rapidly evolving environments, System Verification -- How to develop effective metrics and methods to verify and certify large and complex CPS, Real-time Control and Adaptation -- How to achieve real-time dynamic control and behavior adaptation in a diverse environments, such as clouds and in network-challenged spaces, Manufacturing -- How to harness communication, computation, and control for developing new products, reducing product concepts to realizable designs, and producing integrated software-hardware systems at a pace far exceeding today's timeline. The book is part of the Intelligent Data-Centric Systems: Sensor-Collected Intelligence series edited by Fatos Xhafa, Technical University of Catalonia. Indexing: The books of this series are submitted to EI-Compendex and SCOPUS. Includes in-depth coverage of the latest models and theories that unify perspectives, expressing the interacting dynamics of the computational and physical components of a system in a dynamic environment. Focuses on new design, analysis, and verification tools that embody the scientific principles of CPS and incorporate measurement, dynamics, and control. Covers applications in numerous sectors, including agriculture, energy, transportation, building design and automation, healthcare, and manufacturing.;Elsevier Inc.;Book;Cyber-Physical Systems: Foundations, Principles and Applications;2016-09-11;https://api.elsevier.com/content/abstract/scopus_id/85024134976
247;Industrial automation based on cyber-physical systems technologies: Prototype implementations and challenges;Cyber-Physical Systems (CPS) is an emergent approach that focuses on the integration of computational applications with physical devices, being designed as a network of interacting cyber and physical elements. CPS control and monitor real-world physical infrastructures and thus is starting having a high impact in industrial automation. As such design, implementation and operation of CPS and management of the resulting automation infrastructure is of key importance for the industry. In this work, an overview of key aspects of industrial CPS, their technologies and emerging directions, as well as challenges for their implementation is presented. Based on the hands-on experiences gathered from four European innovation projects over the last decade (i.e. SOCRADES, IMC-AESOP, GRACE and ARUM), a key challenges have been identified and a prioritization and timeline are pointed out with the aim to increase Technology Readiness Levels and lead to their usage in industrial automation environments.;Elsevier;Journal;Computers in Industry;2016-09-01;https://api.elsevier.com/content/abstract/scopus_id/84941662063
248;Smart Agents in Industrial Cyber-Physical Systems;Future industrial systems can be realized using the cyber-physical systems (CPSs) that advocate the coexistence of cyber and physical counterparts in a network structure to perform the system's functions in a collaborative manner. Multiagent systems share common ground with CPSs and can empower them with a multitude of capabilities in their efforts to achieve complexity management, decentralization, intelligence, modularity, flexibility, robustness, adaptation, and responsiveness. This work surveys and analyzes the current state of the industrial application of agent technology in CPSs, and provides a vision on the way agents can effectively enable emerging CPS challenges.;Institute of Electrical and Electronics Engineers Inc.;Journal;Proceedings of the IEEE;2016-05-01;https://api.elsevier.com/content/abstract/scopus_id/84977992593
249;A coordinated approach for managing polypharmacy among children with medical complexity: rationale and design of the Pediatric Medication Therapy Management (pMTM) randomized controlled trial;Background: Children with medical complexity (CMC) often rely upon the use of multiple medications to sustain quality of life and control substantial symptom burden. Pediatric polypharmacy (≥ 5 concurrent medications) is prevalent and increases the risk of medication-related problems (MRPs). Although MRPs are associated with pediatric morbidity and healthcare utilization, polypharmacy is infrequently assessed during routine clinical care for CMC. The aim of this randomized controlled trial is to determine if a structured pharmacist-led Pediatric Medication Therapy Management (pMTM) intervention reduces MRP counts, as well as the secondary outcomes of symptom burden and acute healthcare utilization. Methods: This is a hybrid type 2 randomized controlled trial assessing the effectiveness of pMTM compared to usual care in a large, patient-centered medical home for CMC. Eligible patients include all children ages 2–18 years old, with ≥ 1 complex chronic condition, and with ≥ 5 active medications, as well as their English-speaking primary caregivers. Child participants and their primary parental caregivers will be randomized to pMTM or usual care before a non-acute primary care visit and followed for 90 days. Using generalized linear models, the overall effectiveness of the intervention will be evaluated using total MRP counts at 90 days following pMTM intervention or usual care visit. Following attrition, a total of 296 CMC will contribute measurements at 90 days, which provides > 90% power to detect a clinically significant 1.0 reduction in total MRPs with an alpha level of 0.05. Secondary outcomes include Parent-Reported Outcomes of Symptoms (PRO-Sx) symptom burden scores and acute healthcare visit counts. Program replication costs will be assessed using time-driven activity-based scoring. Discussion: This pMTM trial aims to test hypotheses that a patient-centered medication optimization intervention delivered by pediatric pharmacists will result in lower MRP counts, stable or improved symptom burdens, and fewer cumulative acute healthcare encounters at 90 days following pMTM compared to usual care. The results of this trial will be used to quantify medication-related outcomes, safety, and value for a high-utilization group of CMC, and outcomes may elucidate the role of integrated pharmacist services as a key component of outpatient complex care programs for this priority pediatric population. Trial Registration: This trial was prospectively registered at clinicaltrials.gov (NCT05761847) on Feb 25, 2023.;BioMed Central Ltd;Journal;BMC Health Services Research;2023-12-01;https://api.elsevier.com/content/abstract/scopus_id/85156106397
250;An integrated socio-cyber-physical system framework to assess responsible digitalisation in agriculture: A first application with Living Labs in Europe;"CONTEXT: It is still an open question how to assess the contribution of digitalisation in agriculture to the United Nations' Sustainable Development Goals, and how digitalisation then can be done in a responsible way. A socio-cyber-physical system (SCPS) concept can help this analysis, but little experience exists with its operationalisation and application, and its integration with the Responsible Research and Innovation approach. OBJECTIVE: To address this gap, this paper has a twofold purpose: a) operationalise the SCPS concept within an integrated assessment framework adaptable to multiple levels of analysis, contexts, and purposes (e.g. ex-ante, ongoing, ex post evaluation) to shed light on impacts of digitalisation in relation to SCPS entities, relationships, and activities; b) apply the designed framework in 21 multi-stakeholder platforms (Living Labs), which were established to explore needs and expectations in specific subjects relevant for European agriculture, forestry and rural areas. METHODS: Impacts were assessed through interviews (158 respondents), focus groups (378 participants), online surveys (273 respondents), and other secondary data. RESULTS AND CONCLUSIONS: The findings indicate that the SCPS framework enables elucidating relationships between digital and broader sustainable development goals and needs, and can sharpen earlier assessments, going beyond a pessimistic or optimistic dichotomy associated to digitalisation by specifying effects and trade-offs in terms of enabling, disenabling, boosting and depleting impacts of digital agriculture. However, the framework being comprehensive and open to emerging socio-cyber-physical interactions, makes that Livings Labs doing participatory impact assessments struggled with the complexity and multiple dimensions of the topic. SIGNIFICANCE: The paper provides both conceptual and operational knowledge to set up impact evaluations of responsible digitalisation in agriculture and outline concepts that can help anticipating the consequences and trade-offs.";Elsevier Ltd;Journal;Agricultural Systems;2022-12-01;https://api.elsevier.com/content/abstract/scopus_id/85139835725
251;A Cyber Physical Systems Approach for Agricultural Enterprise and Sustainable Agriculture;The quantity and quality of agricultural products that become part of the food supply chain is becoming an important problem as the population is fast-increasing. Information and Communication Technology is becoming part of Agriculture organizations as farmers have had to explore innovative ways to increase production and minimize risk. The next step therefore is to network these individual systems into cyber-physical production systems integrated with smart devices in the Internet of Things. These can map the entire process electronically, from the farm's computer to the harvesting operation. Thus, these systems have the potential to substantially increase efficiency and quality. The paper proposes an architecture for the future agricultural enterprise as a complex system, addressing sustainability and adaptability towards environmental and market changes.;Institute of Electrical and Electronics Engineers Inc.;Conference Proceeding;Proceedings - 2017 21st International Conference on Control Systems and Computer, CSCS 2017;2017-07-05;https://api.elsevier.com/content/abstract/scopus_id/85027529103
252;Defining a Digital Twin-based Cyber-Physical Production System for autonomous manufacturing in smart shop floors;Smart manufacturing is the core idea of the fourth industrial evolution. For a smart manufacturing shop floor, real-time monitoring, simulation and prediction of manufacturing operations are vital to improve the production efficiency and flexibility. In this paper, the Cyber-Physical System (CPS) and Digital Twin technologies are introduced to build the interconnection and interoperability of a physical shop floor and corresponding cybershop floor. A Digital Twin-based Cyber-Physical Production System (DT-CPPS) is further established, and the configuring mechanism, operating mechanism and real-time data-driven operations control of DT-CPPS are discussed in detail. It is expected that DT-CPPS will provide the basis for shop floors to march towards smart manufacturing.;Taylor and Francis Ltd.michael.wagreich@univie.ac.at;Journal;International Journal of Production Research;2019-10-18;https://api.elsevier.com/content/abstract/scopus_id/85060183300
253;Digital twin-driven manufacturing cyber-physical system for parallel controlling of smart workshop;With increasing diverse product demands, the manufacturing paradigm has been transformed into a mass-individualized one, among which one bottleneck is to achieve the interoperability between physical world and the digital world of manufacturing system for the intelligent organizing of resources. This paper presents a digital twin-driven manufacturing cyber-physical system (MCPS) for parallel controlling of smart workshop under mass individualization paradigm. By establishing cyber-physical connection via decentralized digital twin models, various manufacturing resources can be formed as dynamic autonomous system to co-create personalized products. Clarification on the MCPS concept, characteristics, architecture, configuration, operating mechanism and key enabling technologies are elaborated, respectively. A demonstrative implementation of the digital twin-driven parallel controlling of board-type product smart manufacturing workshop is also presented. It addresses a bi-level online intelligence in proactive decision making for the organization and operation of manufacturing resources.;Springer Verlagservice@springer.de;Journal;Journal of Ambient Intelligence and Humanized Computing;2019-03-13;https://api.elsevier.com/content/abstract/scopus_id/85049566739
254;Representing adaptation options in experimentable digital twins of production systems;Simulations are powerful tools for decision support during factory adaptation processes. In order to provide the most valuable form of decision support, a tool must take into account the decision problem and relevant decision alternatives. Today’s simulation tools, however, only accept simple notions of variability, like numeric parameter ranges. Thereby they ignore most of the variability of production systems, and do not utilise their full potential to aid decisions. To improve this situation, a new concept on how to systematically model the variability of production systems in Digital Twins of production entities is proposed. It combines Model-Based Systems Engineering and Variability Management to model different variants of production systems, and utilises capabilities of production equipment to make Digital Twins modular and reconfigurable. Each valid combination of variants results in a directly 3D-simulable Digital Twin for the whole production system, allowing automatic validation testing and fast feedback loops during system development. The presented concept is not only a very important basis for managing variants. As the variants model can be used as a search space for optimisation algorithms, the concept is an important stepping stone for a more powerful simulation-based optimisation of production systems.;Taylor and Francis Ltd.michael.wagreich@univie.ac.at;Journal;International Journal of Computer Integrated Manufacturing;2019-05-04;https://api.elsevier.com/content/abstract/scopus_id/85064719396
255;A simulation-based architecture for smart cyber-physical systems;In order to accurately predict future states of a smart cyber-physical system, which can change its behavior to a large degree in response to environmental influences, the existence of precise models of the system and its surroundings is demandable. In machine engineering, ultra-high fidelity simulations have been developed to better understand both constraints in system design and possible consequences of external influences during the system's operation. These digital twins enable further applications in software design for complex cyber-physical systems as online planning methods can utilize good simulations to continuously optimize the system behavior, yielding a software architecture framework based on the information flow between the cyber-physical system, its physical environment and the digital twin model.;Institute of Electrical and Electronics Engineers Inc.;Conference Proceeding;Proceedings - 2016 IEEE International Conference on Autonomic Computing, ICAC 2016;2016-09-21;https://api.elsevier.com/content/abstract/scopus_id/84991687905
256;Disruptions are the norm: Cyber-physical multi-agent systems for autonomous real-time resource management;This paper analyses the new requirements for real time resource management systems based on multi-agent technology. It shows the growing demand for developing autonomous systems which combine resource allocation, scheduling, optimization, communication with users and control in one cycle and can respond rapidly to unexpected events in real time. To solve the problem, cyber-physical multi-agent systems are considered. The paper also analyses the new impact which such systems bring into design of modern systems on the way from smart Internet of Things—to new organizations and ways of user motivation.;Springer Verlagservice@springer.de;Book Series;Studies in Computational Intelligence;2017-01-01;https://api.elsevier.com/content/abstract/scopus_id/85014958036
257;Towards autonomous ai systems for resource management: Applications in industry and lessons learned;Complexity of modern resource management is analyzed and related with a number of decision makers, high variety of individual criteria, preferences and constraints, interdependency of all operations, etc. The overview of existing methods and tools of Enterprise Resource Planning is given and key requirements for resource management are specified. The concept of autonomous Artificial Intelligence (AI) systems for adaptive resource management based on multi-agent technology is discussed. Multi-agent model of virtual market and method for solving conflicts and finding consensus for adaptive resource management are presented. Functionality and architecture of autonomous AI systems for adaptive resource management and the approach for measuring adaptive intelligence and autonomy level in these systems are considered. Results of delivery of autonomous AI solutions for managing trucks and factories, mobile teams, supply chains, aerospace and railways are presented. Considerable increase of enterprise resources efficiency is shown. Lessons learned from industry applications are formulated and future developments of AI for solving extremely complex problems of adaptive resource management are outlined.;Springer Verlagservice@springer.de;Book Series;Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics);2018-01-01;https://api.elsevier.com/content/abstract/scopus_id/85049361002
258;Ontology-Driven Multi-Agent Engine for Real Time Adaptive Scheduling;The growing demand for improving business efficiency requests the development of generic resource management systems applicable for solving a wide range of complex problems with minimum cost and time. However, the classical combinatorial or heuristic methods and tools do not provide adequate solutions for solving complex problems of resource management in real time. That is why we consider multi-agent technology as the core part of such solutions-which helps find the balance of many interests and adapt it in a flexible way to unpredictable events, such as a new order, an unavailable resource, etc. In this paper we introduce the use of ontology for scheduling, which provides the opportunity to create ontological model of the enterprise, develop generic multi-agent scheduler and customize matching requirements for each operation in business or technological processes, for example, for applications in manufacturing, project management, supply chains, etc. Semantic Wikipedia on the top of ontology editor will be discussed to support knowledge base of enterprise for resource management. The example of applications for supply chain of insurance company is presented.;Institute of Electrical and Electronics Engineers Inc.;Conference Proceeding;Proceedings - 2018 International Conference on Control, Artificial Intelligence, Robotics and Optimization, ICCAIRO 2018;2018-07-02;https://api.elsevier.com/content/abstract/scopus_id/85065208002
259;Multi-Agent Systems for Real-Time Adaptive Resource Management;According to Stephen Hawking, the twenty-first century will be the century of complexity, compared to the previous century of physics and biology. The growing complexity of the modern real-time economy is already well recognized and is associated with increased uncertainty and the dynamics of demand and supply, the individual approach to clients and resources, etc. The new economy strongly demands adaptive solutions for real-time decision-making support on resource allocation, scheduling, optimization, coordination, and controlling, which need to support a high level of adaptability and responsiveness in real time. However, there is a gap with existing solutions based on combinatorial top-down methods and tools of scheduling.Multi-agent technology is considered to be one of the most innovative and powerful tools for solving this problem. In this paper, we will present our approach for developing adaptive multi-agent solutions for solving scheduling problems in real time and give examples of our commercial applications in industry, which have run in everyday operations for several years and offer both measured and proven benefits. For all these applications, multi-agent technology has been critically important in providing the required functionality of solutions.;Elsevier Inc.;Book;Industrial Agents: Emerging Applications of Software Agents in Industry;2015-03-12;https://api.elsevier.com/content/abstract/scopus_id/84944408580
260;Wireless technologies: Overview for automatic vehicle location applications;Defining the wireless marketing is a challenge in today's world. Companies interested in capitalizing on the wireless market for automatic vehicle location (AVL) have a number of wireless options from which to choose. One of the most exciting wireless combinations is GPS (Global Positioning System) and cellular systems. The Global Positioning System (GPS)/cellular combination can be used to create Automatic Vehicle Location systems for a wide variety of applications, from fleet management to personal security. Unfortunately, no single wireless network fits all the possible AVL applications, and choosing the best network for an application is essential to system performance. This paper reviews the current wireless technologies available in the market-place, discusses why ATX chose the wireless technology it uses, and gazes into the crystal ball to forecast the future of wireless. Copyright © 1998 Society of Automotive Engineers, Inc.;SAE International;Journal;SAE Technical Papers;1998-01-01;https://api.elsevier.com/content/abstract/scopus_id/85072466054
261;Towards Model-Driven Digital Twin Engineering: Current Opportunities and Future Challenges;Digital Twins have emerged since the beginning of this millennium to better support the management of systems based on (real-time) data collected in different parts of the operating systems. Digital Twins have been successfully used in many application domains, and thus, are considered as an important aspect of Model-Based Systems Engineering (MBSE). However, their development, maintenance, and evolution still face major challenges, in particular: (i) the management of heterogeneous models from different disciplines, (ii) the bi-directional synchronization of digital twins and the actual systems, and (iii) the support for collaborative development throughout the complete life-cycle. In the last decades, the Model-Driven Engineering (MDE) community has investigated these challenges in the context of software systems. Now the question arises, which results may be applicable for digital twin engineering as well. In this paper, we identify various MDE techniques and technologies which may contribute to tackle the three mentioned digital twin challenges as well as outline a set of open MDE research challenges that need to be addressed in order to move towards a digital twin engineering discipline.;Springer Science and Business Media Deutschland GmbHinfo@springer-sbm.com;Book Series;Communications in Computer and Information Science;2020-01-01;https://api.elsevier.com/content/abstract/scopus_id/85094117399
262;The next evolution of MDE: a seamless integration of machine learning into domain modeling;Machine learning algorithms are designed to resolve unknown behaviors by extracting commonalities over massive datasets. Unfortunately, learning such global behaviors can be inaccurate and slow for systems composed of heterogeneous elements, which behave very differently, for instance as it is the case for cyber-physical systems and Internet of Things applications. Instead, to make smart decisions, such systems have to continuously refine the behavior on a per-element basis and compose these small learning units together. However, combining and composing learned behaviors from different elements is challenging and requires domain knowledge. Therefore, there is a need to structure and combine the learned behaviors and domain knowledge together in a flexible way. In this paper we propose to weave machine learning into domain modeling. More specifically, we suggest to decompose machine learning into reusable, chainable, and independently computable small learning units, which we refer to as microlearning units. These microlearning units are modeled together with and at the same level as the domain data. We show, based on a smart grid case study, that our approach can be significantly more accurate than learning a global behavior, while the performance is fast enough to be used for live learning.;Springer Verlagservice@springer.de;Journal;Software and Systems Modeling;2019-04-04;https://api.elsevier.com/content/abstract/scopus_id/85019693871
263;Myroute: A graph-dependency based model for real-time route prediction;Mobility prediction is an important problem having numerous applications in mobile computing and pervasive systems. However, many mobility prediction approaches are not noise tolerant, do not consider collective and individual behavior for making predictions, and provide a low accuracy. This paper addresses these issues by proposing a novel dependency-graph based predictor for real-time route prediction, named MyRoute. The proposed approach represents routes as a graph, which is then used to accurately match road network architecture with real-world vehicle movements. Unlike many prediction models, the designed model is noise tolerant, and can thus provide high accuracy even with data that contains noise and inaccuracies such as GPS mobility data. To cope with noise found in trajectory data, a lookahead window is used to build the prediction graph. Besides, the proposed approach integrates two mechanisms to consider both the collective and individual mobility behaviors of drivers. Experiments on real and synthetic datasets have shown that the performance of the designed model is excellent when compared to two state-of-the-art models.;Engineering and Technology Publishing;Journal;Journal of Communications;2017-12-01;https://api.elsevier.com/content/abstract/scopus_id/85057136041
264;A marginalized denoising method for link prediction in relational data;"Missing information is ubiquitous in relational datasets. Imputation of missing relations, a.k.a. link prediction, has become an increasingly crucial problem in relational data analysis as a huge amount of data has been accumulated in various fields. Recent advances in the latent variable models have greatly improved the state-of-the-art in the link prediction accuracy, however it comes at the price of increasing complexity. In this paper, we propose a novel link prediction algorithm, marginalized denoising model (MDM), where the problem of predicting unobserved or missing links in a given relational matrix is cast as a problem of matrix denoising. The method learns a mapping function that models the embedded topological structures of the relational network by capturing the so-called indirect affinities among entities. We train the mapping function by recovering the originally observed matrix from a conceptually ""infinite"" number of corrupted matrices where some links are randomly masked from the observed matrix. By re-applying the learned function to the observed relational matrix, we aim to ""denoise"" the observed matrix and thus to recover the unobserved links. Experimental results on several benchmarks demonstrate the superior performance of the new method over several stateof-the-art link prediction methods. Copyright";Society for Industrial and Applied Mathematics Publicationssupport@jstor.org;Conference Proceeding;SIAM International Conference on Data Mining 2014, SDM 2014;2014-01-01;https://api.elsevier.com/content/abstract/scopus_id/84925504830
265;Model-driven digital twin construction: Synthesizing the integration of cyber-physical systems with their information systems;Digital twins emerge in many disciplines to support engineering, monitoring, controlling, and optimizing cyber-physical systems, such as airplanes, cars, factories, medical devices, or ships. There is an increasing demand to create digital twins as representation of cyber-physical systems and their related models, data traces, aggregated data, and services. Despite a plethora of digital twin applications, there are very few systematic methods to facilitate the modeling of digital twins for a given cyber-physical system. Existing methods focus only on the construction of specific digital twin models and do not consider the integration of these models with the observed cyber-physical system. To mitigate this, we present a fully model-driven method to describe the software of the cyber-physical system, its digital twin information system, and their integration. The integration method relies on MontiArc models of the cyber-physical system's architecture and on UML/P class diagrams from which the digital twin information system is generated. We show the practical application and feasibility of our method on an IoT case study. Explicitly modeling the integration of digital twins and cyber-physical systems eliminates repetitive programming activities and can foster the systematic engineering of digital twins.;Association for Computing Machinery, Inc;Conference Proceeding;Proceedings - 23rd ACM/IEEE International Conference on Model Driven Engineering Languages and Systems, MODELS 2020;2020-10-16;https://api.elsevier.com/content/abstract/scopus_id/85096992990
266;GREYCAT: Efficient what-if analytics for data in motion at scale;Over the last few years, data analytics shifted from adescriptive era, confined to the explanation of past events, to the emergence of predictive techniques. Nonetheless, existing predictive techniques still fail to effectively explore alternative futures, which continuously diverge from current situations when exploring the effects of what-if decisions. Enabling prescriptive analytics therefore calls for the design of scalable systems that can cope with the complexity and the diversity of underlying data models. In this article, we address this challenge by combining graphs and time series within a scalable storage system that can organize a massive amount of unstructured and continuously changing data into multi-dimensional data models, called Many-Worlds Graphs. We demonstrate that our open source implementation, GREYCAT, can efficiently fork and update thousands of parallel worlds composed of millions of timestamped nodes, such as what-if exploration.;Elsevier Ltd;Journal;Information Systems;2019-07-01;https://api.elsevier.com/content/abstract/scopus_id/85063026008
267;EDML 2019 - Proceedings of the 1st Workshop on Evaluation and Experimental Design in Data Mining and Machine Learning, co-located with SIAM International Conference on Data Mining, SDM 2019;"The proceedings contain 6 papers. The topics discussed include: context-driven data mining through bias removal and incompleteness mitigation; EvalNE: a framework for evaluating network embeddings on link prediction; benchmarking nearest neighbor search: influence of local intrinsic dimensionality and result diversity in real-world datasets; instance space analysis for unsupervised outlier detection; characterizing transactional databases for frequent itemset mining; and evaluation of unsupervised learning results: making the seemingly impossible possible.";CEUR-WSceurws@sunsite.informatik.rwth-aachen.de;Conference Proceeding;CEUR Workshop Proceedings;2019-01-01;https://api.elsevier.com/content/abstract/scopus_id/85072749527
268;Reconciling Predictions in the Regression Setting: An Application to Bus Travel Time Prediction;"In different application areas, the prediction of values that are hierarchically related is required. As an example, consider predicting the revenue per month and per year of a company where the prediction of the year should be equal to the sum of the predictions of the months of that year. The idea of reconciliation of prediction on grouped time-series has been previously proposed to provide optimal forecasts based on such data. This method in effect, models the time-series collectively rather than providing a separate model for time-series at each level. While originally, the idea of reconciliation is applicable on data of time-series nature, it is not clear if such an approach can also be applicable to regression settings where multi-attribute data is available. In this paper, we address such a problem by proposing Reconciliation for Regression (R4R), a two-step approach for prediction and reconciliation. In order to evaluate this method, we test its applicability in the context of Travel Time Prediction (TTP) of bus trips where two levels of values need to be calculated: (i) travel times of the links between consecutive bus-stops; and (ii) total trip travel time. The results show that R4R can improve the overall results in terms of both link TTP performance and reconciliation between the sum of the link TTPs and the total trip travel time. We compare the results acquired when using group-based reconciliation methods and show that the proposed reconciliation approach in a regression setting can provide better results in some cases. This method can be generalized to other domains as well.";Springer;Book Series;Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics);2020-01-01;https://api.elsevier.com/content/abstract/scopus_id/85084260249
269;A Hitchhiker's Guide to Model-Driven Engineering for Data-Centric Systems;The models and data framework demystifies the different roles that models and data play in software development and operation and clarifies where machine learning and artificial intelligence techniques could be used.;IEEE Computer Society;Journal;IEEE Software;2021-07-01;https://api.elsevier.com/content/abstract/scopus_id/85085080142
